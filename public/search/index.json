[{"content":"Currently, I\u0026rsquo;m working through this great book An Introduction to Statistical Learning with Applications in R, and I just finished the chapter on linear regression. One thing that I really like about this book is that there is a great balance of learning R and just learning statistics in a mathematical way which really shows the backbone of the R functions that I was just using without really knowing how they work.\nFor this analysis I used the built in dataset \u0026ldquo;airquality\u0026rdquo; in R which stores 153 observations of different air quality measures averaged out by day taken in New York City from May to September 1973. I used this to practice my skills in linear regression, both simple and multivariate.\nLoad Libraries 1 2 3 4 5 6 suppressPackageStartupMessages({ library(tidyverse) library(broom) library(car) library(ggthemes) }) Load Data 1 data(\u0026#34;airquality\u0026#34;) Explore and Clean Data 1 str(airquality) 1 2 3 4 5 6 7 ## \u0026#39;data.frame\u0026#39;:\t153 obs. of 6 variables: ## $ Ozone : int 41 36 12 18 NA 28 23 19 8 NA ... ## $ Solar.R: int 190 118 149 313 NA NA 299 99 19 194 ... ## $ Wind : num 7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ... ## $ Temp : int 67 72 74 62 56 66 65 59 61 69 ... ## $ Month : int 5 5 5 5 5 5 5 5 5 5 ... ## $ Day : int 1 2 3 4 5 6 7 8 9 10 ... Summary Statistics Creating a table of summary statistics that also doubles as a way to check for missing values, as since this dataset only has 153 observations so substantial amounts of missing data will affect the outcomes.\n1 2 3 4 airquality %\u0026gt;% dplyr::select(Ozone, Solar.R, Wind, Temp) %\u0026gt;% map_df(.f = ~broom::tidy(summary(.x)), .id = \u0026#34;variable\u0026#34;) 1 2 3 4 5 6 7 ## # A tibble: 4 × 8 ## variable minimum q1 median mean q3 maximum na ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Ozone 1 18 31.5 42.1 63.2 168 37 ## 2 Solar.R 7 116. 205 186. 259. 334 7 ## 3 Wind 1.7 7.4 9.7 9.96 11.5 20.7 NA ## 4 Temp 56 72 79 77.9 85 97 NA So it looks like there are a decent amount of missing values for the ozone variable, I\u0026rsquo;m going to use mean imputation to deal with this.\nMean Imputation for Missing Values 1 2 3 4 5 6 7 8 9 10 airquality_mi \u0026lt;- airquality %\u0026gt;% mutate( ozone_mi = as.integer(if_else(is.na(Ozone), mean(Ozone, na.rm = TRUE), Ozone)), solar_mi = as.integer(if_else(is.na(Solar.R), mean(Solar.R, na.rm = TRUE), Solar.R)) ) %\u0026gt;% select(-1, -2) Clean Up and Print 1 2 3 4 5 6 names(airquality_mi)[1:6] \u0026lt;- tolower(names(airquality_mi)[1:6]) airquality_mi \u0026lt;- airquality_mi %\u0026gt;% select(month, day, wind, temp, ozone_mi, solar_mi) as_tibble(airquality_mi) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 153 × 6 ## month day wind temp ozone_mi solar_mi ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 5 1 7.4 67 41 190 ## 2 5 2 8 72 36 118 ## 3 5 3 12.6 74 12 149 ## 4 5 4 11.5 62 18 313 ## 5 5 5 14.3 56 42 185 ## 6 5 6 14.9 66 28 185 ## 7 5 7 8.6 65 23 299 ## 8 5 8 13.8 59 19 99 ## 9 5 9 20.1 61 8 19 ## 10 5 10 8.6 69 42 194 ## # ℹ 143 more rows Simple Regression Visualize Variables of Interest 1 2 3 4 5 6 7 8 9 10 ggplot(data = airquality_mi, aes(x = temp, y = ozone_mi)) + geom_jitter() + labs( x = \u0026#34;Temperature Measured in Fahrenheit\u0026#34;, y = \u0026#34;Mean Ozone per Day in Parts Per Billion\u0026#34;, title = \u0026#34;Ozone Concentration vs. Temperature in New York City from May to September 1973\u0026#34; ) + theme_stata() + theme(plot.title = element_text(hjust = 0.5)) Fit Model 1 2 aq.simple \u0026lt;- lm(ozone_mi ~ temp, data = airquality_mi) summary(aq.simple) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## ## Call: ## lm(formula = ozone_mi ~ temp, data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.006 -15.851 -2.160 8.469 120.149 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -101.6222 15.3547 -6.618 5.96e-10 *** ## temp 1.8454 0.1957 9.428 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 22.84 on 151 degrees of freedom ## Multiple R-squared: 0.3706,\tAdjusted R-squared: 0.3664 ## F-statistic: 88.9 on 1 and 151 DF, p-value: \u0026lt; 2.2e-16 Looking at the summary of this model its already pretty evident that there are some issues, the residual standard error is pretty high and the R squared shows that a majority of variation cannot be explained by the model. I\u0026rsquo;m also going to run some diagnostic plots to see if there are other issues.\nDiagnostics 1 2 par(mfrow = c(2, 2)) plot(aq.simple) These plots suggest that there may be a non linear relationship between the variables, so I'm going to apply a transformation and see if that helps. Polynomial Transformation 1 2 aq.simple.trans \u0026lt;- lm(ozone_mi ~ temp + I(temp^2), data = airquality_mi) summary(aq.simple.trans) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## ## Call: ## lm(formula = ozone_mi ~ temp + I(temp^2), data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.463 -13.131 -2.735 9.869 124.916 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 296.43794 94.87393 3.125 0.002138 ** ## temp -8.78275 2.50999 -3.499 0.000615 *** ## I(temp^2) 0.06981 0.01644 4.246 3.8e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 21.65 on 150 degrees of freedom ## Multiple R-squared: 0.4381,\tAdjusted R-squared: 0.4306 ## F-statistic: 58.47 on 2 and 150 DF, p-value: \u0026lt; 2.2e-16 So that\u0026rsquo;s a bit better in terms of R squared, I\u0026rsquo;m going to re-check the diagnostic plots.\n1 2 par(mfrow = c(2, 2)) plot(aq.simple.trans) This is not the best model for the data, but being able to recognize that is also important. I\u0026rsquo;m going to visualize the regression line on my previous plot for the sake of practice.\nVisualize with Least Squares Regression Line 1 2 3 4 5 6 7 8 9 10 11 12 ggplot(data = airquality_mi, aes(x = temp, y = ozone_mi)) + geom_jitter() + stat_smooth(method = \u0026#34;lm\u0026#34;, formula = y ~ poly(x, 2, raw = TRUE)) + labs( x = \u0026#34;Temperature Measured in Fahrenheit\u0026#34;, y = \u0026#34;Mean Ozone per Day in Parts Per Billion\u0026#34;, title = \u0026#34;Ozone Concentration vs. Temperature in New York City from May to September 1973\u0026#34; ) + theme_stata() + theme(plot.title = element_text(hjust = 0.5)) Multivariate Regression I also want to do a multivariate regression with this data to see if I can come up with a better model and be able to practice making predictions using a regression model.\nFit Model 1 2 aq.mv \u0026lt;- lm(ozone_mi ~ temp + wind + solar_mi, data = airquality_mi) summary(aq.mv) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## ## Call: ## lm(formula = ozone_mi ~ temp + wind + solar_mi, data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.541 -14.590 -5.148 12.172 101.198 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -38.20875 18.88354 -2.023 0.04482 * ## temp 1.24096 0.20907 5.935 1.97e-08 *** ## wind -2.71862 0.54280 -5.009 1.53e-06 *** ## solar_mi 0.05772 0.02003 2.881 0.00454 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 20.9 on 149 degrees of freedom ## Multiple R-squared: 0.48,\tAdjusted R-squared: 0.4696 ## F-statistic: 45.85 on 3 and 149 DF, p-value: \u0026lt; 2.2e-16 Diagnostics The first thing I want to check since I have multiple predictor variables in my new model is colinearity. I\u0026rsquo;m going to use the vif function to calculate the variance inflation factor to see if there are any variables that are correlated too strongly.\n1 vif(aq.mv) 1 2 ## temp wind solar_mi ## 1.363082 1.272795 1.080454 Looks good! Moving on to normal diagnostic plots\n1 2 par(mfrow = c(2, 2)) plot(aq.mv) This model could use some work to get it to fit the data and assumptions a bit better, I'm going to see if doing some transformations of the predictor helps. Transformation First I\u0026rsquo;m going to visualize my individual predictor variables with the response variables just to see what I\u0026rsquo;m working with and which transformations might help me.\nI already visualized ozone and temperature in the simple regression, so I\u0026rsquo;m going to visualize ozone and wind.\n1 2 3 4 5 6 7 8 9 10 ggplot(data = airquality_mi, aes(x = wind, y = ozone_mi)) + geom_jitter() + labs( x = \u0026#34;Average Wind Speed Measured in Miles Per Hour\u0026#34;, y = \u0026#34;Mean Ozone per Day in Parts Per Billion\u0026#34;, title = \u0026#34;Ozone Concentration vs. Wind in New York City from May to September 1973\u0026#34; ) + theme_stata() + theme(plot.title = element_text(hjust = 0.5)) And ozone and solar.\n1 2 3 4 5 6 7 8 9 10 ggplot(data = airquality_mi, aes(x = solar_mi, y = ozone_mi)) + geom_jitter() + labs( x = \u0026#34;Solar Radiation Measured in Langleys\u0026#34;, y = \u0026#34;Mean Ozone per Day in Parts Per Billion\u0026#34;, title = \u0026#34;Ozone Concentration vs. Solar Radiation in New York City from May to September 1973\u0026#34; ) + theme_stata() + theme(plot.title = element_text(hjust = 0.5)) I\u0026rsquo;m going to apply polynomial transformations to two out of the three predictor variables.\n1 2 aq.mv.trans \u0026lt;- lm(ozone_mi ~ temp + I(temp^2) + wind + I(wind^2) + solar_mi, data = airquality_mi) summary(aq.mv.trans) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## ## Call: ## lm(formula = ozone_mi ~ temp + I(temp^2) + wind + I(wind^2) + ## solar_mi, data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.293 -10.929 -3.289 8.684 89.513 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 321.25924 81.78038 3.928 0.000131 *** ## temp -7.28999 2.18385 -3.338 0.001069 ** ## I(temp^2) 0.05580 0.01431 3.898 0.000147 *** ## wind -11.10077 1.94439 -5.709 6.07e-08 *** ## I(wind^2) 0.39616 0.08865 4.469 1.56e-05 *** ## solar_mi 0.06206 0.01772 3.503 0.000610 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 18.46 on 147 degrees of freedom ## Multiple R-squared: 0.5998,\tAdjusted R-squared: 0.5862 ## F-statistic: 44.07 on 5 and 147 DF, p-value: \u0026lt; 2.2e-16 This R2 and RSE is definitely better than the simple regression model, as now almost 60% of the variation in the data can be explained by the model rather than being closer to 40%, still not the best but definitely better. Now I\u0026rsquo;m going to re-check the diagnostics to see if this satisfies the assumptions a bit better.\n1 2 par(mfrow = c(2, 2)) plot(aq.mv.trans) This definitely helped deal with the pattern in the residuals vs. fitted plot and helps satisfy the linearity assumption of the model. Make Predictions I\u0026rsquo;m going to use this model to make a couple predictions about ozone levels when given information about temperature, solar radiation, and wind speeds, along with confidence intervals.\n1 2 3 4 5 6 7 8 9 10 11 new_data_point \u0026lt;- data.frame( solar_mi = c(250, 120, 320), temp = c(63, 74, 82), wind = c(10.2, 9.5, 7.0) ) predict( aq.mv.trans, new_data_point, interval = \u0026#34;confidence\u0026#34; ) 1 2 3 4 ## fit lwr upr ## 1 26.96636 19.31629 34.61643 ## 2 25.10863 20.28017 29.93708 ## 3 60.25022 54.08080 66.41964 Conclusion I\u0026rsquo;ve tried to perform linear regressions before, as it is very simple to do in R but I really didn\u0026rsquo;t understand any of the outputs of the model or what constitutes as a good model, but the Linear Regression chapter of Intro to Statistical Learning along with running this analysis helped me understand those concepts better.\nI also, until then, didn\u0026rsquo;t really understand the importance of linear regression and thought that it was just similar to correlation, but we can actually use linear regression in order to input new data to get a prediction for the response variable, in this case the amount of ozone in the atmosphere.\nIn working on this analysis I also learned about mean imputation when dealing with missing values, which has its advantages and disadvantages compared to just removing rows with missing values.\n","date":"2024-02-02T00:00:00Z","image":"https://michelleyg1.github.io/p/airquality/images/smoke_hu5459c0360c2b0cb7a147d2df0eb350ca_571128_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/airquality/","title":"Air Quality Regression"},{"content":"One thing that I’ve learned since I’ve been looking into working with large complex survey data like the CDC’s NHANES is about survey weights and how to use them in your analysis. In this analysis I tried to use the survey package in R to use the survey design to analyze the data, as there are certain demographics that were oversampled and undersampled as a part of the survey design.\nLoad Packages 1 2 3 4 5 6 7 suppressPackageStartupMessages({ library(tidyverse) library(nhanesA) library(janitor) library(survey) library(gtsummary) }) Load Data 1 2 demo \u0026lt;- nhanes(\u0026#34;P_DEMO\u0026#34;) demo \u0026lt;- nhanesTranslate(\u0026#34;P_DEMO\u0026#34;, names(demo), data = demo) ## Translated columns: RIDSTATR RIAGENDR RIDRETH1 RIDRETH3 RIDEXMON DMDBORN4 DMDYRUSZ DMDEDUC2 DMDMARTZ RIDEXPRG SIALANG SIAPROXY SIAINTRP FIALANG FIAPROXY FIAINTRP MIALANG MIAPROXY MIAINTRP AIALANGA 1 2 exam \u0026lt;- nhanes(\u0026#34;P_BMX\u0026#34;) exam \u0026lt;- nhanesTranslate(\u0026#34;P_BMX\u0026#34;, names(exam), data = exam) ## Translated columns: BMDSTATS BMIWT BMIHT BMDBMIC Retain Useful Variables 1 2 3 4 5 demo_select \u0026lt;- demo %\u0026gt;% select(SEQN, RIAGENDR, RIDAGEYR, RIDRETH3, SDMVPSU, SDMVSTRA, WTMECPRP) exam_select \u0026lt;- exam %\u0026gt;% select(SEQN, BMXBMI) Merge Data 1 2 3 merged_data \u0026lt;- merge(demo_select, exam_select, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) merged_data$SEQN \u0026lt;- NULL Clean Dataset to Make Analysis Easier First I wanted to see which race categories there are so that I can capture all of them when I recode them into more simple categories.\n1 merged_data %\u0026gt;% tabyl(RIDRETH3) ## RIDRETH3 n percent ## Mexican American 1990 0.12789203 ## Other Hispanic 1544 0.09922879 ## Non-Hispanic White 5271 0.33875321 ## Non-Hispanic Black 4098 0.26336761 ## Non-Hispanic Asian 1638 0.10526992 ## Other Race - Including Multi-Racial 1019 0.06548843 Now, I’m going to rename some variables, and use case_when() to make the race_cat and bmi_cat columns, as well as factoring these columns so that they can be used in my analysis and are not just recognized as character strings by R.\nI also added at the end to output a tibble of the raw NHANES data without the weights to print out how my clean data looks and make sure that I didn’t miss any columns in the renaming process.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 nhanes \u0026lt;- merged_data %\u0026gt;% rename( gender = RIAGENDR, age = RIDAGEYR, bmi = BMXBMI, psu = SDMVPSU, strata = SDMVSTRA, weight = WTMECPRP) %\u0026gt;% mutate( race_cat = case_when(RIDRETH3 == \u0026#34;Mexican American\u0026#34; | RIDRETH3 == \u0026#34;Other Hispanic\u0026#34; ~ \u0026#34;Hispanic\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic White\u0026#34; ~ \u0026#34;White\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic Black\u0026#34; ~ \u0026#34;Black\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic Asian\u0026#34; ~ \u0026#34;Asian\u0026#34;, RIDRETH3 == \u0026#34;Other Race - Including Multi-Racial\u0026#34; ~ \u0026#34;Other\u0026#34;), bmi_cat = case_when(bmi \u0026lt; 18.5 ~ \u0026#34;Underweight\u0026#34;, bmi \u0026lt; 25 ~ \u0026#34;Normal\u0026#34;, bmi \u0026lt; 30 ~ \u0026#34;Overweight\u0026#34;, bmi \u0026gt;= 30 ~ \u0026#34;Obese\u0026#34;) ) %\u0026gt;% filter(!is.na(bmi)) %\u0026gt;% mutate_if(., is.character, as.factor) %\u0026gt;% select(-RIDRETH3) # Unweighted nhanes_tibble \u0026lt;- as_tibble(nhanes) %\u0026gt;% filter(age \u0026gt;= 18) %\u0026gt;% select(-psu, -strata, -weight) %\u0026gt;% print() ## # A tibble: 8,790 × 5 ## gender age bmi race_cat bmi_cat ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Female 29 37.8 Asian Obese ## 2 Male 49 29.7 White Overweight ## 3 Male 36 21.9 White Normal ## 4 Male 68 30.2 Other Obese ## 5 Male 76 26.6 White Overweight ## 6 Female 44 39.1 Hispanic Obese ## 7 Female 33 28.9 Asian Overweight ## 8 Female 68 28.1 Black Overweight ## 9 Female 42 31.3 Asian Obese ## 10 Male 58 30.5 Hispanic Obese ## # ℹ 8,780 more rows Refactor Order of BMI Categories R shows factored variables autmatically in alphabetical order, but this did not make much sense for my chart as BMI categories make more sense from underweight to obese, so I used the factor() function to remedy this before I make my table.\n1 2 nhanes$bmi_cat \u0026lt;- factor(nhanes$bmi_cat, levels = c(\u0026#34;Underweight\u0026#34;, \u0026#34;Normal\u0026#34;, \u0026#34;Overweight\u0026#34;, \u0026#34;Obese\u0026#34;)) Survey Design I learned that in order to properly analyze large survey data, and to make it representative of the population at large, you have to create a survey design object and use that in your analysis.\n1 2 3 4 5 nhanes_design \u0026lt;- svydesign(id = ~psu, strata = ~strata, weights = ~weight, nest = TRUE, data = nhanes) Apply Eligibility Criteria It is also important that the data is subset in this manner when working with survey data.\n1 nhanes_adult \u0026lt;- subset(nhanes_design, age \u0026gt;= 18) Summary Statistics BMI Categories by Race/Hispanic Origin, Gender, and Average Age Weighted 1 2 3 4 5 6 7 8 tbl_svysummary(nhanes_adult, by = bmi_cat, include = c(gender, age, race_cat), label = list(gender ~ \u0026#34;Gender\u0026#34;, age ~ \u0026#34;Age\u0026#34;, race_cat ~ \u0026#34;Race/Hispanic Origin\u0026#34;), statistic = list(age ~ \u0026#34;{mean} ({sd})\u0026#34;) ) \u0026#10; Characteristic Underweight, N = 3,797,8841 Normal, N = 62,452,8711 Overweight, N = 76,984,8391 Obese, N = 101,342,0061 Gender Male 1,148,289 (30%) 27,234,973 (44%) 41,256,750 (54%) 48,289,482 (48%) Female 2,649,595 (70%) 35,217,899 (56%) 35,728,089 (46%) 53,052,524 (52%) Age 37 (17) 44 (19) 50 (17) 48 (17) Race/Hispanic Origin Asian 385,874 (10%) 6,348,964 (10%) 5,473,194 (7.1%) 2,321,160 (2.3%) Black 613,071 (16%) 6,036,949 (9.7%) 7,381,673 (9.6%) 13,643,866 (13%) Hispanic 440,910 (12%) 7,245,902 (12%) 14,181,024 (18%) 17,625,000 (17%) Other 106,290 (2.8%) 2,276,951 (3.6%) 2,692,021 (3.5%) 4,801,387 (4.7%) White 2,251,739 (59%) 40,544,106 (65%) 47,256,927 (61%) 62,950,593 (62%) \u0026#10; 1 n (%); Mean (SD) BMI Categories by Race/Hispanic Origin, Gender, and Average Age Unweighted 1 2 3 4 5 6 7 8 tbl_summary(nhanes_tibble, by = bmi_cat, include = c(gender, age, race_cat), label = list(gender ~ \u0026#34;Gender\u0026#34;, age ~ \u0026#34;Age\u0026#34;, race_cat ~ \u0026#34;Race/Hispanic Origin\u0026#34;), statistic = list(age ~ \u0026#34;{mean} ({sd})\u0026#34;) ) \u0026#10; Characteristic Normal, N = 2,1851 Obese, N = 3,6881 Overweight, N = 2,7671 Underweight, N = 1501 Gender Male 1,044 (48%) 1,644 (45%) 1,521 (55%) 62 (41%) Female 1,141 (52%) 2,044 (55%) 1,246 (45%) 88 (59%) Age 46 (20) 50 (17) 52 (18) 39 (20) Race/Hispanic Origin Asian 478 (22%) 163 (4.4%) 403 (15%) 27 (18%) Black 494 (23%) 1,175 (32%) 609 (22%) 48 (32%) Hispanic 341 (16%) 879 (24%) 701 (25%) 19 (13%) Other 110 (5.0%) 198 (5.4%) 113 (4.1%) 6 (4.0%) White 762 (35%) 1,273 (35%) 941 (34%) 50 (33%) \u0026#10; 1 n (%); Mean (SD) Gender vs BMI T-Test Normality Assumption 1 svyqqmath(~bmi, design = nhanes_adult) This deviates from the normal distribution, so I applied a log transformation to the BMI variable to remedy this deviation from the normal distribution at the tail end of the data.\n1 svyqqmath(~log10(bmi), design = nhanes_adult) That looks a lot better, now time to visualize the data and run the analysis. Boxplot Visualization 1 2 3 4 5 svyboxplot(log10(bmi) ~ gender, design = nhanes_adult, xlab = \u0026#34;Gender\u0026#34;, main = \u0026#34;Weighted Boxplot of Mean Log Transformed BMI by Gender\u0026#34;, ylab = \u0026#34;Log Transformed BMI\u0026#34;) T-Test 1 svyttest(log10(bmi) ~ gender, nhanes_adult) ## ## Design-based t-test ## ## data: log10(bmi) ~ gender ## t = 0.46451, df = 24, p-value = 0.6465 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -0.005848037 0.009244940 ## sample estimates: ## difference in mean ## 0.001698451 Race/Hispanic Origin vs. BMI ANOVA 1 2 3 4 5 6 svyboxplot(log10(bmi) ~ race_cat, design = nhanes_adult, xlab = \u0026#34;Race/Hispanic Origin\u0026#34;, ylab = \u0026#34;Log Transformed BMI\u0026#34;, main = \u0026#34;Weighted Boxplot of Mean Log Transformed BMI by Race/Hispanic Origin\u0026#34;) In visually inspecting the boxplot, it looks like there is a difference between the various race/hispanic origin groups and BMI, but lets see. Fit Model 1 race.bmi.glm \u0026lt;- svyglm(log10(bmi) ~ race_cat, nhanes_adult) ANOVA A barrier that I ran into was that the survey package in R does not seen to have an ANOVA function, and when you run the survey GLM through the regular ANOVA function it does not work as I thought it would. I did some researching around and I found that this is a way to assess this for complex surveys using a Wald test.\n1 2 options(scipen = 999) regTermTest(race.bmi.glm, ~race_cat) ## Wald test for race_cat ## in svyglm(formula = log10(bmi) ~ race_cat, design = nhanes_adult) ## F = 184.1178 on 4 and 21 df: p= 0.00000000000000050064 So it looks like there is a statistically significant difference between the various Race/Hispanic Origin categories and BMI, as hypothesized by the boxplot.\nConclusion While the NHANES dataset is still very useful, I did have to dig a lot deeper and do more research to find out how I can properly use this data.\nAs I am also working through another book on statistics, I found out that there is not just parametric statistics if your data is normal, and non parmetric statistics if it isn’t, but that you’re allowed to transform your data to help fit better into the normal distribution so that you can use parametric statistics like T-Tests with more confidence.\n","date":"2024-01-20T00:00:00Z","image":"https://michelleyg1.github.io/p/nhanes-bmi-analysis-using-survey-package/images/green_hu5459c0360c2b0cb7a147d2df0eb350ca_2075947_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/nhanes-bmi-analysis-using-survey-package/","title":"NHANES BMI Analysis Using Survey Package"},{"content":"For this analysis I decided to use the CDC NHANES data (2017 - pre pandemic 2020). I found this guide on importing and cleaning NHANES data using the nhanesA package, and found it very useful in this project.\nNHANES is a very large survey conducted by the CDC that is a crucial resource for public health data analysis, and is all available for free public use. As this was my first time using this dataset, I decided to keep it simple, but I will definitely be coming back to it as there is just so much information collected that can be used for all types of public health analysis.\nLoad Packages 1 2 3 4 5 6 7 suppressPackageStartupMessages({ library(tidyverse) library(nhanesA) library(tableone) library(arsenal) library(agricolae) }) Grab Datasets that I Need Here I used the nhanesA package to get the datasets (or Data Files as they are called on the NHANES site), the two that I needed for this analysis were the demographics dataset\n1 2 demo \u0026lt;- nhanes(\u0026#34;P_DEMO\u0026#34;) demo_translate \u0026lt;- nhanesTranslate(\u0026#34;P_DEMO\u0026#34;, names(demo), data = demo) 1 ## Translated columns: RIDSTATR RIAGENDR RIDRETH1 RIDRETH3 RIDEXMON DMDBORN4 DMDYRUSZ DMDEDUC2 DMDMARTZ RIDEXPRG SIALANG SIAPROXY SIAINTRP FIALANG FIAPROXY FIAINTRP MIALANG MIAPROXY MIAINTRP AIALANGA and the total cholesterol one\n1 2 exam \u0026lt;- nhanes(\u0026#34;P_TCHOL\u0026#34;) exam_translate \u0026lt;- nhanesTranslate(\u0026#34;P_TCHOL\u0026#34;, names(exam), data = exam) 1 2 ## Warning in nhanesTranslate(\u0026#34;P_TCHOL\u0026#34;, names(exam), data = exam): No columns ## were translated Retain Useful Variables Here I am only retaining the variables that I want to bring into my final dataset.\nFrom the demographics file, I decided on SEQN which is the sequence number that will help us merge the two datasets, RIDEXPRG which indicates if the respondent is pregnant, RIAGENDR which stores the participants gender, RIDAGEYR which stores the participant\u0026rsquo;s age in years, and lastly RIDRETH3 which categorizes the participant\u0026rsquo;s race.\nFrom the exam file I only extracted the sequence number, and lab value for total cholesterol.\n1 2 3 4 5 demo_select \u0026lt;- demo_translate %\u0026gt;% select(SEQN, RIDEXPRG, RIAGENDR, RIDAGEYR, RIDRETH3) exam_select \u0026lt;- exam_translate %\u0026gt;% select(SEQN, LBXTC) Merge the Data Using SEQN to Match Values The structure of the NHANES database makes it easy to match which lab values belong to which patient in including the SEQN column on the seperate datasets. I also went ahead and dropped the SEQN column when I was done merging as I only needed it for that task.\n1 2 3 merged_data \u0026lt;- merge(demo_select, exam_select, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) merged_data$SEQN \u0026lt;- NULL Initial Investigaton of the Data Here I want to see the different categories and how much we have of each so that I can apply the eligibility criteria and recode the values properly.\n1 2 3 4 initial_table \u0026lt;- CreateTableOne(data = merged_data, includeNA = TRUE) print(initial_table, showAllLevels = TRUE) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## ## level ## n ## RIDEXPRG (%) Yes, positive lab pregnancy test or self-reported pregnant at exam ## The participant was not pregnant at exam ## Cannot ascertain if the participant is pregnant at exam ## \u0026lt;NA\u0026gt; ## RIAGENDR (%) Male ## Female ## RIDAGEYR (mean (SD)) ## RIDRETH3 (%) Mexican American ## Other Hispanic ## Non-Hispanic White ## Non-Hispanic Black ## Non-Hispanic Asian ## Other Race - Including Multi-Racial ## LBXTC (mean (SD)) ## ## Overall ## n 15560 ## RIDEXPRG (%) 87 ( 0.6) ## 1604 (10.3) ## 183 ( 1.2) ## 13686 (88.0) ## RIAGENDR (%) 7721 (49.6) ## 7839 (50.4) ## RIDAGEYR (mean (SD)) 33.74 (25.32) ## RIDRETH3 (%) 1990 (12.8) ## 1544 ( 9.9) ## 5271 (33.9) ## 4098 (26.3) ## 1638 (10.5) ## 1019 ( 6.5) ## LBXTC (mean (SD)) 177.46 (40.36) Apply Eligibility Criteria I decided for the eligibility criteria to exclude pregnant women, as well as those under the age of 20, I did that using dplyr\u0026rsquo;s filter() function, I also filtered out the columns where there was no total cholesterol lab value as that is not useful to my analysis.\n1 2 3 4 analytic_data \u0026lt;- merged_data %\u0026gt;% filter(!is.na(LBXTC), RIDAGEYR \u0026gt;= 20, RIDEXPRG != \u0026#34;Yes, positive lab pregnancy test or self-reported pregnant at exam\u0026#34; | is.na(RIDEXPRG)) Recode and Make Categories In this section, I created several new variables using dplyr\u0026rsquo;s mutate() function. I created the age_cat variable that groups the participants by age, the total_cholesterol_cat variable that groups the lab values by their normal, borderline and high ranges, and I also simplified the race data into more concise and broad categories.\nI also renamed the remaining variables to match the naming conventions in the other newly created variables, also to give some more sense to them as NHANES is great but the variables all look like keyboard smashes to me. Their variable search tool helps with that.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 category_data \u0026lt;- analytic_data %\u0026gt;% mutate( age_cat = cut(analytic_data$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE), total_cholesterol_cat = cut(analytic_data$LBXTC, c(0, 200, 240, Inf), labels = c(\u0026#34;Normal\u0026#34;, \u0026#34;Borderline\u0026#34;, \u0026#34;High\u0026#34;), right = FALSE), race = car::recode(analytic_data$RIDRETH3, \u0026#34;c(\u0026#39;Mexican American\u0026#39;, \u0026#39;Other Hispanic\u0026#39;) = \u0026#39;Hispanic\u0026#39;; \u0026#39;Non-Hispanic White\u0026#39; = \u0026#39;White\u0026#39;; \u0026#39;Non-Hispanic Black\u0026#39; = \u0026#39;Black\u0026#39;; \u0026#39;Non-Hispanic Asian\u0026#39; = \u0026#39;Asian\u0026#39;; \u0026#39;Other Race - Including Multi-Racial\u0026#39; = \u0026#39;Other\u0026#39;; else = NA\u0026#34;) ) %\u0026gt;% rename( pregnancy_status = RIDEXPRG, gender = RIAGENDR, age = RIDAGEYR, total_cholesterol = LBXTC ) %\u0026gt;% select(-RIDRETH3, -pregnancy_status) 1 2 cholesterol \u0026lt;- as_tibble(category_data) cholesterol 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 7,845 × 6 ## gender age total_cholesterol age_cat total_cholesterol_cat race ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Female 29 195 [20,40) Normal Asian ## 2 Male 49 147 [40,60) Normal White ## 3 Male 36 164 [20,40) Normal White ## 4 Male 68 105 [60,Inf) Normal Other ## 5 Male 76 233 [60,Inf) Borderline White ## 6 Female 44 212 [40,60) Borderline Hispanic ## 7 Female 68 165 [60,Inf) Normal Black ## 8 Female 42 229 [40,60) Borderline Asian ## 9 Male 58 172 [40,60) Normal Hispanic ## 10 Male 44 189 [40,60) Normal White ## # ℹ 7,835 more rows Table of Total Cholesterol Categories Here I used the arsenal package to make a table displaying how the different total cholesterol categories look within our study sample.\n1 2 3 labels \u0026lt;- list(age_cat = \u0026#34;Age Range\u0026#34;, gender = \u0026#34;Gender\u0026#34;, race = \u0026#34;Race\u0026#34;) tab \u0026lt;- arsenal::tableby(total_cholesterol_cat ~ age_cat + gender + race, data = cholesterol, test = FALSE) summary(tab, labelTranslations = labels, text=TRUE) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## ## ## | | Normal (N=5236) | Borderline (N=1873) | High (N=736) | Total (N=7845) | ## |:-----------|:---------------:|:-------------------:|:------------:|:--------------:| ## |Age Range | | | | | ## |- [20,40) | 1789 (34.2%) | 412 (22.0%) | 117 (15.9%) | 2318 (29.5%) | ## |- [40,60) | 1499 (28.6%) | 789 (42.1%) | 338 (45.9%) | 2626 (33.5%) | ## |- [60,Inf) | 1948 (37.2%) | 672 (35.9%) | 281 (38.2%) | 2901 (37.0%) | ## |Gender | | | | | ## |- Male | 2659 (50.8%) | 850 (45.4%) | 323 (43.9%) | 3832 (48.8%) | ## |- Female | 2577 (49.2%) | 1023 (54.6%) | 413 (56.1%) | 4013 (51.2%) | ## |Race | | | | | ## |- Asian | 559 (10.7%) | 271 (14.5%) | 108 (14.7%) | 938 (12.0%) | ## |- Black | 1430 (27.3%) | 400 (21.4%) | 162 (22.0%) | 1992 (25.4%) | ## |- Hispanic | 1169 (22.3%) | 432 (23.1%) | 164 (22.3%) | 1765 (22.5%) | ## |- Other | 245 (4.7%) | 96 (5.1%) | 36 (4.9%) | 377 (4.8%) | ## |- White | 1833 (35.0%) | 674 (36.0%) | 266 (36.1%) | 2773 (35.3%) | Checking Assumptions Assumption of Normal Distribution Next before I look and see if there are any statistically significant differences between the groups of my choice, I have to look and see the distribution of our data to see what kind of assumptions we can make when running the stats.\n1 2 3 4 5 6 7 ggplot(cholesterol, aes(x = total_cholesterol)) + geom_histogram(binwidth = 15, color = \u0026#34;black\u0026#34;, fill = \u0026#34;#bbbbff\u0026#34;) + labs(x = \u0026#34;Total Cholesterol (mg/dL)\u0026#34;, y = \u0026#34;Count\u0026#34;, title = \u0026#34;Total Cholesterol (mg/dL) Distribution\u0026#34;, caption = \u0026#34;Source: CDC NHANES 2017 - Pre Pandemic 2020\u0026#34;) + theme(plot.title = element_text(hjust = 0.5)) This follows a generally normal distribution, but I\u0026rsquo;m going to take a closer look as I do see outliers towards the end of our bell curve.\n1 2 cholesterol_model \u0026lt;- lm(total_cholesterol ~ race, data = cholesterol) plot(cholesterol_model, which = 2) It seems like there is a systematic deviation from the expected relationship if the data were to be normally distributed. The data is not normally distributed. Assumption of Constant Variance 1 plot(cholesterol_model, which = 3) There is constant variance throughout the data, however since our assumption of normal distribution was violated I am going to check one more assumption that would be needed to run a non parametric test assessing the differences between the central tendency of our chosen groups. Assumption of Similar Skewness for Each Category 1 2 3 4 5 6 7 8 9 ggplot(cholesterol, aes(x = total_cholesterol, fill = race)) + geom_histogram(binwidth = 15, color = \u0026#34;black\u0026#34;) + labs(x = \u0026#34;Total Cholesterol (mg/dL)\u0026#34;, y = \u0026#34;Count\u0026#34;, title = \u0026#34;Total Cholesterol (mg/dL) Distribution by Race/Hispanic Origin\u0026#34;, caption = \u0026#34;Source: CDC NHANES 2017 - Pre Pandemic 2020\u0026#34;) + theme(plot.title = element_text(hjust = 0.5)) + facet_wrap(~race) + scale_fill_manual(values = c(\u0026#34;#E6FFFD\u0026#34;, \u0026#34;#AEE2FF\u0026#34;, \u0026#34;#ACBCFF\u0026#34;, \u0026#34;#B799FF\u0026#34;, \u0026#34;#AA77FF\u0026#34;)) Looks like each category of Race/Hispanic origin does have a similar skewness. Statistical Testing Visualize Data with a Boxplot 1 2 3 4 5 6 7 8 9 10 11 ggplot(cholesterol, aes(x = race, y = total_cholesterol, fill = race)) + geom_boxplot() + scale_fill_manual(values = c(\u0026#34;#E6FFFD\u0026#34;, \u0026#34;#AEE2FF\u0026#34;, \u0026#34;#ACBCFF\u0026#34;, \u0026#34;#B799FF\u0026#34;, \u0026#34;#AA77FF\u0026#34;)) + labs( x = \u0026#34;Race/Hispanic Origin\u0026#34;, y = \u0026#34;Total Cholesterol (mg/dL)\u0026#34;, title = \u0026#34;Total Cholesterol (mg/dL) by Race/Hispanic Origin of Participant\u0026#34;, caption = \u0026#34;Source: CDC NHANES 2017 - Pre Pandemic 2020\u0026#34; ) + theme(legend.position = \u0026#34;none\u0026#34;, plot.title = element_text(hjust = 0.5)) Kruskal Wallis Test Since I checked various assumptions, and the assumption of normality was violated, I decided to go with a non parametric Kruskal Wallis Test to test my hypothesis that there is a different mean total cholesterol value between the 5 different racial/hispanic origin categories.\n1 2 options(scipen = 999) kruskal.test(total_cholesterol ~ race, data = cholesterol) 1 2 3 4 5 6 ## ## Kruskal-Wallis rank sum test ## ## data: total_cholesterol by race ## Kruskal-Wallis chi-squared = 60.582, df = 4, p-value = ## 0.000000000002188 The p value is very very small, meaning there is one or more categories that have a mean total cholesterol that differs from the other. To check which groups are causing there to be a statisticially signifigant difference, I\u0026rsquo;ll run a pairwise comparison.\nPairwise Comparison Test 1 2 pairwise.wilcox.test(cholesterol$total_cholesterol, cholesterol$race, p.adjust.method = \u0026#34;BH\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 ## ## Pairwise comparisons using Wilcoxon rank sum test with continuity correction ## ## data: cholesterol$total_cholesterol and cholesterol$race ## ## Asian Black Hispanic Other ## Black 0.0000000000011 - - - ## Hispanic 0.00096 0.0000031636325 - - ## Other 0.01722 0.01722 0.77812 - ## White 0.0000031636325 0.00069 0.12485 0.61744 ## ## P value adjustment method: BH Looks like there are multiple groups in which a statistically significant difference can be observed, in fact it is present in all except Other and Hispanic, White and Hispanic, and Other and White.\nConclusion I was always hesitant to use the CDC\u0026rsquo;s NHANES dataset as it is so big and there are so many different variables but I\u0026rsquo;m glad that I found the nhanesA package as that made it so easy to do this analysis, and I\u0026rsquo;ll definitely be using it again to avoid having to dig for datasets and also having to download gigantic ones to my poor old computer lol.\nI also learned some other things during this analysis, in terms of R skills I learned about the arsenal package that builds tables to display the various categories that were pre-existing in the data, as well as the categories that I created using the cut() function based on the typical ranges used in medicine for total cholesterol.\nIn terms of statistics, I still have some confusion about the central limit theorem and normal distributions and if you can actually use parametric tests on non normal distributions, as there seems to be a lot of heated debate over this on the various statistics forums that I visited in hope of getting an answer to this question. I decided to play it safe and use a non parametric test, but I do want to learn more about how and when you can violate assumptions if you should at all.\n","date":"2024-01-12T00:00:00Z","image":"https://michelleyg1.github.io/p/total-cholesterol-cdc-nhanes-analysis/images/purple_hu3d03a01dcc18bc5be0e67db3d8d209a6_1915810_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/total-cholesterol-cdc-nhanes-analysis/","title":"Total Cholesterol CDC NHANES Analysis"},{"content":"I found this dataset by filtering through the various settings that they have on the National Agriculture Statistics Service quick stats tool to see the condition of blueberries by year and week in the state of New Jersey.\nA lot of people think that New Jersey is only the city and the shore, but its not called the Garden State for nothing! For its small size, New Jersey punches above its weight class in producing and exporting various agricultural products.\nOne crop that New Jersey is known for is its blueberries.\nLoad Package 1 library(tidyverse) 1 2 3 4 5 6 7 8 9 10 ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors 1 library(janitor) 1 2 3 4 5 6 ## ## Attaching package: \u0026#39;janitor\u0026#39; ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## chisq.test, fisher.test Import Dataset 1 blueberries \u0026lt;- read_csv(\u0026#34;/Users/michellegulotta/Desktop/my_first_project/blueberries/blueberry.csv\u0026#34;) 1 2 3 4 5 6 7 8 9 10 ## Rows: 225 Columns: 21 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026#34;,\u0026#34; ## chr (9): Program, Period, Geo Level, State, watershed_code, Commodity, Data... ## dbl (3): Year, State ANSI, Value ## lgl (8): Ag District, Ag District Code, County, County ANSI, Zip Code, Regi... ## date (1): Week Ending ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 1 2 3 4 5 6 7 8 blueberries \u0026lt;- blueberries %\u0026gt;% janitor::clean_names(., \u0026#34;snake\u0026#34;) %\u0026gt;% select(year, period, week_ending, data_item, value) %\u0026gt;% rename( condition = data_item, percent = value, week_number = period ) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 225 × 5 ## year week_number week_ending condition percent ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## 2 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## 3 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 100 ## 4 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## 5 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## 6 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 18 ## 7 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 19 ## 8 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 44 ## 9 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 19 ## 10 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## # ℹ 215 more rows Cleaning Data Clean up Condition and Week Number Column I wanted to clean up the condition column as I noticed it was pretty redundant, and all we really needed was the condition. This would also make it eaiser as this is going to be the legend on our graph. Another thing that I wanted to do was to make the week number column more simple, as the name of the variable already tells us that this is the week number, so all we really need in the observation is the number.\n1 2 blueberries$condition \u0026lt;- str_replace_all(blueberries$condition, \u0026#34;BLUEBERRIES, TAME - CONDITION, MEASURED IN PCT\u0026#34;, \u0026#34;\u0026#34;) blueberries$week_number \u0026lt;- str_replace_all(blueberries$week_number, \u0026#34;WEEK #\u0026#34;, \u0026#34;\u0026#34;) Checking to make sure that the different conditions look good\n1 unique(blueberries$condition) 1 ## [1] \u0026#34; EXCELLENT\u0026#34; \u0026#34; FAIR\u0026#34; \u0026#34; GOOD\u0026#34; \u0026#34; POOR\u0026#34; \u0026#34; VERY POOR\u0026#34; It looks like there\u0026rsquo;s some random leading and trailing white space, so I\u0026rsquo;m going to clean that up using the trimws() function, as well as the str_to_title() function to make it more readable and look nicer on our graph\n1 2 blueberries$condition \u0026lt;- trimws(blueberries$condition) blueberries$condition \u0026lt;- str_to_title(blueberries$condition) Checking the conditions again\n1 unique(blueberries$condition) 1 ## [1] \u0026#34;Excellent\u0026#34; \u0026#34;Fair\u0026#34; \u0026#34;Good\u0026#34; \u0026#34;Poor\u0026#34; \u0026#34;Very Poor\u0026#34; Looks good! Time to make a data visualization\nData Visualization 1 2 3 4 5 6 blueberries_2021 \u0026lt;- blueberries %\u0026gt;% filter(year == 2021) %\u0026gt;% mutate( condition_f = factor(condition) ) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 55 × 6 ## year week_number week_ending condition percent condition_f ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 2021 25 2021-06-27 Excellent 25 Excellent ## 2 2021 25 2021-06-27 Fair 25 Fair ## 3 2021 25 2021-06-27 Good 50 Good ## 4 2021 25 2021-06-27 Poor 0 Poor ## 5 2021 25 2021-06-27 Very Poor 0 Very Poor ## 6 2021 26 2021-07-04 Excellent 84 Excellent ## 7 2021 26 2021-07-04 Fair 8 Fair ## 8 2021 26 2021-07-04 Good 8 Good ## 9 2021 26 2021-07-04 Poor 0 Poor ## 10 2021 26 2021-07-04 Very Poor 0 Very Poor ## # ℹ 45 more rows Factor to Change Order of Legend One of the problems that I came across while trying to make this graph was that the legend would appear in alphabetical order rather than in the order that made sense from excellent to very poor. After some trying and researching solutions, I realized that I could use the factor() function to change the order of the levels.\n1 2 blueberries_2021$condition_f \u0026lt;- factor(blueberries_2021$condition_f, levels = c(\u0026#34;Excellent\u0026#34;, \u0026#34;Good\u0026#34;, \u0026#34;Fair\u0026#34;, \u0026#34;Poor\u0026#34;, \u0026#34;Very Poor\u0026#34;)) Make Plot 1 2 3 4 5 6 7 8 9 10 11 ggplot(blueberries_2021, aes(x = week_ending, y = percent, fill = condition_f)) + geom_area(alpha = 0.5, position = \u0026#34;identity\u0026#34;) + scale_fill_manual(values = c(\u0026#34;#785EF0\u0026#34;, \u0026#34;#009E73\u0026#34;, \u0026#34;#FFB000\u0026#34;, \u0026#34;#FE6100\u0026#34;, \u0026#34;#DC267F\u0026#34;)) + labs(title = \u0026#34;Condition of Blueberries Measured in Percent in New Jersey in 2021\u0026#34;, x = \u0026#34;Week\u0026#34;, y = \u0026#34;Percent\u0026#34;, fill = \u0026#34;Condition\u0026#34;, caption = \u0026#34;Source: USDA National Agrigultural Statistics Service\u0026#34;) + theme_light() Conclusion Now it\u0026rsquo;s the beginning of January but this has me looking forward to the beginning of July!\nWorking on this specific visualization helped me work on my data cleaning skills. I also learned more about how factors work and how R automatically puts a character vector in alphabetical order, and that in order to get it to appear in the order you want on a legend, you have to change the order of the factor variable. I didn\u0026rsquo;t think about that before I started but it definitely makes sense, R doesn\u0026rsquo;t just know how humans rank the quality of blueberries.\nI also learned about geom_area(), I first tried to use geom_line() and was able to make that pretty easily, but when I tried to change it to geom_area() there was a lot that I had to change in order to get the effect that I wanted on my graph.\n","date":"2024-01-07T00:00:00Z","image":"https://michelleyg1.github.io/p/jersey-blueberries/images/blueberry_hu3d03a01dcc18bc5be0e67db3d8d209a6_1579336_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/jersey-blueberries/","title":"Jersey Blueberries"},{"content":"I found this interesting dataset on Kaggle and wanted to do an exploratory analysis as I also have had asthma since I was a kid and wanted to see if I could find any interesting patterns within the data.\nLoad Packages 1 2 3 4 suppressPackageStartupMessages({ library(tidyverse) library(janitor) }) Import Dataset 1 2 3 asthma \u0026lt;- read.csv(\u0026#34;/Users/michellegulotta/Desktop/my_first_project/asthma/CDIAsthmaByStateTransposed2010-2020.csv\u0026#34;) asthma \u0026lt;- asthma %\u0026gt;% janitor::clean_names(., \u0026#34;snake\u0026#34;) Taking a look at the first few rows of the data to see what kind of information this dataset is providing\n1 head(asthma) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## geo_loc year state pop_est ## 1 POINT (-86.63186076199969 32.84057112200048) 2010 Alabama 4785514 ## 2 POINT (-147.72205903599973 64.84507995700051) 2010 Alaska 713982 ## 3 POINT (-111.76381127699972 34.865970280000454) 2010 Arizona 6407342 ## 4 POINT (-92.27449074299966 34.74865012400045) 2010 Arkansas 2921998 ## 5 POINT (-120.99999953799971 37.63864012300047) 2010 California 37319550 ## 6 POINT (-106.13361092099967 38.843840757000464) 2010 Colorado 5047539 ## f_fatal m_fatal o_fatal f_er m_er o_er f_hosp m_hosp o_hosp ## 1 44 17 61 NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA ## 3 31 27 58 22280 18335 40650 4717 3352 8073 ## 4 29 8 37 NA NA NA 1876 962 2841 ## 5 251 152 403 116571 96999 217637 19399 12958 34583 ## 6 33 15 48 NA NA NA 2385 1942 4336 Asthma Fatalities by Gender in New Jersey Data Visualization Clean Up Data Since its my home state, I decided to narrow the data that I wanted to look at down a bit to just New Jersey. I also only was interested in looking at fatalities for this particular graph as it is the most severe type of asthma incident recorded in this data.\nThe first thing that I needed to do was to pivot the data so that gender was its own observation, I originally missed out on this step when I was trying to make the graph and had a hard time coming up with the ggplot code as I was just repeating adding a different shape to my graph for each column. After a bit of trial and error I realized there was probably a way to do it where I wouldn’t have to add the columns individually as their own geoms.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 nj_asthma_fatalities \u0026lt;- asthma %\u0026gt;% filter(state == \u0026#34;New Jersey\u0026#34;) %\u0026gt;% select(year, ends_with(\u0026#34;_fatal\u0026#34;)) %\u0026gt;% pivot_longer( cols = ends_with(\u0026#34;_fatal\u0026#34;), names_to = \u0026#34;gender\u0026#34;, values_to = \u0026#34;fatalities\u0026#34;, names_pattern = \u0026#34;(.*)_fatal\u0026#34; ) %\u0026gt;% mutate( gender = factor(gender, c(\u0026#34;f\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;o\u0026#34;), c(\u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;, \u0026#34;Overall\u0026#34;)) ) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 33 × 3 ## year gender fatalities ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Female 49 ## 2 2010 Male 31 ## 3 2010 Overall 80 ## 4 2011 Female 62 ## 5 2011 Male 32 ## 6 2011 Overall 94 ## 7 2012 Female 63 ## 8 2012 Male 40 ## 9 2012 Overall 103 ## 10 2013 Female 73 ## # ℹ 23 more rows Create a Graph Now to take my newly pivoted data and create a graph using the ggplot2 package:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ggplot(data = nj_asthma_fatalities) + geom_smooth(mapping = aes( x = year, y = fatalities, group = gender, color = gender), se = FALSE) + scale_color_manual(values = c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;black\u0026#34;)) + scale_x_continuous(n.breaks = 10) + labs(title = \u0026#34;Asthma Fatalities in New Jersey by Gender from 2010 to 2020\u0026#34;, x = \u0026#34;Years\u0026#34;, y = \u0026#34;Number of Fatalities\u0026#34;) + theme(plot.title = element_text(hjust=0.5)) + theme_bw() 1 ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula = \u0026#39;y ~ x\u0026#39; This graph shows an interesting trend that at least in the state of New Jersey over the 2010s decade asthma fatality incidents increased and then decreased, just to increase once again around 2017. Also, another trend that caught my eye was that that females made up the majority of overall fatalities that were recorded.\nAs these are not proportional to the population, they do not tell us the whole story. I’m interested in seeing if the mortality rates show the same pattern as anyone living in New Jersey can tell you that the population has increased over this past decade just from the traffic alone, does the population change account for the increase in asthma fatality incidents?\nCalculate Mortality Rate per 100,000 People Column I calculated the cause specific mortality rate per 100,000 people in the whole population of the state.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 nj_asthma_mortality_rate \u0026lt;- asthma %\u0026gt;% filter(state == \u0026#34;New Jersey\u0026#34;) %\u0026gt;% select(year, pop_est, ends_with(\u0026#34;_fatal\u0026#34;)) %\u0026gt;% pivot_longer( cols = ends_with(\u0026#34;_fatal\u0026#34;), names_to = \u0026#34;gender\u0026#34;, values_to = \u0026#34;fatalities\u0026#34;, names_pattern = \u0026#34;(.*)_fatal\u0026#34; ) %\u0026gt;% mutate( gender = factor(gender, c(\u0026#34;f\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;o\u0026#34;), c(\u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;, \u0026#34;Overall\u0026#34;)), mortality_rate = round(((fatalities / pop_est) * 100000), 2) ) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 33 × 5 ## year pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 8799451 Female 49 0.56 ## 2 2010 8799451 Male 31 0.35 ## 3 2010 8799451 Overall 80 0.91 ## 4 2011 8828552 Female 62 0.7 ## 5 2011 8828552 Male 32 0.36 ## 6 2011 8828552 Overall 94 1.06 ## 7 2012 8845671 Female 63 0.71 ## 8 2012 8845671 Male 40 0.45 ## 9 2012 8845671 Overall 103 1.16 ## 10 2013 8857821 Female 73 0.82 ## # ℹ 23 more rows Create a Graph of Asthma Mortality Rate per 100,000 People and Group By Gender Then I used ggplot2 to graph this new column to compare the mortality rate per 100,000 people over the decade of 2010 to 2020 to see if the same trend emerges:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ggplot(data = nj_asthma_mortality_rate) + geom_smooth(mapping = aes(x = year, y = mortality_rate, group = gender, color = gender), se = FALSE) + scale_color_manual(values = c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;black\u0026#34;)) + scale_x_continuous(n.breaks = 10) + labs(title = \u0026#34;Asthma Mortality Rate in New Jersey by Gender from 2010 to 2020\u0026#34;, x = \u0026#34;Years\u0026#34;, y = \u0026#34;Number of Fatalities\u0026#34;) + theme(plot.title = element_text(hjust=0.5)) + theme_bw() 1 ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula = \u0026#39;y ~ x\u0026#39; Interesting, the graphs look pretty much the same to me so the population increase is not the reason for the increase in asthma fatality incidents as the mortality rate from asthma follows the same pattern as the fatality incidents over time, as well as the gender disparity.\nNationwide Analysis of Gender Differences In Asthma Mortality Rate Clean Up Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 asthma_mortality \u0026lt;- asthma %\u0026gt;% select(year, state, pop_est, f_fatal, m_fatal) %\u0026gt;% pivot_longer( cols = ends_with(\u0026#34;_fatal\u0026#34;), names_to = \u0026#34;gender\u0026#34;, values_to = \u0026#34;fatalities\u0026#34;, names_pattern = \u0026#34;(.*)_fatal\u0026#34; ) %\u0026gt;% mutate( gender = factor(gender, c(\u0026#34;f\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;o\u0026#34;), c(\u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;, \u0026#34;Overall\u0026#34;)), mortality_rate = round(((fatalities / pop_est) * 100000), 2) ) %\u0026gt;% filter(gender != \u0026#34;Overall\u0026#34;) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 1,122 × 6 ## year state pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Alabama 4785514 Female 44 0.92 ## 2 2010 Alabama 4785514 Male 17 0.36 ## 3 2010 Alaska 713982 Female NA NA ## 4 2010 Alaska 713982 Male NA NA ## 5 2010 Arizona 6407342 Female 31 0.48 ## 6 2010 Arizona 6407342 Male 27 0.42 ## 7 2010 Arkansas 2921998 Female 29 0.99 ## 8 2010 Arkansas 2921998 Male 8 0.27 ## 9 2010 California 37319550 Female 251 0.67 ## 10 2010 California 37319550 Male 152 0.41 ## # ℹ 1,112 more rows I noticed there were a decent amount of missing values just by glancing at this lets see how many exactly\n1 sum(is.na(asthma_mortality$mortality_rate)) 1 ## [1] 264 Hm, 264/1122 that is not that bad, also it does look like they\u0026rsquo;re states with limited populations, so that might be the reason is that there just weren\u0026rsquo;t any fatal asthma incidents in that particular year.\nI\u0026rsquo;m going to remove the missing values for our next step in this analysis\n1 2 3 asthma_mortality_no_miss \u0026lt;- asthma_mortality %\u0026gt;% drop_na(mortality_rate) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 858 × 6 ## year state pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Alabama 4785514 Female 44 0.92 ## 2 2010 Alabama 4785514 Male 17 0.36 ## 3 2010 Arizona 6407342 Female 31 0.48 ## 4 2010 Arizona 6407342 Male 27 0.42 ## 5 2010 Arkansas 2921998 Female 29 0.99 ## 6 2010 Arkansas 2921998 Male 8 0.27 ## 7 2010 California 37319550 Female 251 0.67 ## 8 2010 California 37319550 Male 152 0.41 ## 9 2010 Colorado 5047539 Female 33 0.65 ## 10 2010 Colorado 5047539 Male 15 0.3 ## # ℹ 848 more rows Create a Histogram of Fatal Asthma Incidents for Each Gender I then decided to create a histogram to look at how these asthma fatalities are distributed.\nI\u0026rsquo;m going to use ggplot2 to make a histogram comparing the distribution of mortality rates between the two populations\n1 2 3 4 5 6 ggplot(data = asthma_mortality_no_miss, aes(x = mortality_rate, fill = gender)) + geom_histogram(color = \u0026#34;black\u0026#34;) + scale_fill_manual(values=c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;)) + labs(title = \u0026#34;Asthma Mortality Rate by Gender in the USA from 2010 to 2020\u0026#34;, x = \u0026#34;Mortality Rate\u0026#34;, y= \u0026#34;Number of Observations\u0026#34;) 1 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. It looks like there are some outliers, let me take a look at the data sorted by mortality rate descending\n1 asthma_mortality_no_miss[order(asthma_mortality_no_miss$mortality_rate, decreasing = TRUE),] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 858 × 6 ## year state pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2020 New Mexico 211839 Female 26 12.3 ## 2 2020 Pennsylvania 1299444 Female 97 7.46 ## 3 2020 Illinois 1278658 Male 89 6.96 ## 4 2020 Illinois 1278658 Female 88 6.88 ## 5 2020 New Mexico 211839 Male 13 6.14 ## 6 2020 Pennsylvania 1299444 Male 69 5.31 ## 7 2014 Mississippi 2991892 Female 39 1.3 ## 8 2016 Mississippi 2990595 Female 38 1.27 ## 9 2014 Oregon 3965447 Female 50 1.26 ## 10 2014 Iowa 3110643 Female 38 1.22 ## # ℹ 848 more rows Wow, it looks like in 2020 there was a huge increase due to the pandemic most likely. I\u0026rsquo;m going to take a look at the data from 2010-2019 to get a closer look at the distribution.\n1 2 3 asthma_mortality_drop_2020 \u0026lt;- asthma_mortality_no_miss %\u0026gt;% filter(year != 2020) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 780 × 6 ## year state pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Alabama 4785514 Female 44 0.92 ## 2 2010 Alabama 4785514 Male 17 0.36 ## 3 2010 Arizona 6407342 Female 31 0.48 ## 4 2010 Arizona 6407342 Male 27 0.42 ## 5 2010 Arkansas 2921998 Female 29 0.99 ## 6 2010 Arkansas 2921998 Male 8 0.27 ## 7 2010 California 37319550 Female 251 0.67 ## 8 2010 California 37319550 Male 152 0.41 ## 9 2010 Colorado 5047539 Female 33 0.65 ## 10 2010 Colorado 5047539 Male 15 0.3 ## # ℹ 770 more rows And repeat the histogram process with the 2010 to 2019 data\n1 2 3 4 5 6 ggplot(data = asthma_mortality_drop_2020, aes(x = mortality_rate, fill = gender)) + geom_histogram(color = \u0026#34;black\u0026#34;) + scale_fill_manual(values=c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;)) + labs(title = \u0026#34;Asthma Mortality Rate by Gender in the USA from 2010 to 2019\u0026#34;, x = \u0026#34;Mortality Rate\u0026#34;, y= \u0026#34;Number of Observations\u0026#34;) 1 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. I want to compare the means of these populations to see if there is a statistically significant difference in fatal asthma incidence between the two genders throughout the whole USA.\nStatistical Testing From the histogram, these don\u0026rsquo;t really seem to fit a normal distribution, but rather a skewed distribution, but let\u0026rsquo;s run the Shapiro-Wilk test to be sure instead of just eyeballing it and assuming.\n1 2 options(scipen = 999) shapiro.test(asthma_mortality_no_miss$mortality_rate) 1 2 3 4 5 ## ## Shapiro-Wilk normality test ## ## data: asthma_mortality_no_miss$mortality_rate ## W = 0.27179, p-value \u0026lt; 0.00000000000000022 Okay, since the p value is not greater than 0.05, I can\u0026rsquo;t use a t-test as that assumes normal distribution.\nI\u0026rsquo;m going to go for a non-parametric test since we\u0026rsquo;re not assuming any particular distribution here to test my hypothesis that when it comes to asthma fatalities females have a higher mortality rate than males. Since I have two unpaired samples and I want to test how their values compare, I\u0026rsquo;m going to use the Mann-Whitney test.\n1 wilcox.test(asthma_mortality_no_miss$mortality_rate) 1 2 3 4 5 6 ## ## Wilcoxon signed rank test with continuity correction ## ## data: asthma_mortality_no_miss$mortality_rate ## V = 368511, p-value \u0026lt; 0.00000000000000022 ## alternative hypothesis: true location is not equal to 0 With these results we can reject the null hypothesis, and say within this data there is a statistically significant difference between the two populations.\nConclusion Before this analysis I had no idea of the gender differences that arise in asthma incidents, upon looking into this further after completing this analysis I came across a paper that discusses the gender differences in asthma prevalence and severity on a biological level.\nAccording to the Asthma and Allergy Network, In people under 18, it is more common for boys to have asthma than girls, but this prevalence switches when analyzing adult populations. The fact that women have a higher risk of death from asthma when compared to men is also confirmed by this source.\n","date":"2024-01-03T00:00:00Z","image":"https://michelleyg1.github.io/p/asthma-incidents-analysis/images/featured_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/asthma-incidents-analysis/","title":"Asthma Incidents Analysis"}]