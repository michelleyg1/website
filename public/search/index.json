[{"content":"For this post I wanted to check out other publicly available large data sets that have a package associated with it so that I wouldn\u0026rsquo;t have to download it locally as I generally try to conserve space on my desktop and keep everything organized. I saw that the WHO Global Health Observatory had exactly what I was looking for and decided to explore some of the data that they had available.\nLoad Libraries 1 2 3 4 suppressPackageStartupMessages({ library(tidyverse) library(rgho) }) View and Import Data With this package there are vignettes available to search the different dimensions/data sets that can be pulled using the rgho package. I was able to access it by running this code.\n1 2 3 4 # Look at Dimensions vignette(\u0026#34;b-dimensions\u0026#34;, \u0026#34;rgho\u0026#34;) # Look at GHO values vignette(\u0026#34;c-values-gho\u0026#34;, \u0026#34;rgho\u0026#34;) As there are several data sets that need to be imported, I wrote a simple function to grab the data that I want, select the variables that I want, and filter for columns where region was present that way that I can see the totals per region, rather than per country.\n1 2 3 4 5 gho \u0026lt;- function(data, code = \u0026#34;CODE\u0026#34;) { data \u0026lt;- get_gho_data(code = code) %\u0026gt;% dplyr::select(REGION, YEAR, NumericValue) %\u0026gt;% filter(!is.na(REGION)) } And then I used this function to import the data.\n1 2 3 4 5 6 7 8 9 hiv \u0026lt;- gho(hiv, code = \u0026#34;HIV_0000000001\u0026#34;) %\u0026gt;% rename(hiv = NumericValue) art \u0026lt;- gho(art, code = \u0026#34;HIV_0000000009\u0026#34;) %\u0026gt;% rename(art = NumericValue) art_pct \u0026lt;- gho(art_pct, code = \u0026#34;HIV_ARTCOVERAGE\u0026#34;) %\u0026gt;% rename(art_pct = NumericValue) region \u0026lt;- get_gho_values(dimension = \u0026#34;REGION\u0026#34;) %\u0026gt;% rename(REGION = Code, region = Title) And then to merge the data sets.\n1 2 3 4 5 dfs \u0026lt;- list(hiv, art, art_pct) df \u0026lt;- dfs %\u0026gt;% reduce(full_join, by = c(\u0026#34;REGION\u0026#34;, \u0026#34;YEAR\u0026#34;)) dff \u0026lt;- full_join(df, region, by = \u0026#34;REGION\u0026#34;) As I made some variables lower case already, I use the tolower() function to standardize the case of all of the variables.\n1 2 3 dff \u0026lt;- dff %\u0026gt;% dplyr::select(-REGION) names(dff) \u0026lt;- tolower(names(dff)) The variables in the dataset represent the following information:\nhiv: number of people of all ages living with HIV art: number of people receiving ART art_pct: art therapy coverage among people living with HIV Visualization People Living With HIV 1 2 3 4 5 6 7 8 9 10 11 ggplot(data = dff) + geom_smooth(mapping = aes(x = year, y = hiv, group = region, color = region)) + labs(title = \u0026#34;People Living with HIV by Region from 1990 to 2022\u0026#34;, y = \u0026#34;Percent\u0026#34;, x = \u0026#34;Year\u0026#34;, color = \u0026#34;Region\u0026#34;, caption = \u0026#34;Source: WHO Global Health Observatory\u0026#34;) + theme(plot.title = element_text(hjust = 0.5)) 1 ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula = \u0026#39;y ~ x\u0026#39; 1 2 ## Warning: Removed 34 rows containing non-finite outside the scale range ## (`stat_smooth()`). ART Coverage 1 2 3 4 5 6 7 8 9 10 11 12 ggplot(data = dff) + geom_smooth(mapping = aes(x = year, y = art_pct, group = region, color = region)) + labs(title = \u0026#34;Antiretroviral Therapy Coverage Among People Living with HIV by Region from 1990 to 2022\u0026#34;, y = \u0026#34;Percent\u0026#34;, x = \u0026#34;Year\u0026#34;, color = \u0026#34;Region\u0026#34;, caption = \u0026#34;Source: WHO Global Health Observatory\u0026#34;) + theme(plot.title = element_text(hjust = 0.5)) 1 ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula = \u0026#39;y ~ x\u0026#39; ART Coverage in 2022 1 2 3 4 5 6 7 8 9 10 11 dff_filtered \u0026lt;- dff %\u0026gt;% filter(year == 2022) ggplot(data = dff_filtered, aes(x = region, y = art_pct)) + geom_col(aes(fill = region)) + labs(title = \u0026#34;Antiretroviral Therapy Coverage Among People Living with HIV in 2022\u0026#34;, y = \u0026#34;Percent\u0026#34;, x = \u0026#34;Region\u0026#34;, caption = \u0026#34;Source: WHO Global Health Observatory\u0026#34;) + theme(legend.position = \u0026#34;none\u0026#34;, plot.title = element_text(hjust = 0.5)) Conclusion This was just a quick exploration of a small fraction of the data that is available through the rgho package, and it was useful to see how this works and how many different variables the Global Health Observatory tracks.\nIn taking a look at this data and creating visualizations, I was able to see how far we have come in providing antiretroviral therapy to those who have received an HIV diagnosis, but also that there is still a lot of work to be done in making sure that everyone is covered by the life saving treatment. There is also a large disparity between the rest of the world and the eastern Mediterranean region (which generally covers the Middle East and North Africa), so there should be work done to break down barriers to ART access focused in this region.\n","date":"2024-04-16T00:00:00Z","image":"https://michelleyg1.github.io/p/gho-hiv/images/red_hu5459c0360c2b0cb7a147d2df0eb350ca_5061373_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/gho-hiv/","title":"HIV and Antiretroviral Therapy Across the Globe"},{"content":"I finally decided to search for data outside of the public health and biostatistics realm and looked into the type of data that NJ Transit (the main public transit system in New Jersey) collected and published online to be viewed and analyzed by the public. I chose to analyze the system wide bus system performance data.\nFirst to load some libraries.\nLoad Libraries 1 2 3 4 5 6 suppressPackageStartupMessages({ library(tidyverse) library(lubridate) library(FSA) library(rcompanion) }) Import Data I\u0026rsquo;m just going to do a simple import from my desktop.\n1 bus \u0026lt;- read_csv(\u0026#34;/Users/michellegulotta/Desktop/BUS_OTP_DATA.csv\u0026#34;) 1 2 3 4 5 6 7 ## Rows: 182 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026#34;,\u0026#34; ## chr (5): OTP_YEAR, OTP_MONTH, OTP, TOTAL_TRIPS, TOTAL_LATES ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Clean Data Data found in the wild, collected by government agencies (in my experience) is generally pretty ugly and needs some cleaning up, this one wasn\u0026rsquo;t really that bad though, just some changing the data types and removing the first row as it just was a line that seperated the variable names from the data on the csv file.\nI also created some variables. The date variable which uses the lubridate package function make_date(), the season variable using the months of the year, pct_late which reflects the percent of busses in that particular year and month combination that were recorded to have arrived late to the bus stop.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 bus \u0026lt;- bus[-1, ] names(bus) \u0026lt;- tolower(names(bus)) bus \u0026lt;- bus %\u0026gt;% mutate( across( .cols = c(1:5), .fns = ~as.numeric(.x) ), across( .cols = c(1, 2), .fns = ~as.factor(.x), .names = \u0026#34;{col}_f\u0026#34; ), pct_late = (total_lates / total_trips) * 100, date = make_date(otp_year, otp_month), season = as.factor(case_when( otp_month \u0026lt; 4 ~ \u0026#34;Winter\u0026#34;, otp_month \u0026lt; 7 ~ \u0026#34;Spring\u0026#34;, otp_month \u0026lt; 10 ~ \u0026#34;Summer\u0026#34;, otp_month \u0026gt;= 10 ~ \u0026#34;Fall\u0026#34; )) ) Percent of Busses Arriving Late from 2009 to 2024 I\u0026rsquo;m going to use ggplot to look at how the pct_late variable has changed over the years that this data has been recorded.\n1 2 3 4 5 6 7 8 9 ggplot(data = bus, mapping = aes(x = date, y = pct_late)) + geom_smooth(fill = \u0026#34;#F4823C\u0026#34;, color = \u0026#34;#04529C\u0026#34;, alpha = 0.2) + scale_x_date(breaks = \u0026#34;year\u0026#34;, date_labels = \u0026#34;%Y\u0026#34;) + labs(title = \u0026#34;Percent of NJ Transit Busses that Arrived Late from 2009 to 2024\u0026#34;, x = \u0026#34;Year\u0026#34;, y = \u0026#34;Percent\u0026#34;, caption = \u0026#34;Source: NJ Transit Performance Data\u0026#34;) + theme_linedraw() + theme(plot.title = element_text(hjust=0.5)) 1 ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula = \u0026#39;y ~ x\u0026#39; These differences look pretty drastic, but ggplot did scale down the y axis quite a bit, so we are looking at a tiny fraction of the variable (under 10%).\nLinear Regression I\u0026rsquo;m going to use the factored year variable to treat the year as a category, and see if there were any years associated with increased or decreased percent of busses arriving late. Looking at the graph above there are certainly ups and downs when the y axis is scaled down.\n1 2 bus.fit \u0026lt;- lm(pct_late ~ otp_year_f, bus) summary(bus.fit) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ## ## Call: ## lm(formula = pct_late ~ otp_year_f, data = bus) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0887 -0.8516 -0.0588 0.8064 5.3810 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 6.50673 0.44010 14.785 \u0026lt; 2e-16 *** ## otp_year_f2010 -0.02779 0.62240 -0.045 0.964444 ## otp_year_f2011 1.46707 0.62240 2.357 0.019591 * ## otp_year_f2012 2.93640 0.62240 4.718 5.05e-06 *** ## otp_year_f2013 2.46509 0.62240 3.961 0.000111 *** ## otp_year_f2014 2.65628 0.62240 4.268 3.32e-05 *** ## otp_year_f2015 2.67063 0.62240 4.291 3.02e-05 *** ## otp_year_f2016 2.60015 0.62240 4.178 4.76e-05 *** ## otp_year_f2017 3.14401 0.62240 5.051 1.15e-06 *** ## otp_year_f2018 2.83756 0.62240 4.559 9.97e-06 *** ## otp_year_f2019 1.16573 0.62240 1.873 0.062844 . ## otp_year_f2020 -4.06767 0.62240 -6.535 7.56e-10 *** ## otp_year_f2021 -2.45210 0.62240 -3.940 0.000120 *** ## otp_year_f2022 -1.29334 0.62240 -2.078 0.039259 * ## otp_year_f2023 1.15551 0.62240 1.857 0.065160 . ## otp_year_f2024 0.02389 1.58682 0.015 0.988008 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.525 on 165 degrees of freedom ## Multiple R-squared: 0.6778,\tAdjusted R-squared: 0.6485 ## F-statistic: 23.14 on 15 and 165 DF, p-value: \u0026lt; 2.2e-16 The coefficient that stood out the most to me was the one associated with the year 2020, with a very large decrease in pct_late it is interesting to see how the pandemic affected these metrics. Another interesting observation was that during the years that Chris Christie was governor of New Jersey, there were statistically significant increases from the 2009 intercept, and the year that he left office there was still an increase but only about half the size of the other years.\nDiagnostics 1 2 par(mfrow = c(2, 2)) plot(bus.fit) 1 2 ## Warning: not plotting observations with leverage one: ## 181 Besides the normality of the data, it looks okay. Season Associated with Late Busses Next, I wanted to see if there were any differences between the four seasons and percent of busses arriving late, I created the season variable when cleaning the data, and it does not capture it totally accurately as it was only based off of the month number, and there is some overlap in every month there is a season change, but the day to day data is not something that NJ transit collects.\nBox Plot I\u0026rsquo;m going to use ggplot again to make a box plot of the data which shows average percent of NJ Transit busses arriving late by season.\n1 2 3 4 5 6 7 8 9 ggplot(data = bus, mapping = aes(x = season, y = pct_late)) + geom_boxplot(fill = c(\u0026#34;#F4823C\u0026#34;, \u0026#34;#BC228C\u0026#34;, \u0026#34;#04529C\u0026#34;, \u0026#34;#F4F2F4\u0026#34;)) + labs(title = \u0026#34;Average Percent of NJ Transit Busses that Arrived Late from 2009 to 2024 by Season\u0026#34;, x = \u0026#34;Season\u0026#34;, y = \u0026#34;Average Percent of Busses Arriving Late\u0026#34;, caption = \u0026#34;Source: NJ Transit Performance Data\u0026#34;) + theme_linedraw() + theme(plot.title = element_text(hjust=0.5)) Kruskal Wallace Test Next, since the data is relatively non normal I\u0026rsquo;m going to use a non-parametric alternative to compare means.\n1 kruskal.test(pct_late ~ season, bus) 1 2 3 4 5 ## ## Kruskal-Wallis rank sum test ## ## data: pct_late by season ## Kruskal-Wallis chi-squared = 18.292, df = 3, p-value = 0.0003828 So there are statistically significant differences between the means at a very low p value.\nPost Hoc Comparisons Something else that I learned when researching for this post was that there is a non-parametric post hoc comparison that can be done to see within which groups the differences lie, called the Dunn test.\nHere, I utilize the functions in the FSA and rcompanion package to display the results of this test.\n1 2 3 4 5 bus.dunn \u0026lt;- dunnTest(pct_late ~ season, bus, method = \u0026#34;bh\u0026#34;) bus.dunn.res \u0026lt;- bus.dunn$res cldList(comparison = bus.dunn.res$Comparison, p.value = bus.dunn.res$P.adj, threshold = 0.05) 1 2 3 4 5 ## Group Letter MonoLetter ## 1 Fall a a ## 2 Spring ab ab ## 3 Summer ab ab ## 4 Winter b b As I thought from looking at the graph, the differences lie in the fall and winter average percentage.\nAverage Total Rides by Season Lastly, I wanted to take a quick look at the mean total trips as this may reflect a pattern in why busses tend to be later in the fall than in the winter, perhaps since there are less busses to be late in the winter?\n1 2 3 4 5 bus_mean \u0026lt;- bus %\u0026gt;% group_by(season) %\u0026gt;% summarise_at(vars(total_trips), list(mean_trips = mean)) print(bus_mean) 1 2 3 4 5 6 7 ## # A tibble: 4 × 2 ## season mean_trips ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Fall 41623. ## 2 Spring 41963. ## 3 Summer 44010. ## 4 Winter 41564. The season with the most rides is summer, and between winter there are almost the same amount of trips, so it is interesting that in the Fall they tend to be late more often than in the winter.\nPerhaps since all of December was counted in the Fall category, and it is influenced by the holidays. There is really not enough information in this dataset to be able to draw conclusions, so just speculating why this is the case.\nConclusion It was nice to take a step back from health related statistics for this analysis, especially away from survey data as I have been using that very often. This also allowed me to practice more with ggplot as I have been using base R plot syntax with the survey package for a lot of my posts recently.\nI\u0026rsquo;m interested to see how NJ transit collects this data and what is considered late for a bus, as this is an important issue as many New Jerseyans rely on public transit to get to work, appointments, and other important places. It will be interesting to repeat this analysis in a few years now that we are (for the most part) in a post pandemic, work from home world.\n","date":"2024-04-04T00:00:00Z","image":"https://michelleyg1.github.io/p/late-busses/images/bus_hu5459c0360c2b0cb7a147d2df0eb350ca_889232_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/late-busses/","title":"NJ Transit Bus Performance"},{"content":"I revisited the NHANES 2017 to pre pandemic 2020 data, as there are so many variables to analyse. For this post, I wanted to analyze the labs that were performed on participants of the Mobile Examination Center portion of the exam, particularly the heavy metal labs. According to the article Heavy Metals Toxicity and the Environment, \u0026ldquo;Because of their high degree of toxicity, arsenic, cadmium, chromium, lead, and mercury rank among the priority metals that are of public health significance. These metallic elements are considered systemic toxicants that are known to induce multiple organ damage, even at lower levels of exposure.\u0026rdquo;\nI wanted to take a look at not only the proportion of people who are at or above the detection limit, but if there are certain demographic variables that are correlated with higher amounts of heavy metals detected in the blood of participants.\nLoad Packages 1 2 3 4 5 6 7 suppressPackageStartupMessages({ library(tidyverse) library(nhanesA) library(survey) library(janitor) library(tableone) }) Import Data 1 2 3 4 demo \u0026lt;- nhanes(\u0026#34;P_DEMO\u0026#34;) %\u0026gt;% dplyr::select(SEQN, RIAGENDR, RIDAGEYR, RIDEXPRG, RIDRETH3, SDMVPSU, SDMVSTRA, WTMECPRP) lab \u0026lt;- nhanes(\u0026#34;P_PBCD\u0026#34;) %\u0026gt;% dplyr::select(SEQN, LBXBPB, LBDBPBLC, LBXBCD, LBDBCDLC, LBXTHG, LBDTHGLC) Merge Data 1 2 3 4 5 df \u0026lt;- merge(demo, lab, by = c(\u0026#34;SEQN\u0026#34;), all.y = TRUE) df$SEQN \u0026lt;- NULL Investigate and Clean Data 1 2 init.table \u0026lt;- CreateTableOne(data = df, includeNA = TRUE) print(init.table, showAllLevels = TRUE) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 ## ## level ## n ## RIAGENDR (%) Male ## Female ## RIDAGEYR (mean (SD)) ## RIDEXPRG (%) Yes, positive lab pregnancy test or self-reported pregnant at exam ## The participant was not pregnant at exam ## Cannot ascertain if the participant is pregnant at exam ## \u0026lt;NA\u0026gt; ## RIDRETH3 (%) Mexican American ## Other Hispanic ## Non-Hispanic White ## Non-Hispanic Black ## Non-Hispanic Asian ## Other Race - Including Multi-Racial ## SDMVPSU (mean (SD)) ## SDMVSTRA (mean (SD)) ## WTMECPRP (mean (SD)) ## LBXBPB (mean (SD)) ## LBDBPBLC (%) At or above the detection limit ## Below lower detection limit ## \u0026lt;NA\u0026gt; ## LBXBCD (mean (SD)) ## LBDBCDLC (%) At or above the detection limit ## Below lower detection limit ## \u0026lt;NA\u0026gt; ## LBXTHG (mean (SD)) ## LBDTHGLC (%) At or above the detection limit ## Below lower detection limit ## \u0026lt;NA\u0026gt; ## ## Overall ## n 13772 ## RIAGENDR (%) 6818 (49.5) ## 6954 (50.5) ## RIDAGEYR (mean (SD)) 35.16 (24.73) ## RIDEXPRG (%) 87 ( 0.6) ## 1604 (11.6) ## 59 ( 0.4) ## 12022 (87.3) ## RIDRETH3 (%) 1763 (12.8) ## 1373 (10.0) ## 4565 (33.1) ## 3688 (26.8) ## 1483 (10.8) ## 900 ( 6.5) ## SDMVPSU (mean (SD)) 1.54 (0.54) ## SDMVSTRA (mean (SD)) 160.36 (6.95) ## WTMECPRP (mean (SD)) 23148.77 (27878.70) ## LBXBPB (mean (SD)) 1.03 (1.16) ## LBDBPBLC (%) 11103 (80.6) ## 4 ( 0.0) ## 2665 (19.4) ## LBXBCD (mean (SD)) 0.36 (0.48) ## LBDBCDLC (%) 9720 (70.6) ## 2382 (17.3) ## 1670 (12.1) ## LBXTHG (mean (SD)) 1.07 (2.06) ## LBDTHGLC (%) 8296 (60.2) ## 3806 (27.6) ## 1670 (12.1) Here, I\u0026rsquo;m going to rename some variables to make the names more intuitive, as well as make the names of the categories more simple so that it is more readable when the output of the multivariate regression is printed.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 dff \u0026lt;- df %\u0026gt;% mutate( preg = as.factor(case_when( RIDEXPRG == \u0026#34;Yes, positive lab pregnancy test or self-reported pregnant at exam\u0026#34; ~ \u0026#34;Yes\u0026#34;, RIDEXPRG == \u0026#34;The participant was not pregnant at exam\u0026#34; ~ \u0026#34;No\u0026#34;, TRUE ~ NA)), race = as.factor(case_when( RIDRETH3 == \u0026#34;Mexican American\u0026#34; | RIDRETH3 == \u0026#34;Other Hispanic\u0026#34; ~ \u0026#34;Hispanic\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic White\u0026#34; ~ \u0026#34;White\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic Black\u0026#34; ~\u0026#34;Black\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic Asian\u0026#34; ~ \u0026#34;Asian\u0026#34;, RIDRETH3 == \u0026#34;Other Race - Including Multi-Racial\u0026#34; ~ \u0026#34;Other\u0026#34;)), gender = as.factor(RIAGENDR) ) %\u0026gt;% rename( age = RIDAGEYR, psu = SDMVPSU, strata = SDMVSTRA, weight = WTMECPRP, lead = LBXBPB, lead_lim = LBDBPBLC, cadmium = LBXBCD, cadmium_lim = LBDBCDLC, mercury = LBXTHG, mercury_lim = LBDTHGLC ) %\u0026gt;% dplyr::select(-RIDEXPRG, -RIDRETH3, -RIAGENDR) Checking again, making sure that I didn\u0026rsquo;t miss anything.\n1 2 clean.table \u0026lt;- CreateTableOne(data = dff, includeNA = TRUE) print(clean.table, showAllLevels = TRUE) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ## ## level Overall ## n 13772 ## age (mean (SD)) 35.16 (24.73) ## psu (mean (SD)) 1.54 (0.54) ## strata (mean (SD)) 160.36 (6.95) ## weight (mean (SD)) 23148.77 (27878.70) ## lead (mean (SD)) 1.03 (1.16) ## lead_lim (%) At or above the detection limit 11103 (80.6) ## Below lower detection limit 4 ( 0.0) ## \u0026lt;NA\u0026gt; 2665 (19.4) ## cadmium (mean (SD)) 0.36 (0.48) ## cadmium_lim (%) At or above the detection limit 9720 (70.6) ## Below lower detection limit 2382 (17.3) ## \u0026lt;NA\u0026gt; 1670 (12.1) ## mercury (mean (SD)) 1.07 (2.06) ## mercury_lim (%) At or above the detection limit 8296 (60.2) ## Below lower detection limit 3806 (27.6) ## \u0026lt;NA\u0026gt; 1670 (12.1) ## preg (%) No 1604 (11.6) ## Yes 87 ( 0.6) ## \u0026lt;NA\u0026gt; 12081 (87.7) ## race (%) Asian 1483 (10.8) ## Black 3688 (26.8) ## Hispanic 3136 (22.8) ## Other 900 ( 6.5) ## White 4565 (33.1) ## gender (%) Male 6818 (49.5) ## Female 6954 (50.5) Survey Design Creating the survey design object here.\n1 2 3 4 5 6 metal.svy \u0026lt;- svydesign(id = ~psu, strata = ~strata, weights = ~weight, nest = TRUE, survey.lonely.psu = \u0026#34;adjust\u0026#34;, data = dff) Proportions I want to use the variables that show if the participant was at or above the detection limit to take a look at the weighted proportions of the survey participants blood heavy metal concentrations, which reflects the population as a whole.\n1 svymean(~lead_lim, metal.svy, na.rm = TRUE) 1 2 3 ## mean SE ## lead_limAt or above the detection limit 0.99970726 2e-04 ## lead_limBelow lower detection limit 0.00029274 2e-04 1 svymean(~mercury_lim, metal.svy, na.rm = TRUE) 1 2 3 ## mean SE ## mercury_limAt or above the detection limit 0.72948 0.014 ## mercury_limBelow lower detection limit 0.27052 0.014 1 svymean(~cadmium_lim, metal.svy, na.rm = TRUE) 1 2 3 ## mean SE ## cadmium_limAt or above the detection limit 0.8448 0.0071 ## cadmium_limBelow lower detection limit 0.1552 0.0071 Wow, almost everyone is at or above the detection limit for lead, and a large majority for the other heavy metals.\nFit Multivariate Regressions First I want to take a subset of the survey design object to remove missing values for lab values for any of the heavy metals.\n1 2 3 4 metal.subset \u0026lt;- subset(metal.svy, !is.na(lead) \u0026amp; !is.na(cadmium) \u0026amp; !is.na(mercury)) Then, to fit the model using svyglm() with the subset of the survey design object.\n1 2 3 4 5 6 lead.fit \u0026lt;- svyglm(lead ~ gender + age + race, metal.subset) mercury.fit \u0026lt;- svyglm(mercury ~ gender + age + race, metal.subset) cadmium.fit \u0026lt;- svyglm(cadmium ~ gender + age + race, metal.subset) Diagnostics I\u0026rsquo;m going to take a quick look at the diagnostic plots for each model, to make sure that there is a linear relationship. I\u0026rsquo;m going to start with the lead model.\n1 2 par(mfrow = c(2, 2)) plot(lead.fit) And then mercury.\n1 2 par(mfrow = c(2, 2)) plot(mercury.fit) And lastly, cadmium.\n1 2 par(mfrow = c(2, 2)) plot(cadmium.fit) Most of the plots look good, there are departures from the normal distribution, but overall the relationship is linear. Now to take a look at the coefficients of the models.\nSignifigant Coefficients For lead,\n1 summary(lead.fit) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ## ## Call: ## svyglm(formula = lead ~ gender + age + race, design = metal.subset) ## ## Survey design: ## subset(metal.svy, !is.na(lead) \u0026amp; !is.na(cadmium) \u0026amp; !is.na(mercury)) ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.750177 0.058333 12.860 7.98e-11 *** ## genderFemale -0.268034 0.025504 -10.509 2.35e-09 *** ## age 0.016640 0.000466 35.705 \u0026lt; 2e-16 *** ## raceBlack -0.242977 0.054023 -4.498 0.000246 *** ## raceHispanic -0.308710 0.067818 -4.552 0.000218 *** ## raceOther -0.327197 0.062801 -5.210 4.98e-05 *** ## raceWhite -0.395893 0.058607 -6.755 1.88e-06 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.9845273) ## ## Number of Fisher Scoring iterations: 2 All significant statistically, some that stand out to me are that Non-Hispanic white is associated with the largest decrease when compared to the intercept which is Asian. Another is that the only positive coefficient is age although it is quite small. Female is also lower than male, which is interesting as you would think they would be the same from household exposures as men and women live together, perhaps there is another place that men were more frequently exposed to lead.\nAnd mercury,\n1 summary(mercury.fit) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ## ## Call: ## svyglm(formula = mercury ~ gender + age + race, design = metal.subset) ## ## Survey design: ## subset(metal.svy, !is.na(lead) \u0026amp; !is.na(cadmium) \u0026amp; !is.na(mercury)) ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 2.127359 0.300128 7.088 9.63e-07 *** ## genderFemale -0.172979 0.047452 -3.645 0.00172 ** ## age 0.015134 0.001399 10.819 1.46e-09 *** ## raceBlack -1.532638 0.298384 -5.136 5.87e-05 *** ## raceHispanic -1.582814 0.293034 -5.401 3.27e-05 *** ## raceOther -1.639871 0.297172 -5.518 2.53e-05 *** ## raceWhite -1.597285 0.287334 -5.559 2.32e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 3.284387) ## ## Number of Fisher Scoring iterations: 2 it seems to follow the same patterns as lead.\nAnd cadmium,\n1 summary(cadmium.fit) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ## ## Call: ## svyglm(formula = cadmium ~ gender + age + race, design = metal.subset) ## ## Survey design: ## subset(metal.svy, !is.na(lead) \u0026amp; !is.na(cadmium) \u0026amp; !is.na(mercury)) ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.2393745 0.0270553 8.848 3.64e-08 *** ## genderFemale 0.0748863 0.0116488 6.429 3.66e-06 *** ## age 0.0044088 0.0003573 12.338 1.62e-10 *** ## raceBlack -0.0203956 0.0282558 -0.722 0.47920 ## raceHispanic -0.1487407 0.0241972 -6.147 6.58e-06 *** ## raceOther 0.0074420 0.0360248 0.207 0.83854 ## raceWhite -0.0868630 0.0281021 -3.091 0.00601 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.2304773) ## ## Number of Fisher Scoring iterations: 2 it definitely deviates from the pattern seen in the other two model\u0026rsquo;s coefficients: Black and Other Race are not statistically significant coefficients, and surprisingly in this model Female has a positive coefficient instead of negative. I want to dig deeper into these gender differences.\nGender and Heavy Metal Concentration in Blood First I want to visualize the mean concentration for each type of heavy metal, in the two different gender groups.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 par(mfrow = c(1, 3), xpd = NA) svyboxplot(lead ~ gender, metal.subset, ylim = c(0, 3), xaxt = \u0026#34;n\u0026#34;, ylab = \u0026#34;Blood Lead (ug/dL)\u0026#34;, col = c(\u0026#34;#235347\u0026#34;, \u0026#34;#8EB69B\u0026#34;)) svyboxplot(mercury ~ gender, metal.subset, ylim = c(0, 4), xaxt = \u0026#34;n\u0026#34;, ylab = \u0026#34;Blood Mercury (ug/L)\u0026#34;, col = c(\u0026#34;#235347\u0026#34;, \u0026#34;#8EB69B\u0026#34;)) svyboxplot(cadmium ~ gender, metal.subset, ylim = c(0, 2), xaxt = \u0026#34;n\u0026#34;, ylab = \u0026#34;Blood Cadmium (ug/L)\u0026#34;, col = c(\u0026#34;#235347\u0026#34;, \u0026#34;#8EB69B\u0026#34;), bty = \u0026#34;L\u0026#34;) legend(\u0026#34;bottomleft\u0026#34;, legend = c(\u0026#34;Male\u0026#34;, \u0026#34;Female\u0026#34;), fill = c(\u0026#34;#235347\u0026#34;, \u0026#34;#8EB69B\u0026#34;), horiz = TRUE, inset = c(-1.5, -0.15), cex = 1.2) mtext(\u0026#34;Heavy Metal Concentration in Blood by Gender\u0026#34;, side = 3, line = -2.5, outer = TRUE) So there are some visible differences in the lead and cadmium group, and not as much within the mercury group. However, statistical significance can\u0026rsquo;t be determined just by looking at the graph.\nWilcoxon Signed Rank Test I\u0026rsquo;m going to use the Wilcoxon Signed Rank test rather than the parametric t-test as the data showed a deviation from the normal distribution in the q-q plot.\n1 svyranktest(lead ~ gender, metal.subset, test = c(\u0026#34;wilcoxon\u0026#34;)) 1 2 3 4 5 6 7 8 9 ## ## Design-based KruskalWallis test ## ## data: lead ~ gender ## t = -10.78, df = 24, p-value = 1.111e-10 ## alternative hypothesis: true difference in mean rank score is not equal to 0 ## sample estimates: ## difference in mean rank score ## -0.08826645 1 svyranktest(cadmium ~ gender, metal.subset, test = c(\u0026#34;wilcoxon\u0026#34;)) 1 2 3 4 5 6 7 8 9 ## ## Design-based KruskalWallis test ## ## data: cadmium ~ gender ## t = 12.499, df = 24, p-value = 5.349e-12 ## alternative hypothesis: true difference in mean rank score is not equal to 0 ## sample estimates: ## difference in mean rank score ## 0.1000802 1 svyranktest(mercury ~ gender, metal.subset, test = c(\u0026#34;wilcoxon\u0026#34;)) 1 2 3 4 5 6 7 8 9 ## ## Design-based KruskalWallis test ## ## data: mercury ~ gender ## t = -0.91282, df = 24, p-value = 0.3704 ## alternative hypothesis: true difference in mean rank score is not equal to 0 ## sample estimates: ## difference in mean rank score ## -0.006822827 This test confirms my suspicions from the graph, that lead and cadmium do have statistically significant differences in the means between men and women.\nConclusion Looking into this aspect of the NHANES survey was an interesting exploratory analysis that raised some questions for me regarding gender disparities in heavy metal exposure.\nExposure to these heavy metals can be linked to adverse health outcomes and it is concerning that such a large marjority of the population has detectable exposure in their blood.\n","date":"2024-03-28T00:00:00Z","image":"https://michelleyg1.github.io/p/heavy-metal/images/metal_hu5459c0360c2b0cb7a147d2df0eb350ca_3458104_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/heavy-metal/","title":"Concentration of Heavy Metals in Blood"},{"content":"For this analysis I decided to try analyzing a different survey, the 2021 National Survey on Drug Use and Health conducted by the Substance Abuse and Mental Health Services Administration, also known as SAMHSA.\nLoad Libraries 1 library(tidyverse) 1 2 3 4 5 6 7 8 9 10 ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.0 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors 1 2 library(haven) library(survey) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## Loading required package: grid ## Loading required package: Matrix ## ## Attaching package: \u0026#39;Matrix\u0026#39; ## ## The following objects are masked from \u0026#39;package:tidyr\u0026#39;: ## ## expand, pack, unpack ## ## Loading required package: survival ## ## Attaching package: \u0026#39;survey\u0026#39; ## ## The following object is masked from \u0026#39;package:graphics\u0026#39;: ## ## dotchart Import Data I came across this useful guide on how to import the data from the 2021 SAMHSA NSDUH survey, as I did not want to download all of the data locally since the files for these complex surveys tend to be very big, using the method below I was able to import it directly into R.\n1 2 3 4 5 6 7 8 9 10 11 12 zip_tf \u0026lt;- tempfile() zip_url \u0026lt;- paste0( \u0026#34;https://www.datafiles.samhsa.gov/sites/default/files/field-uploads-protected/\u0026#34;, \u0026#34;studies/NSDUH-2021/NSDUH-2021-datasets/NSDUH-2021-DS0001/\u0026#34;, \u0026#34;NSDUH-2021-DS0001-bundles-with-study-info/NSDUH-2021-DS0001-bndl-data-r_v3.zip\u0026#34;) download.file(zip_url, zip_tf, mode = \u0026#39;wb\u0026#39;) nsduh_rdata \u0026lt;- unzip(zip_tf, exdir = tempdir()) nsduh_rdata_contents \u0026lt;- load(nsduh_rdata) nsduh_df_name \u0026lt;- grep(\u0026#39;PUF\u0026#39; , nsduh_rdata_contents , value = TRUE) nsduh_df \u0026lt;- get(nsduh_df_name) names( nsduh_df ) \u0026lt;- tolower(names(nsduh_df)) nsduh_df[ , \u0026#34;one\u0026#34;] \u0026lt;- 1 Select Variables of Interest After importing the data I realized that there were 2989 different variables that represented the various answers to all of the survey questions. As I was only interested in some demographic variables and two variables that represented questions about cigarette use, I used dplyr\u0026rsquo;s select() to dramatically pare down this data. I also included the id, strata, and weight variables as these are very important aspects of analyzing survey data and will be used later in my survey design object.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cig \u0026lt;- nsduh_df %\u0026gt;% dplyr::select(cigever, # Have you ever smoked a cigarette? cigtry, # Age when first smoked a cigarette catag6, # Age category (6 Levels) sexident, # Sexual identity speakengl, # How well do you speak English? irsex, # Gender eduhighcat, # Level of Education newrace2, # Race/Hispanic Origin wrkstatwk2, # What was your work situation in the past week? irpinc3, # Income Level govtprog, # Participated in Government Assistance verep, # ID vestr_c, # Strata analwt_c) # Weights Now to take a look at the variables of the new and smaller data set.\n1 str(cig) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## \u0026#39;data.frame\u0026#39;:\t58034 obs. of 14 variables: ## $ cigever : int 1 1 2 1 2 1 1 2 2 2 ... ## $ cigtry : int 19 15 991 9 991 15 23 991 991 991 ... ## $ catag6 : int 3 6 2 4 2 5 2 4 1 2 ... ## $ sexident : int 3 1 1 1 1 1 1 1 99 1 ... ## $ speakengl : int 1 1 1 1 2 3 1 1 1 1 ... ## $ irsex : int 2 1 2 1 1 1 1 2 2 2 ... ## $ eduhighcat: int 3 4 4 1 2 1 4 4 5 2 ... ## $ newrace2 : int 1 1 1 1 7 7 5 1 2 7 ... ## $ wrkstatwk2: int 5 8 1 5 9 9 1 1 99 2 ... ## $ irpinc3 : int 2 7 1 1 1 1 3 5 1 1 ... ## $ govtprog : int 1 2 2 2 2 2 2 2 1 2 ... ## $ verep : int 1 2 2 2 1 2 2 1 2 2 ... ## $ vestr_c : int 40047 40037 40037 40045 40006 40041 40021 40008 40007 40022 ... ## $ analwt_c : num 675 12436 647 11275 351 ... ## - attr(*, \u0026#34;var.labels\u0026#34;)= chr [1:2988] \u0026#34;RESPONDENT IDENTIFICATION\u0026#34; \u0026#34;CREATION DATE OF THE DATA FILE\u0026#34; \u0026#34;EVER SMOKED A CIGARETTE\u0026#34; \u0026#34;IF BEST FRIEND OFFERED, WOULD YOU SMOKE CIG\u0026#34; ... Data Cleaning I\u0026rsquo;m going to re-code some variables as I realized while reading through the documentation that binary variables were coded as 1 and 2 rather than 0 and 1, and missing values were coded in various numeric ways and would throw off my calculations if I were to not replace them with NAs. As I needed to do this across multiple columns, to avoid repetitive code I used the across() function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 cig2 \u0026lt;- cig %\u0026gt;% mutate( across( .cols = c(cigtry, sexident, speakengl, wrkstatwk2), .fns = ~if_else(.x == 985 | .x == 991 | .x == 994 | .x == 997 | .x == 85 | .x == 94 | .x == 97 | .x == 98 | .x == 99, NA, .x) ), across( .cols = c(cigever, govtprog, irsex), .fns = ~if_else(.x == 2, 0, 1) ) ) After re-coding the variables, I wanted to factor them and give them labels so that my graphics and model outputs would be more interpretable without having to keep going back to the NSDUH survey documentation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 cig3 \u0026lt;- cig2 %\u0026gt;% mutate( cigever = factor(cigever, levels = c(0, 1), labels = c(\u0026#34;No\u0026#34;, \u0026#34;Yes\u0026#34;)), catag6 = factor(catag6, levels = c(1:6), labels = c(\u0026#34;12-17 years old\u0026#34;, \u0026#34;18-25 years old\u0026#34;, \u0026#34;26-34 years old\u0026#34;, \u0026#34;35-49 years old\u0026#34;, \u0026#34;50-64 years old\u0026#34;, \u0026#34;65+ years old\u0026#34;)), sexident = factor(sexident, levels = c(1:3), labels = c(\u0026#34;Straight\u0026#34;, \u0026#34;Lesbian or Gay\u0026#34;, \u0026#34;Bisexual\u0026#34;)), speakengl = factor(speakengl, levels = c(1:4), labels = c(\u0026#34;Very Well\u0026#34;, \u0026#34;Well\u0026#34;, \u0026#34;Not Well\u0026#34;, \u0026#34;Not at All\u0026#34;)), irsex = factor(irsex, levels = c(0, 1), labels = c(\u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;)), eduhighcat = factor(eduhighcat, levels = c(1:5), labels = c(\u0026#34;Less than High School\u0026#34;, \u0026#34;High School Graduate\u0026#34;, \u0026#34;Some College / Associates Degree\u0026#34;, \u0026#34;College Graduate\u0026#34;, \u0026#34;12-17 Year Old\u0026#34;)), newrace2 = factor(newrace2, levels = c(1:7), labels = c(\u0026#34;White\u0026#34;, \u0026#34;Black\u0026#34;, \u0026#34;Native American / Alaska Native\u0026#34;, \u0026#34;Native Hawaiian / Pacific Islander\u0026#34;, \u0026#34;Asian\u0026#34;, \u0026#34;More Than One Race\u0026#34;, \u0026#34;Hispanic\u0026#34;)), wrkstatwk2 = factor(wrkstatwk2, levels = c(1:9), labels = c(\u0026#34;Full Time Job\u0026#34;, \u0026#34;Part Time Job\u0026#34;, \u0026#34;Has Job / Volunteer Work but Did Not Work Last Week\u0026#34;, \u0026#34;Unemployed / Laid Off Looking for Work\u0026#34;, \u0026#34;Disabled\u0026#34;, \u0026#34;Homemaker\u0026#34;, \u0026#34;In School / Training\u0026#34;, \u0026#34;Retired\u0026#34;, \u0026#34;Does Not Have a Job, Other Reason\u0026#34;)), irpinc3 = factor(irpinc3, levels = c(1:7), labels = c(\u0026#34;Less than $10,000\u0026#34;, \u0026#34;Between $10,000 and $19,000\u0026#34;, \u0026#34;Between $20,000 and $29,000\u0026#34;, \u0026#34;Between $30,000 and $39,000\u0026#34;, \u0026#34;Between $40,000 and $49,000\u0026#34;, \u0026#34;Between $50,000 and $74,000\u0026#34;, \u0026#34;$75,000 or More\u0026#34;)), govtprog = factor(govtprog, levels = c(0, 1), labels = c(\u0026#34;No\u0026#34;, \u0026#34;Yes\u0026#34;)) ) Survey Design Object Below, I create my survey design object using the id, strata, and weights provided by the NSDUH data set using the svydesign() function.\n1 2 3 4 5 6 7 cig.svy \u0026lt;- svydesign( id = ~verep, strata = ~vestr_c, weights = ~analwt_c, data = cig3, nest = TRUE ) Participation in Government Assistance Program(s) and Cigarette Use The first predictor variable I wanted to look at was participation in one or more government programs and differences in age when the participant first tried a cigarette, and if they ever tried a cigarette at all.\nMean Age First Tried a Cigarette and Government Assistance Program Participant Here, I used the svyby() function to check out the means for the two different groups.\n1 svyby(~cigtry, ~govtprog, cig.svy, svymean, na.rm = TRUE) 1 2 3 ## govtprog cigtry se ## No No 16.35160 0.05977798 ## Yes Yes 15.79158 0.12337171 And to graph this data.\n1 2 3 4 5 6 7 8 9 svyboxplot(cigtry ~ govtprog, cig.svy, ylim = c(0, 30), xlab = \u0026#34;Participated in One or More Government Assistance Programs\u0026#34;, ylab = \u0026#34;Age\u0026#34;, col = c(\u0026#34;#A6A9C8\u0026#34;, \u0026#34;#554D74\u0026#34;)) title(main = \u0026#34;Average Age Participant First Tried a Cigarette by Participation in Government Assistance Programs\u0026#34;, adj = 0.5) It seems like the mean age where a cigarette was first tried for participants who have used one or more government assistance programs is slightly lower on average, lets see if this difference is statistically significant.\nFirst, I\u0026rsquo;m going to look at how the weighted cigtry variable is distributed.\n1 2 3 4 5 svyhist(~cigtry, cig.svy, breaks = 30, col = c(\u0026#34;#A6A9C8\u0026#34;), main = \u0026#34;Distribution of Age Participant First Tried a Cigarette\u0026#34;) It seems that there are some outliers who first tried a cigarette at an older age, but the distribution is mostly bell curve shaped. I\u0026rsquo;m going to run a weighted t-test using the svyttest() function.\n1 svyttest(cigtry ~ govtprog, cig.svy) 1 2 3 4 5 6 7 8 9 10 11 ## ## Design-based t-test ## ## data: cigtry ~ govtprog ## t = -4.2839, df = 49, p-value = 8.546e-05 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -0.8227208 -0.2973152 ## sample estimates: ## difference in mean ## -0.560018 It looks like this difference between the two groups is statistically significant, and those who have utilized one or more government assistance programs tend to first try a cigarette at a younger age.\nHas the Participant Ever Tried a Cigarette and Government Assistance Program Participation In order to assess this I\u0026rsquo;m going to use the svychisq() function to determine if there is a difference between the proportions within the two different binary variables. I\u0026rsquo;m going to set the Ntotal setting to true as the weighted aspect of the survey package results in very large numbers, making it a bit less readable from just glancing at it.\n1 svytable(~cigever + govtprog, cig.svy, Ntotal = TRUE) 1 2 3 4 ## govtprog ## cigever No Yes ## No 0.38847802 0.09974387 ## Yes 0.39291777 0.11886035 1 svychisq(~cigever + govtprog, cig.svy, statistic = \u0026#34;Chisq\u0026#34;) 1 2 3 4 5 ## ## Pearson\u0026#39;s X^2: Rao \u0026amp; Scott adjustment ## ## data: svychisq(~cigever + govtprog, cig.svy, statistic = \u0026#34;Chisq\u0026#34;) ## X-squared = 66.313, df = 1, p-value = 6.518e-06 It looks participants who have utilized one or more government assistance programs more often have tried a cigarette in their lives.\nEver Smoked a Cigarette Logistic Regression Lastly, I am going to take several predictor variables of interest and fit a multivariable weighted logistic regression model. I would like to see which coefficients have a statistically significant impact on whether a participant has ever smoked a cigarette in their lives.\n1 2 3 4 5 6 7 8 9 10 11 cig.fit \u0026lt;- svyglm(cigever ~ sexident + speakengl + irsex + eduhighcat + newrace2 + wrkstatwk2 + irpinc3 + govtprog, cig.svy, family = quasibinomial) summary(cig.fit) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 ## ## Call: ## svyglm(formula = cigever ~ sexident + speakengl + irsex + eduhighcat + ## newrace2 + wrkstatwk2 + irpinc3 + govtprog, design = cig.svy, ## family = quasibinomial) ## ## Survey design: ## svydesign(id = ~verep, strata = ~vestr_c, weights = ~analwt_c, ## data = cig3, nest = TRUE) ## ## Coefficients: ## Estimate ## (Intercept) 0.10862 ## sexidentLesbian or Gay 0.24135 ## sexidentBisexual 0.31938 ## speakenglWell -0.31160 ## speakenglNot Well -0.81491 ## speakenglNot at All -0.88363 ## irsexMale 0.37537 ## eduhighcatHigh School Graduate -0.18217 ## eduhighcatSome College / Associates Degree -0.09616 ## eduhighcatCollege Graduate -0.41272 ## newrace2Black -0.94815 ## newrace2Native American / Alaska Native -0.14689 ## newrace2Native Hawaiian / Pacific Islander -0.86070 ## newrace2Asian -1.04402 ## newrace2More Than One Race -0.09915 ## newrace2Hispanic -0.64055 ## wrkstatwk2Part Time Job -0.15217 ## wrkstatwk2Has Job / Volunteer Work but Did Not Work Last Week -0.04618 ## wrkstatwk2Unemployed / Laid Off Looking for Work 0.25849 ## wrkstatwk2Disabled 0.54665 ## wrkstatwk2Homemaker 0.21088 ## wrkstatwk2In School / Training -1.07989 ## wrkstatwk2Retired -0.01107 ## wrkstatwk2Does Not Have a Job, Other Reason 0.11649 ## irpinc3Between $10,000 and $19,000 0.36938 ## irpinc3Between $20,000 and $29,000 0.48541 ## irpinc3Between $30,000 and $39,000 0.48611 ## irpinc3Between $40,000 and $49,000 0.53580 ## irpinc3Between $50,000 and $74,000 0.52905 ## irpinc3$75,000 or More 0.54041 ## govtprogYes 0.44118 ## Std. Error ## (Intercept) 0.09094 ## sexidentLesbian or Gay 0.11183 ## sexidentBisexual 0.06567 ## speakenglWell 0.07126 ## speakenglNot Well 0.18639 ## speakenglNot at All 0.22132 ## irsexMale 0.03388 ## eduhighcatHigh School Graduate 0.07062 ## eduhighcatSome College / Associates Degree 0.08069 ## eduhighcatCollege Graduate 0.08046 ## newrace2Black 0.05909 ## newrace2Native American / Alaska Native 0.22734 ## newrace2Native Hawaiian / Pacific Islander 0.27240 ## newrace2Asian 0.09878 ## newrace2More Than One Race 0.12547 ## newrace2Hispanic 0.05912 ## wrkstatwk2Part Time Job 0.06677 ## wrkstatwk2Has Job / Volunteer Work but Did Not Work Last Week 0.07789 ## wrkstatwk2Unemployed / Laid Off Looking for Work 0.09772 ## wrkstatwk2Disabled 0.11551 ## wrkstatwk2Homemaker 0.09539 ## wrkstatwk2In School / Training 0.12776 ## wrkstatwk2Retired 0.06071 ## wrkstatwk2Does Not Have a Job, Other Reason 0.07895 ## irpinc3Between $10,000 and $19,000 0.06627 ## irpinc3Between $20,000 and $29,000 0.07007 ## irpinc3Between $30,000 and $39,000 0.07732 ## irpinc3Between $40,000 and $49,000 0.07806 ## irpinc3Between $50,000 and $74,000 0.08146 ## irpinc3$75,000 or More 0.07447 ## govtprogYes 0.04725 ## t value Pr(\u0026gt;|t|) ## (Intercept) 1.194 0.246323 ## sexidentLesbian or Gay 2.158 0.043245 ## sexidentBisexual 4.863 9.41e-05 ## speakenglWell -4.373 0.000294 ## speakenglNot Well -4.372 0.000295 ## speakenglNot at All -3.992 0.000716 ## irsexMale 11.078 5.51e-10 ## eduhighcatHigh School Graduate -2.579 0.017907 ## eduhighcatSome College / Associates Degree -1.192 0.247336 ## eduhighcatCollege Graduate -5.129 5.11e-05 ## newrace2Black -16.047 6.89e-13 ## newrace2Native American / Alaska Native -0.646 0.525550 ## newrace2Native Hawaiian / Pacific Islander -3.160 0.004929 ## newrace2Asian -10.569 1.24e-09 ## newrace2More Than One Race -0.790 0.438653 ## newrace2Hispanic -10.835 8.08e-10 ## wrkstatwk2Part Time Job -2.279 0.033780 ## wrkstatwk2Has Job / Volunteer Work but Did Not Work Last Week -0.593 0.559934 ## wrkstatwk2Unemployed / Laid Off Looking for Work 2.645 0.015528 ## wrkstatwk2Disabled 4.732 0.000127 ## wrkstatwk2Homemaker 2.211 0.038876 ## wrkstatwk2In School / Training -8.453 4.93e-08 ## wrkstatwk2Retired -0.182 0.857172 ## wrkstatwk2Does Not Have a Job, Other Reason 1.475 0.155657 ## irpinc3Between $10,000 and $19,000 5.574 1.86e-05 ## irpinc3Between $20,000 and $29,000 6.927 9.99e-07 ## irpinc3Between $30,000 and $39,000 6.287 3.88e-06 ## irpinc3Between $40,000 and $49,000 6.864 1.14e-06 ## irpinc3Between $50,000 and $74,000 6.495 2.48e-06 ## irpinc3$75,000 or More 7.257 5.08e-07 ## govtprogYes 9.337 9.90e-09 ## ## (Intercept) ## sexidentLesbian or Gay * ## sexidentBisexual *** ## speakenglWell *** ## speakenglNot Well *** ## speakenglNot at All *** ## irsexMale *** ## eduhighcatHigh School Graduate * ## eduhighcatSome College / Associates Degree ## eduhighcatCollege Graduate *** ## newrace2Black *** ## newrace2Native American / Alaska Native ## newrace2Native Hawaiian / Pacific Islander ** ## newrace2Asian *** ## newrace2More Than One Race ## newrace2Hispanic *** ## wrkstatwk2Part Time Job * ## wrkstatwk2Has Job / Volunteer Work but Did Not Work Last Week ## wrkstatwk2Unemployed / Laid Off Looking for Work * ## wrkstatwk2Disabled *** ## wrkstatwk2Homemaker * ## wrkstatwk2In School / Training *** ## wrkstatwk2Retired ## wrkstatwk2Does Not Have a Job, Other Reason ## irpinc3Between $10,000 and $19,000 *** ## irpinc3Between $20,000 and $29,000 *** ## irpinc3Between $30,000 and $39,000 *** ## irpinc3Between $40,000 and $49,000 *** ## irpinc3Between $50,000 and $74,000 *** ## irpinc3$75,000 or More *** ## govtprogYes *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for quasibinomial family taken to be 1.12446) ## ## Number of Fisher Scoring iterations: 4 From looking at this model, there are many variables that have an impact on whether the participant reports that they have tried a cigarette in their lives. Some of these have a negative coefficient meaning it makes them more likely to have not tried a cigarette, while others have a positive coefficient meaning the opposite.\nSome comparatively large significant negative coefficients that I noticed were that Asian race and in school/training. The positive coefficients were not as dramatic, but some to point out are work status being disabled and income in the range of $75,000 plus.\nConclusion It was interesting to analyze another complex survey besides NHANES, as I got to discover a new method of importing a large amount of data into R without having to save it locally. Publicly available survey data is a truly valuable resource, it enables me to learn data analysis skills for free and learn more about the health trends of the country I live in.\nI will definitely be coming back to this particular survey, and others that are provided in the guide to practice my skills, and find out more about health behaviors of the American public and the factors that influence them.\n","date":"2024-03-21T00:00:00Z","image":"https://michelleyg1.github.io/p/cigarettes-samhsa/images/purp_hu5459c0360c2b0cb7a147d2df0eb350ca_676910_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/cigarettes-samhsa/","title":"SAMHSA National Survey on Drug Use and Health Analysis"},{"content":"For this post, I decided to analyze NHANES data from the years 2011-2016 once again, as there are so many variables to explore and possible research questions that can be looked into from this data alone. I decided to do an analysis of trends in very general liver health data, in this case the proportion of the population that has been told that they have a liver condition, and average age at diagnosis in those that responded yes.\nThere are many kinds of liver conditions that can be caused by either genetic, viral, or lifestyle factors that are not always serious, but can definitely be if it goes untreated. So, I was interested in seeing if there was a change in the responses to these survey interview questions over the three two-year long cycles.\nLoad Packages 1 2 3 4 5 6 7 suppressPackageStartupMessages({ library(tidyverse) library(nhanesA) library(survey) library(janitor) library(tableone) }) Load Data Looking back at my prior NHANES projects, I realized that the code to import the data was quite repetitive, so I decided to face my fear of writing functions and write some simple functions to import the data that I needed from the three different survey cycles. First, I created one for the demographic data, and then the liver condition data as I would be selecting the same columns from all three data sets.\nDemographic Import Function 1 2 3 4 demo \u0026lt;- function(nhanes = \u0026#34;DEMO\u0026#34;) { nhanes(nhanes) %\u0026gt;% dplyr::select(SEQN, RIAGENDR, RIDAGEYR, RIDEXPRG, RIDRETH3, SDMVPSU, SDMVSTRA, WTINT2YR) } Liver Import Function 1 2 3 4 liv \u0026lt;- function(nhanes = \u0026#34;MCQ\u0026#34;) { nhanes(nhanes) %\u0026gt;% dplyr::select(SEQN, MCQ160L, MCQ180L) } and now to apply my functions to import the data that I need in a less repetitive manner.\n1 2 3 4 5 6 7 8 # Demographics Data demo_11 \u0026lt;- demo(\u0026#34;DEMO_G\u0026#34;) demo_13 \u0026lt;- demo(\u0026#34;DEMO_H\u0026#34;) demo_15 \u0026lt;- demo(\u0026#34;DEMO_I\u0026#34;) # Liver Data liv_11 \u0026lt;- liv(\u0026#34;MCQ_G\u0026#34;) liv_13 \u0026lt;- liv(\u0026#34;MCQ_H\u0026#34;) liv_15 \u0026lt;- liv(\u0026#34;MCQ_I\u0026#34;) Merge Data I\u0026rsquo;m also going to create a function to merge the demographic and liver data by the unique identifier SEQN, and then drop the column as that won\u0026rsquo;t be needed past this step to create the final dataset.\n1 2 3 4 5 merge.nhanes \u0026lt;- function(demo, liv) { merged \u0026lt;- merge(demo, liv, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) merged$SEQN \u0026lt;- NULL merged } Then I applied the merge.nhanes() functions, and also created a separate column that identifies which cycle that the specific data came from as that is the variable I am interested in comparing as I\u0026rsquo;m trying to see if there are any changes over time that have occurred. And finally, I create my final, unclean, and merged dataset using rbind().\n1 2 3 4 5 6 7 8 df_11 \u0026lt;- merge.nhanes(demo_11, liv_11) %\u0026gt;% mutate(cycle = \u0026#34;2011-2012 Cycle\u0026#34;) df_13 \u0026lt;- merge.nhanes(demo_13, liv_13) %\u0026gt;% mutate(cycle = \u0026#34;2013-2014 Cycle\u0026#34;) df_15 \u0026lt;- merge.nhanes(demo_15, liv_15) %\u0026gt;% mutate(cycle = \u0026#34;2015-2016 Cycle\u0026#34;) df \u0026lt;- rbind(df_11, df_13, df_15) Clean Data I then use dyplr\u0026rsquo;s mutate, rename, and select to clean up the dataset and give it more intuitive variable names. I also had to go back in and make sure that all types of coding for missing data was accounted for as I noticed when doing summary statistics later that the liver age variable was way outside of a reasonable age that someone would even be alive,\nso I assumed there was alternative coding for NAs that I must have missed, and found out some observations were coded as 99999 instead of NA, and I fixed that with a simple if_else() function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 dff \u0026lt;- df %\u0026gt;% mutate( preg = as.factor(case_when( RIDEXPRG == \u0026#34;Yes, positive lab pregnancy test or self-reported pregnant at exam\u0026#34; ~ \u0026#34;Yes\u0026#34;, RIDEXPRG == \u0026#34;The participant was not pregnant at exam\u0026#34; ~ \u0026#34;No\u0026#34;, TRUE ~ NA)), race = as.factor(case_when( RIDRETH3 == \u0026#34;Mexican American\u0026#34; | RIDRETH3 == \u0026#34;Other Hispanic\u0026#34; ~ \u0026#34;Hispanic\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic White\u0026#34; ~ \u0026#34;White\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic Black\u0026#34; ~\u0026#34;Black\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic Asian\u0026#34; ~ \u0026#34;Asian\u0026#34;, RIDRETH3 == \u0026#34;Other Race - Including Multi-Racial\u0026#34; ~ \u0026#34;Other\u0026#34;)), liver = as.factor(case_when( MCQ160L == \u0026#34;Yes\u0026#34; ~ \u0026#34;Yes\u0026#34;, MCQ160L == \u0026#34;No\u0026#34; ~ \u0026#34;No\u0026#34;, TRUE ~ NA)), liverage = if_else(MCQ180L == 99999, NA, MCQ180L), cycle = as.factor(cycle) ) %\u0026gt;% rename( gender = RIAGENDR, age = RIDAGEYR, psu = SDMVPSU, strata = SDMVSTRA, weight = WTINT2YR ) %\u0026gt;% dplyr::select(-RIDRETH3, -RIDEXPRG, -MCQ160L, -MCQ180L) Summary Statistics Now to take a look at a summary table of my data to make sure that everything was coded in the way that I want it to be, and that there are no glaring outliers anymore that may show that the data was not cleaned properly.\n1 2 init.table \u0026lt;- CreateTableOne(data = dff, includeNA = TRUE) print(init.table, showAllLevels = TRUE) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## ## level Overall ## n 29902 ## gender (%) Male 14751 (49.3) ## Female 15151 (50.7) ## age (mean (SD)) 31.60 (24.59) ## psu (mean (SD)) 1.54 (0.56) ## strata (mean (SD)) 111.13 (13.03) ## weight (mean (SD)) 31244.60 (31423.20) ## cycle (%) 2011-2012 Cycle 9756 (32.6) ## 2013-2014 Cycle 10175 (34.0) ## 2015-2016 Cycle 9971 (33.3) ## preg (%) No 3341 (11.2) ## Yes 192 ( 0.6) ## \u0026lt;NA\u0026gt; 26369 (88.2) ## race (%) Asian 3398 (11.4) ## Black 7079 (23.7) ## Hispanic 8350 (27.9) ## Other 1362 ( 4.6) ## White 9713 (32.5) ## liver (%) No 16307 (54.5) ## Yes 712 ( 2.4) ## \u0026lt;NA\u0026gt; 12883 (43.1) ## liverage (mean (SD)) 42.00 (16.61) Looks good! Doesn\u0026rsquo;t say that the average age that someone was told that they had a liver condition was 49,000 anymore haha.\nSurvey Design Objects Now an important part of the survey analysis process in R, to create a survey design object with my cleaned up data frame using the psu weight, strata variables created by NHANES.\n1 2 3 4 5 6 svy.des \u0026lt;- svydesign(id = ~psu, strata = ~strata, weights = ~weight, nest = TRUE, survey.lonely.psu = \u0026#34;adjust\u0026#34;, data = dff) Apply Exclusion Criteria And to apply my exclusion criteria to the survey design object to exclude participants under 18, ones that are currently confirmed to be pregnant, and ones with missing values for the liver variable (which stores the response to the question, \u0026ldquo;Were you ever told that you have a liver condition?\u0026rdquo;)\n1 2 3 4 svy.subset \u0026lt;- subset(svy.des, age \u0026gt;= 18 \u0026amp; preg == \u0026#34;No\u0026#34; | is.na(preg) \u0026amp; !is.na(liver)) Proportion of Population with a Liver Condition I\u0026rsquo;m going to use the svyby() function to calculate the proportion of people diagnosed with a liver condition, using the survey package takes the weights into account and makes it reflect the proportion of the whole population that has been told that they have a liver condition, rather than just the group surveyed.\nI also found that you can extract elements of the svyby function if you assign the output to an object, which I did below in order to graph the values in a bar chart.\n1 2 3 4 5 6 7 bys \u0026lt;- svyby(~liver, ~cycle, svy.subset, svymean, na.rm = TRUE) barplot(bys$liverYes, names.arg = bys$cycle, col = c(\u0026#34;#f0bdf7\u0026#34;, \u0026#34;#efa2ef\u0026#34;, \u0026#34;#ff6399\u0026#34;)) title(main = \u0026#34;Proportion of Population Told that They Have a Liver Condition\u0026#34;, xlab = \u0026#34;Survey Cycle Years\u0026#34;, ylab = \u0026#34;Proportion\u0026#34;) From the graph it appears that the proportion of the population who have been told that they have a liver condition is slightly increasing over the three different survey cycles. However, the y axis had to be scaled to a very small number in order for this difference to be seen on the graph, as a very small proportion of the population have a liver condition. Change in Proportion of Population with a Liver Condition Over Time I\u0026rsquo;m going to make a quick table using the svytable() function to take a look at how many individuals in the population have been told that they have a liver condition vs. those that do not over the three survey cycles.\n1 svytable(~liver + cycle, svy.subset) 1 2 3 4 ## cycle ## liver 2011-2012 Cycle 2013-2014 Cycle 2015-2016 Cycle ## No 214089692 218439531 221466497 ## Yes 7262141 7746168 9628194 Just by looking at the table, the number of yes answers has gotten greater, but so has the population overall. I\u0026rsquo;m going to do a chi squared test to see if there are any statistically differences in the proportions.\n1 svychisq(~liver + cycle, svy.subset, statistic = \u0026#34;Chisq\u0026#34;) 1 2 3 4 5 ## ## Pearson\u0026#39;s X^2: Rao \u0026amp; Scott adjustment ## ## data: svychisq(~liver + cycle, svy.subset, statistic = \u0026#34;Chisq\u0026#34;) ## X-squared = 7.3014, df = 2, p-value = 0.1832 At a p value threshold of 0.05, this null hypothesis that there are no differences in the proportions cannot be rejected.\nAverage Age at Diagnosis Another variable that I am interested in taking a closer look at how it has changed over the three survey cycles is the liverage variable. This is a follow up question for those who responded yes to the previous question if they were ever told that they had a liver condition to capture the age that they were first told that they had a liver condition.\nI used the svyboxplot() function to visualize this data comparing the average age at diagnosis with a liver condition between survey cycles.\n1 2 3 4 5 6 svyboxplot(liverage ~ cycle, svy.subset, col = c(\u0026#34;#f0bdf7\u0026#34;, \u0026#34;#efa2ef\u0026#34;, \u0026#34;#ff6399\u0026#34;)) title(main = \u0026#34;Average Age when Diagnosed with a Liver Condition\u0026#34;, xlab = \u0026#34;Survey Cycle Years\u0026#34;, ylab = \u0026#34;Average Age at Diagnosis\u0026#34;) Change in Average Age at Diagnosis of a Liver Condition Over Time In order to compare the average age at diagnosis within each survey cycle group, I performed an analysis of variance test by fitting a svyglm() model along with using the regTermTest() function, as there is no specific anova function in this package that takes into consideration the survey design.\n1 2 liv.fit \u0026lt;- svyglm(liverage ~ cycle, svy.subset) regTermTest(liv.fit, ~cycle) 1 2 3 ## Wald test for cycle ## in svyglm(formula = liverage ~ cycle, design = svy.subset) ## F = 1.845992 on 2 and 45 df: p= 0.16962 It looks like this variable as well does not show any statistically significant differences between the survey cycle years at a p = 0.05 threshold.\nConclusion I wanted to do a trend analysis over the years as it was a bit of a departure from what I have already done with the NHANES data in the past. In doing this analysis, I also started using R\u0026rsquo;s ability to write custom functions to avoid writing repetitive code, which was very intimidating to be before starting this, but I realized that it\u0026rsquo;s just generalizing regular R code.\nI would like to do future analyses with even more years, as since there were only three cycles to compare there were not too many changes noted, and perhaps there is more of a long term (10+ year) trend that I\u0026rsquo;m missing by just focusing on a six year time period. I\u0026rsquo;m also excited for when the newest NHANES data releases that would be the first post-pandemic data, so that we can see how the pandemic changed up trends in population health data.\nThis trend analysis can also be expanded upon by adding the laboratory data for liver health markers from the Mobile Examination Center (MEC) portion of the survey. One thing to note that would change if this was added as a variable is the survey weights, which would be represented by the MEC weigthts rather than the interview weights, as a smaller amount of people came in for the MEC exam compared to the at home interview.\n","date":"2024-03-11T00:00:00Z","image":"https://michelleyg1.github.io/p/liver-condition-trends/images/liver_hu5459c0360c2b0cb7a147d2df0eb350ca_229405_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/liver-condition-trends/","title":"Liver Condition Trends"},{"content":"For this analysis I wanted to explore the connection between bone health and alcohol consumption. From my non-medical professional prospective, I assumed that there would be decreased bone health measures as alcohol consumption increased, but some papers actually show the opposite effect. I decided to see if the NHANES 2011 to 2016 data reflects this patter as well, and also to practice analyzing complex surveys and explore more that the survey package has to offer.\nLoad Packages 1 2 3 4 5 6 7 suppressPackageStartupMessages({ library(tidyverse) library(nhanesA) library(survey) library(janitor) library(tableone) }) Load Data I combined three cycles of NHANES data from the years 2011 to 2016, in doing this I saw in the documentation that new sample weights would have to be created to accommodate for this. Also, in reading the documentation I saw that the questionnaire that included the questions about alcohol consumption were administered in the mobile examination center, rather than in the in home interview questionnaire and therefore the exam weights would have to be used.\nI performed this calculation when I was manipulating the other columns using dplyr\u0026rsquo;s mutate function, and for now I\u0026rsquo;m starting off with importing the relevant data using the nhanesA package.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # Demographic Data demo11 \u0026lt;- nhanes(\u0026#34;DEMO_G\u0026#34;) %\u0026gt;% dplyr::select(SEQN, RIAGENDR, RIDAGEYR, RIDEXPRG, RIDRETH3, SDMVPSU, SDMVSTRA, WTMEC2YR) demo13 \u0026lt;- nhanes(\u0026#34;DEMO_H\u0026#34;) %\u0026gt;% dplyr::select(SEQN, RIAGENDR, RIDAGEYR, RIDEXPRG, RIDRETH3, SDMVPSU, SDMVSTRA, WTMEC2YR) demo15 \u0026lt;- nhanes(\u0026#34;DEMO_I\u0026#34;) %\u0026gt;% dplyr::select(SEQN, RIAGENDR, RIDAGEYR, RIDEXPRG, RIDRETH3, SDMVPSU, SDMVSTRA, WTMEC2YR) # Alcohol Consumption Data alc11 \u0026lt;- nhanes(\u0026#34;ALQ_G\u0026#34;) %\u0026gt;% dplyr::select(SEQN, ALQ101, ALQ110, ALQ120Q, ALQ120U, ALQ130) alc13 \u0026lt;- nhanes(\u0026#34;ALQ_H\u0026#34;) %\u0026gt;% dplyr::select(SEQN, ALQ101, ALQ110, ALQ120Q, ALQ120U, ALQ130) alc15 \u0026lt;- nhanes(\u0026#34;ALQ_I\u0026#34;) %\u0026gt;% dplyr::select(SEQN, ALQ101, ALQ110, ALQ120Q, ALQ120U, ALQ130) # Bone Density Data bone11 \u0026lt;- nhanes(\u0026#34;DXX_G\u0026#34;) %\u0026gt;% dplyr::select(SEQN, DXDTOBMC, DXDTOBMD) bone13 \u0026lt;- nhanes(\u0026#34;DXX_H\u0026#34;) %\u0026gt;% dplyr::select(SEQN, DXDTOBMC, DXDTOBMD) bone15 \u0026lt;- nhanes(\u0026#34;DXX_I\u0026#34;) %\u0026gt;% dplyr::select(SEQN, DXDTOBMC, DXDTOBMD) Merge Data 1 2 3 4 5 6 7 df11 \u0026lt;- merge(demo11, alc11, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) %\u0026gt;% merge(bone11, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) df13 \u0026lt;- merge(demo13, alc13, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) %\u0026gt;% merge(bone13, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) df15 \u0026lt;- merge(demo15, alc15, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) %\u0026gt;% merge(bone15, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) df \u0026lt;- rbind(df11, df13, df15) df$SEQN \u0026lt;- NULL Data Cleaning Assess Variables First I wanted to know the values of my selected variables so that I could make more concise categories and also assess for missing values and how they are coded.\n1 df %\u0026gt;% tabyl(RIDRETH3) 1 2 3 4 5 6 7 ## RIDRETH3 n percent ## Mexican American 5006 0.16741355 ## Other Hispanic 3344 0.11183198 ## Non-Hispanic White 9713 0.32482777 ## Non-Hispanic Black 7079 0.23674002 ## Non-Hispanic Asian 3398 0.11363788 ## Other Race - Including Multi-Racial 1362 0.04554879 1 df %\u0026gt;% tabyl(RIDEXPRG) 1 2 3 4 5 6 7 8 9 10 ## RIDEXPRG n ## Yes, positive lab pregnancy test or self-reported pregnant at exam 192 ## The participant was not pregnant at exam 3341 ## Cannot ascertain if the participant is pregnant at exam 272 ## \u0026lt;NA\u0026gt; 26097 ## percent valid_percent ## 0.006420975 0.05045992 ## 0.111731657 0.87805519 ## 0.009096382 0.07148489 ## 0.872750987 NA 1 df %\u0026gt;% tabyl(ALQ101) 1 2 3 4 5 ## ALQ101 n percent valid_percent ## Yes 10844 0.3626513277 0.6948609509 ## No 4748 0.1587853655 0.3042419582 ## Don\u0026#39;t know 14 0.0004681961 0.0008970909 ## \u0026lt;NA\u0026gt; 14296 0.4780951107 NA 1 df %\u0026gt;% tabyl(ALQ110) 1 2 3 4 5 6 ## ALQ110 n percent valid_percent ## Yes 2047 6.845696e-02 0.4298614028 ## No 2700 9.029496e-02 0.5669886602 ## Don\u0026#39;t know 14 4.681961e-04 0.0029399412 ## Refused 1 3.344258e-05 0.0002099958 ## \u0026lt;NA\u0026gt; 25140 8.407464e-01 NA 1 df %\u0026gt;% tabyl(ALQ120Q) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 ## ALQ120Q n percent valid_percent ## 0 2581 8.631530e-02 0.2002172058 ## 1 3001 1.003612e-01 0.2327980762 ## 2 2527 8.450940e-02 0.1960282368 ## 3 1508 5.043141e-02 0.1169808393 ## 4 855 2.859341e-02 0.0663253433 ## 5 753 2.518226e-02 0.0584128462 ## 6 427 1.427998e-02 0.0331238849 ## 7 606 2.026620e-02 0.0470095415 ## 8 77 2.575079e-03 0.0059731596 ## 9 9 3.009832e-04 0.0006981615 ## 10 192 6.420975e-03 0.0148941122 ## 11 3 1.003277e-04 0.0002327205 ## 12 69 2.307538e-03 0.0053525716 ## 13 2 6.688516e-05 0.0001551470 ## 14 13 4.347535e-04 0.0010084555 ## 15 63 2.106882e-03 0.0048871306 ## 16 4 1.337703e-04 0.0003102940 ## 17 1 3.344258e-05 0.0000775735 ## 18 3 1.003277e-04 0.0002327205 ## 19 1 3.344258e-05 0.0000775735 ## 20 53 1.772457e-03 0.0041113955 ## 21 1 3.344258e-05 0.0000775735 ## 22 1 3.344258e-05 0.0000775735 ## 24 8 2.675406e-04 0.0006205880 ## 25 8 2.675406e-04 0.0006205880 ## 28 3 1.003277e-04 0.0002327205 ## 29 1 3.344258e-05 0.0000775735 ## 30 35 1.170490e-03 0.0027150725 ## 35 2 6.688516e-05 0.0001551470 ## 36 1 3.344258e-05 0.0000775735 ## 40 3 1.003277e-04 0.0002327205 ## 42 1 3.344258e-05 0.0000775735 ## 45 2 6.688516e-05 0.0001551470 ## 48 4 1.337703e-04 0.0003102940 ## 50 2 6.688516e-05 0.0001551470 ## 52 1 3.344258e-05 0.0000775735 ## 60 6 2.006555e-04 0.0004654410 ## 61 1 3.344258e-05 0.0000775735 ## 64 1 3.344258e-05 0.0000775735 ## 72 1 3.344258e-05 0.0000775735 ## 80 2 6.688516e-05 0.0001551470 ## 90 4 1.337703e-04 0.0003102940 ## 96 1 3.344258e-05 0.0000775735 ## 97 1 3.344258e-05 0.0000775735 ## 100 5 1.672129e-04 0.0003878675 ## 108 1 3.344258e-05 0.0000775735 ## 112 1 3.344258e-05 0.0000775735 ## 120 1 3.344258e-05 0.0000775735 ## 138 1 3.344258e-05 0.0000775735 ## 140 1 3.344258e-05 0.0000775735 ## 150 2 6.688516e-05 0.0001551470 ## 160 1 3.344258e-05 0.0000775735 ## 168 1 3.344258e-05 0.0000775735 ## 178 1 3.344258e-05 0.0000775735 ## 180 2 6.688516e-05 0.0001551470 ## 182 1 3.344258e-05 0.0000775735 ## 200 4 1.337703e-04 0.0003102940 ## 220 1 3.344258e-05 0.0000775735 ## 245 1 3.344258e-05 0.0000775735 ## 250 1 3.344258e-05 0.0000775735 ## 300 1 3.344258e-05 0.0000775735 ## 305 1 3.344258e-05 0.0000775735 ## 340 1 3.344258e-05 0.0000775735 ## 350 3 1.003277e-04 0.0002327205 ## 365 8 2.675406e-04 0.0006205880 ## 777 2 6.688516e-05 0.0001551470 ## 999 12 4.013109e-04 0.0009308820 ## NA 17011 5.688917e-01 NA 1 df %\u0026gt;% tabyl(ALQ120U) 1 2 3 4 5 ## ALQ120U n percent valid_percent ## Week 4140 0.13845228 0.4020979 ## Month 2880 0.09631463 0.2797203 ## Year 3276 0.10955789 0.3181818 ## \u0026lt;NA\u0026gt; 19606 0.65567521 NA 1 df %\u0026gt;% tabyl(ALQ130) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 ## ALQ130 n percent valid_percent ## 1 3601 1.204267e-01 3.493064e-01 ## 2 2858 9.557889e-02 2.772335e-01 ## 3 1498 5.009698e-02 1.453099e-01 ## 4 777 2.598488e-02 7.537104e-02 ## 5 467 1.561768e-02 4.530022e-02 ## 6 525 1.755735e-02 5.092638e-02 ## 7 89 2.976390e-03 8.633233e-03 ## 8 139 4.648518e-03 1.348336e-02 ## 9 20 6.688516e-04 1.940052e-03 ## 10 103 3.444586e-03 9.991270e-03 ## 11 10 3.344258e-04 9.700262e-04 ## 12 128 4.280650e-03 1.241634e-02 ## 13 7 2.340981e-04 6.790183e-04 ## 14 9 3.009832e-04 8.730236e-04 ## 15 33 1.103605e-03 3.201086e-03 ## 16 7 2.340981e-04 6.790183e-04 ## 17 1 3.344258e-05 9.700262e-05 ## 18 5 1.672129e-04 4.850131e-04 ## 20 8 2.675406e-04 7.760210e-04 ## 21 2 6.688516e-05 1.940052e-04 ## 23 1 3.344258e-05 9.700262e-05 ## 24 4 1.337703e-04 3.880105e-04 ## 25 1 3.344258e-05 9.700262e-05 ## 30 2 6.688516e-05 1.940052e-04 ## 64 1 3.344258e-05 9.700262e-05 ## 82 1 3.344258e-05 9.700262e-05 ## 777 2 6.688516e-05 1.940052e-04 ## 999 10 3.344258e-04 9.700262e-04 ## NA 19593 6.552405e-01 NA This was a useful part of data cleaning, as I was able to see that certain missing values were coded as 999 or 777 rather than NA, this would\u0026rsquo;ve seriously thrown off my calculations when I tried to create the variable to capture average drinks over last 12 months time period.\nCreating New Variables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 dff \u0026lt;- df %\u0026gt;% mutate(weight = 1/3*df$WTMEC2YR, preg = as.factor(case_when( RIDEXPRG == \u0026#34;Yes, positive lab pregnancy test or self-reported pregnant at exam\u0026#34; ~ \u0026#34;Yes\u0026#34;, RIDEXPRG == \u0026#34;The participant was not pregnant at exam\u0026#34; ~ \u0026#34;No\u0026#34;, TRUE ~ NA)), race = as.factor(case_when( RIDRETH3 == \u0026#34;Mexican American\u0026#34; | RIDRETH3 == \u0026#34;Other Hispanic\u0026#34; ~ \u0026#34;Hispanic\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic White\u0026#34; ~ \u0026#34;White\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic Black\u0026#34; ~ \u0026#34;Black\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic Asian\u0026#34; ~ \u0026#34;Asian\u0026#34;, RIDRETH3 == \u0026#34;Other Race - Including Multi-Racial\u0026#34; ~ \u0026#34;Other\u0026#34;)), drinksyr = if_else(ALQ101 == \u0026#34;Don\u0026#39;t know\u0026#34;, NA, ALQ101), drinkslife = if_else(ALQ110 == \u0026#34;Don\u0026#39;t know\u0026#34; | ALQ110 == \u0026#34;Refused\u0026#34;, NA, ALQ110), drinksfreq = if_else(ALQ120Q == 777 | ALQ120Q == 999, NA, ALQ120Q), drinksamt = if_else(ALQ130 == 777 | ALQ130 == 999, NA, ALQ130), drinksfreqcalc = case_when( ALQ120U == \u0026#34;Week\u0026#34; ~ drinksfreq * 52, ALQ120U == \u0026#34;Month\u0026#34; ~ drinksfreq * 12, ALQ120U == \u0026#34;Year\u0026#34; ~ drinksfreq, ALQ120Q == 0 \u0026amp; ALQ120U == NA ~ 0, ALQ120Q == NA \u0026amp; ALQ120U == NA ~ NA), drinkstotalyr = drinksfreqcalc * drinksamt, drinktype = as_factor(case_when( RIAGENDR == \u0026#34;Female\u0026#34; \u0026amp; ALQ130 \u0026gt;= 4 ~ \u0026#34;Binge Drinker\u0026#34;, RIAGENDR == \u0026#34;Female\u0026#34; \u0026amp; ALQ130 \u0026lt; 4 ~ \u0026#34;Drinker\u0026#34;, RIAGENDR == \u0026#34;Male\u0026#34; \u0026amp; ALQ130 \u0026gt;= 5 ~ \u0026#34;Binge Drinker\u0026#34;, RIAGENDR == \u0026#34;Male\u0026#34; \u0026amp; ALQ130 \u0026lt; 5 ~ \u0026#34;Drinker\u0026#34;, ALQ110 == \u0026#34;No\u0026#34; ~ \u0026#34;Non Drinker\u0026#34;)) ) %\u0026gt;% rename( gender = RIAGENDR, age = RIDAGEYR, psu = SDMVPSU, strata = SDMVSTRA, bmc = DXDTOBMC, bmd = DXDTOBMD ) %\u0026gt;% dplyr::select(-RIDRETH3, -RIDEXPRG, -ALQ101, -ALQ110, - ALQ120Q, - ALQ120U, -ALQ130, -WTMEC2YR) Now I\u0026rsquo;m going to check out how my new columns look and make sure that there\u0026rsquo;s nothing unexpected that I didn\u0026rsquo;t cover through all of my case_when()s and if_else()s.\n1 str(dff) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## \u0026#39;data.frame\u0026#39;:\t29902 obs. of 16 variables: ## $ gender : Factor w/ 2 levels \u0026#34;Male\u0026#34;,\u0026#34;Female\u0026#34;: 1 2 1 2 2 1 1 1 1 1 ... ## $ age : num 22 3 14 44 14 9 0 6 21 15 ... ## $ psu : num 1 3 3 1 2 1 2 2 1 3 ... ## $ strata : num 91 92 90 94 90 91 92 103 92 91 ... ## $ bmc : num 2589 NA 1931 2039 1944 ... ## $ bmd : num 1.21 NA 1.02 1.07 1.06 ... ## $ weight : num 34746 5372 2623 42655 4461 ... ## $ preg : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: NA NA NA 1 NA NA NA NA NA NA ... ## $ race : Factor w/ 5 levels \u0026#34;Asian\u0026#34;,\u0026#34;Black\u0026#34;,..: 5 3 1 5 2 5 1 4 1 4 ... ## $ drinksyr : Factor w/ 3 levels \u0026#34;Yes\u0026#34;,\u0026#34;No\u0026#34;,\u0026#34;Don\u0026#39;t know\u0026#34;: 2 NA NA NA NA NA NA NA 1 NA ... ## $ drinkslife : Factor w/ 4 levels \u0026#34;Yes\u0026#34;,\u0026#34;No\u0026#34;,\u0026#34;Don\u0026#39;t know\u0026#34;,..: 2 NA NA NA NA NA NA NA NA NA ... ## $ drinksfreq : num NA NA NA NA NA NA NA NA 1 NA ... ## $ drinksamt : num NA NA NA NA NA NA NA NA 2 NA ... ## $ drinksfreqcalc: num NA NA NA NA NA NA NA NA 12 NA ... ## $ drinkstotalyr : num NA NA NA NA NA NA NA NA 24 NA ... ## $ drinktype : Factor w/ 3 levels \u0026#34;Non Drinker\u0026#34;,..: 1 NA NA NA NA NA NA NA 2 NA ... It looks like factor levels that have no observations are still appearing as a valid level, I\u0026rsquo;m going to use the droplevels() function to clean this up.\n1 2 dff$drinksyr \u0026lt;- droplevels(dff$drinksyr) dff$drinkslife \u0026lt;- droplevels(dff$drinkslife) And now re-checking.\n1 str(dff) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## \u0026#39;data.frame\u0026#39;:\t29902 obs. of 16 variables: ## $ gender : Factor w/ 2 levels \u0026#34;Male\u0026#34;,\u0026#34;Female\u0026#34;: 1 2 1 2 2 1 1 1 1 1 ... ## $ age : num 22 3 14 44 14 9 0 6 21 15 ... ## $ psu : num 1 3 3 1 2 1 2 2 1 3 ... ## $ strata : num 91 92 90 94 90 91 92 103 92 91 ... ## $ bmc : num 2589 NA 1931 2039 1944 ... ## $ bmd : num 1.21 NA 1.02 1.07 1.06 ... ## $ weight : num 34746 5372 2623 42655 4461 ... ## $ preg : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: NA NA NA 1 NA NA NA NA NA NA ... ## $ race : Factor w/ 5 levels \u0026#34;Asian\u0026#34;,\u0026#34;Black\u0026#34;,..: 5 3 1 5 2 5 1 4 1 4 ... ## $ drinksyr : Factor w/ 2 levels \u0026#34;Yes\u0026#34;,\u0026#34;No\u0026#34;: 2 NA NA NA NA NA NA NA 1 NA ... ## $ drinkslife : Factor w/ 2 levels \u0026#34;Yes\u0026#34;,\u0026#34;No\u0026#34;: 2 NA NA NA NA NA NA NA NA NA ... ## $ drinksfreq : num NA NA NA NA NA NA NA NA 1 NA ... ## $ drinksamt : num NA NA NA NA NA NA NA NA 2 NA ... ## $ drinksfreqcalc: num NA NA NA NA NA NA NA NA 12 NA ... ## $ drinkstotalyr : num NA NA NA NA NA NA NA NA 24 NA ... ## $ drinktype : Factor w/ 3 levels \u0026#34;Non Drinker\u0026#34;,..: 1 NA NA NA NA NA NA NA 2 NA ... Variable Summary My final dataset includes the following variables that I\u0026rsquo;m going to use to assess alcohol consumption\ndrinksyr: have you had at least 12 drinks in a year? drinkslife: have you had at least 12 drinks in your life? drinksamt: ALQ130 with missing values recoded as NA drinksfreq: ALQ120Q with missing values recoded as NA drinksfreqcalc: drinking days during a year period drinkstotalyr: average total drinks a year created by multiplying average drinks/drinking day with average amount of drinking days in a year drinktype: if an average drinking day for an individual is considered binge drinking (note that these amounts are different for men and women) Table 1 2 3 4 init.table \u0026lt;- CreateTableOne(data = dff, includeNA = TRUE) print(init.table, showAllLevels = TRUE) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ## ## level Overall ## n 29902 ## gender (%) Male 14751 (49.3) ## Female 15151 (50.7) ## age (mean (SD)) 31.60 (24.59) ## psu (mean (SD)) 1.54 (0.56) ## strata (mean (SD)) 111.13 (13.03) ## bmc (mean (SD)) 2080.01 (604.26) ## bmd (mean (SD)) 1.05 (0.15) ## weight (mean (SD)) 10414.87 (10842.12) ## preg (%) No 3341 (11.2) ## Yes 192 ( 0.6) ## \u0026lt;NA\u0026gt; 26369 (88.2) ## race (%) Asian 3398 (11.4) ## Black 7079 (23.7) ## Hispanic 8350 (27.9) ## Other 1362 ( 4.6) ## White 9713 (32.5) ## drinksyr (%) Yes 10844 (36.3) ## No 4748 (15.9) ## \u0026lt;NA\u0026gt; 14310 (47.9) ## drinkslife (%) Yes 2047 ( 6.8) ## No 2700 ( 9.0) ## \u0026lt;NA\u0026gt; 25155 (84.1) ## drinksfreq (mean (SD)) 3.49 (14.26) ## drinksamt (mean (SD)) 2.77 (2.64) ## drinksfreqcalc (mean (SD)) 72.23 (96.89) ## drinkstotalyr (mean (SD)) 227.84 (465.05) ## drinktype (%) Non Drinker 2700 ( 9.0) ## Drinker 8482 (28.4) ## Binge Drinker 1827 ( 6.1) ## \u0026lt;NA\u0026gt; 16893 (56.5) Survey Object Here I used the survey package to create a survey design object with my clean dataset.\n1 2 3 4 5 6 alc.svy \u0026lt;- svydesign(id = ~psu, strata = ~strata, weights = ~weight, nest = TRUE, survey.lonely.psu = \u0026#34;adjust\u0026#34;, data = dff) Exclusion Criteria As survey data is only supposed to be filtered through the subset() method of an already existing survey object, I used this method to apply my exclusion criteria of age being over 21 and the participant not being pregnant. I also filtered out observations where the first question about alcohol consumption behaviors was missing, and missing information about bone mineral density and bone mineral content.\n1 2 3 4 5 6 alc.subset \u0026lt;- subset(alc.svy, age \u0026gt;= 21 \u0026amp; preg == \u0026#34;No\u0026#34; | is.na(preg) \u0026amp; !is.na(drinksyr) \u0026amp; !is.na(bmc) \u0026amp; !is.na(bmd)) Graphics Box Plot of Bone Health Metrics by Type of Drinker Here I used the built in graphics in the survey package to create a box plot.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 par(mfrow = c(1, 2), xpd = NA) svyboxplot(bmd ~ drinktype, alc.subset, ylab = \u0026#34;Bone Mineral Density\u0026#34;, col = c(\u0026#34;#98bad5\u0026#34;, \u0026#34;#305674\u0026#34;, \u0026#34;#c6d3e3\u0026#34;), show.names = FALSE) svyboxplot(bmc ~ drinktype, alc.subset, ylab = \u0026#34;Bone Mineral Content\u0026#34;, col = c(\u0026#34;#98bad5\u0026#34;, \u0026#34;#305674\u0026#34;, \u0026#34;#c6d3e3\u0026#34;), show.names = FALSE, bty = \u0026#34;L\u0026#34;) legend(\u0026#34;bottomleft\u0026#34;, legend = c(\u0026#34;Non Drinker\u0026#34;, \u0026#34;Normal Drinker\u0026#34;, \u0026#34;Binge Drinker\u0026#34;), fill = c(\u0026#34;#98bad5\u0026#34;, \u0026#34;#305674\u0026#34;, \u0026#34;#c6d3e3\u0026#34;), inset = c(-1.25, -0.3), cex = 0.9, horiz = TRUE) mtext(\u0026#34;Bone Mineral Content and Density by Type of Drinker\u0026#34;, side = 3, line = -2.5, cex = 1.2, outer = TRUE) Bone Health Metrics by Average Amount of Alcoholic Drinks Consumed in One Year Here I use the built in graphics to take a look at the relationship between the two different bone health measurements and total average alcohol consumption over the course of a year.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 par(mfrow = c(1, 2)) svyplot(bmd ~ drinkstotalyr, alc.subset, xlim = c(0, 5000), xlab = \u0026#34;\u0026#34;, ylab = \u0026#34;Bone Mineral Density (g/cm^2)\u0026#34;, style = c(\u0026#34;transparent\u0026#34;)) lines(svysmooth(bmd ~ drinkstotalyr, alc.subset, bandwidth = 200), col = \u0026#34;#2C54A7\u0026#34;, lwd = 2) svyplot(bmc ~ drinkstotalyr, alc.subset, xlim = c(0, 5000), ylab = \u0026#34;Bone Mineral Content (g)\u0026#34;, xlab = \u0026#34;\u0026#34;, style = c(\u0026#34;transparent\u0026#34;)) lines(svysmooth(bmc ~ drinkstotalyr, alc.subset, bandwidth = 200), col = \u0026#34;#2C54A7\u0026#34;, lwd = 2) mtext(\u0026#34;Bone Mineral Density(g/cm^2) and Bone Mineral Content by Average Alcoholic Drinks in One Year\u0026#34;, side = 3, line = -2.5, cex = 1.2, outer = TRUE) mtext(\u0026#34;Average Alcoholic Drinks in One Year\u0026#34;, side = 1, line = -2.5, outer = TRUE) Average Total Drinks in One Year Regression Now I\u0026rsquo;m going to fit some models to assess the relationship between my selected responses and covariates to see if there are any relationships between alcohol consumption and bone health measures when adjusting for other variables. I also plot the diagnostic plots to assess these models.\nBone Density 1 2 3 4 bmd.glm \u0026lt;- svyglm(bmd ~ drinkstotalyr + gender + age + race, alc.subset) par(mfrow = c(2, 2)) plot(bmd.glm) Diagnostics look good, the data is relatively normal and there are no obvious patterns in the other values that would throw off any assumptions. Now I'm going to take a look at a summary of the model to get the coefficients. 1 summary(bmd.glm) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## ## Call: ## svyglm(formula = bmd ~ drinkstotalyr + gender + age + race, design = alc.subset) ## ## Survey design: ## subset(alc.svy, age \u0026gt;= 21 \u0026amp; preg == \u0026#34;No\u0026#34; | is.na(preg) \u0026amp; !is.na(drinksyr) \u0026amp; ## !is.na(bmc) \u0026amp; !is.na(bmd)) ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.124e+00 7.061e-03 159.139 \u0026lt; 2e-16 *** ## drinkstotalyr -3.275e-06 3.813e-06 -0.859 0.39562 ## genderFemale -6.502e-02 3.100e-03 -20.974 \u0026lt; 2e-16 *** ## age -4.602e-04 1.337e-04 -3.441 0.00137 ** ## raceBlack 1.052e-01 5.846e-03 17.991 \u0026lt; 2e-16 *** ## raceHispanic 2.298e-02 4.843e-03 4.744 2.66e-05 *** ## raceOther 4.947e-02 9.054e-03 5.463 2.68e-06 *** ## raceWhite 3.928e-02 5.236e-03 7.502 3.79e-09 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.01040399) ## ## Number of Fisher Scoring iterations: 2 So it looks like drinkstotalyr is not statistically significant.\nBone Mineral Content Now to check out the same things for the bone mineral content variable instead of the bone mineral density.\n1 2 3 4 bmc.glm \u0026lt;- svyglm(bmc ~ drinkstotalyr + gender + age + race, alc.subset) par(mfrow = c(2, 2)) plot(bmc.glm) This looks okay, I'm going to look at the summary. 1 summary(bmc.glm) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## ## Call: ## svyglm(formula = bmc ~ drinkstotalyr + gender + age + race, design = alc.subset) ## ## Survey design: ## subset(alc.svy, age \u0026gt;= 21 \u0026amp; preg == \u0026#34;No\u0026#34; | is.na(preg) \u0026amp; !is.na(drinksyr) \u0026amp; ## !is.na(bmc) \u0026amp; !is.na(bmd)) ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 2397.58331 19.63872 122.085 \u0026lt; 2e-16 *** ## drinkstotalyr -0.01464 0.01079 -1.357 0.1824 ## genderFemale -501.15816 9.84552 -50.902 \u0026lt; 2e-16 *** ## age -0.76125 0.37994 -2.004 0.0519 . ## raceBlack 467.51804 19.12565 24.445 \u0026lt; 2e-16 *** ## raceHispanic 139.32230 17.15159 8.123 5.39e-10 *** ## raceOther 278.44977 38.19013 7.291 7.40e-09 *** ## raceWhite 264.39021 17.33046 15.256 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 128137.6) ## ## Number of Fisher Scoring iterations: 2 This is not statistically significant either.\nType of Drinker Regression I\u0026rsquo;m now going to look at my created drinktype variable to see if there is any signifigance within a model that has that variable instead of total drinks consumed in a year on average.\nBone Mineral Density 1 2 3 bmd.glm2 \u0026lt;- svyglm(bmd ~ drinktype + gender + age + race, alc.subset) summary(bmd.glm2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## ## Call: ## svyglm(formula = bmd ~ drinktype + gender + age + race, design = alc.subset) ## ## Survey design: ## subset(alc.svy, age \u0026gt;= 21 \u0026amp; preg == \u0026#34;No\u0026#34; | is.na(preg) \u0026amp; !is.na(drinksyr) \u0026amp; ## !is.na(bmc) \u0026amp; !is.na(bmd)) ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.1014758 0.0066615 165.350 \u0026lt; 2e-16 *** ## drinktypeDrinker 0.0188781 0.0041735 4.523 5.56e-05 *** ## drinktypeBinge Drinker 0.0144244 0.0046685 3.090 0.003686 ** ## genderFemale -0.0655285 0.0030256 -21.658 \u0026lt; 2e-16 *** ## age -0.0004467 0.0001254 -3.561 0.000992 *** ## raceBlack 0.1079738 0.0054775 19.712 \u0026lt; 2e-16 *** ## raceHispanic 0.0251814 0.0036067 6.982 2.27e-08 *** ## raceOther 0.0545921 0.0081870 6.668 6.14e-08 *** ## raceWhite 0.0428369 0.0047636 8.992 4.74e-11 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.00985392) ## ## Number of Fisher Scoring iterations: 2 Bone Mineral Content 1 2 3 bmc.glm2 \u0026lt;- svyglm(bmc ~ drinktype + gender + age + race, alc.subset) summary(bmc.glm2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## ## Call: ## svyglm(formula = bmc ~ drinktype + gender + age + race, design = alc.subset) ## ## Survey design: ## subset(alc.svy, age \u0026gt;= 21 \u0026amp; preg == \u0026#34;No\u0026#34; | is.na(preg) \u0026amp; !is.na(drinksyr) \u0026amp; ## !is.na(bmc) \u0026amp; !is.na(bmd)) ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 2321.5997 17.1247 135.571 \u0026lt; 2e-16 *** ## drinktypeDrinker 65.2218 17.4097 3.746 0.00058 *** ## drinktypeBinge Drinker 61.9370 17.8532 3.469 0.00129 ** ## genderFemale -500.5703 9.3263 -53.673 \u0026lt; 2e-16 *** ## age -0.6723 0.3902 -1.723 0.09281 . ## raceBlack 464.5450 18.3295 25.344 \u0026lt; 2e-16 *** ## raceHispanic 138.4712 14.0689 9.842 4.01e-12 *** ## raceOther 291.6212 37.1853 7.842 1.54e-09 *** ## raceWhite 270.4601 15.9841 16.921 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 121702.2) ## ## Number of Fisher Scoring iterations: 2 So these models do show a positive increase of bone mineral content and density when the individual is classified as a drinker and also, to a lesser extent, as a binge drinker, which reinforces the findings from previous papers about this topic.\nConclusion Things that are generally considered bad for you aren\u0026rsquo;t always bad for everything, sometimes they can also be neutral when you\u0026rsquo;re looking at very specific measures of someone\u0026rsquo;s health rather than the big picture.\nI\u0026rsquo;m definitely used to using ggplot2 for graphics, but using R\u0026rsquo;s default graphics syntax was a bit challenging and definitely took me a bit to find out how to do certain things, like putting the legend on the box plot for example. I was originally hesitant to use these built in graphics due to that, but I decided to go ahead and use them as it is meant to be used with survey data and the content was correct, it just took me longer to customize the plots.\n","date":"2024-03-04T00:00:00Z","image":"https://michelleyg1.github.io/p/alcohol-bone-health/images/bone_hu5459c0360c2b0cb7a147d2df0eb350ca_7068840_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/alcohol-bone-health/","title":"Alcohol Consumption and Bone Health"},{"content":"I\u0026rsquo;ve been working through the chapter in Intro to Statistical Learning with Applications in R that discusses how to use decision trees to make predictions with both categorical and continuous response variables. I wanted to do a quick post working through my own models using the techniques that I learned in this chapter. For these models, I used the Data from the 1986 Medicaid Survey that can be found in the blapsr package.\nLoad Libraries 1 2 3 4 5 6 7 8 9 suppressPackageStartupMessages({ library(tidyverse) library(blapsr) library(tree) library(randomForest) library(gbm) library(BART) library(vtable) }) Load and Clean Data 1 2 data(medicaid) str(medicaid) 1 2 3 4 5 6 7 8 9 10 11 ## \u0026#39;data.frame\u0026#39;:\t485 obs. of 10 variables: ## $ numvisits : int 0 1 0 0 11 3 0 6 1 0 ... ## $ exposure : int 100 90 106 114 115 102 92 92 117 101 ... ## $ children : int 1 3 4 2 1 1 2 1 1 1 ... ## $ age : int 24 19 17 29 26 22 24 21 21 24 ... ## $ income1000 : int 14500 6000 8377 6000 8500 6000 4000 6000 6000 6000 ... ## $ access : int 50 17 42 33 67 25 50 67 25 67 ... ## $ pc1times1000: int 495 520 -1227 -1524 173 -905 -1202 656 -1227 -235 ... ## $ maritalstat : int 0 0 0 0 0 0 0 1 0 1 ... ## $ sex : int 1 1 1 1 1 1 1 1 1 1 ... ## $ race : int 1 1 1 1 1 0 1 1 1 1 ... There are 10 different variables in this data frame, it looks like the factor variables maritalstat, sex, and race got read as integers rather than factors so I\u0026rsquo;m going to fix that below and also use case_when to show which dummy variable represents which level of the factor as they are not very intuitive.\n1 2 3 4 5 6 7 8 9 10 11 12 medicaid \u0026lt;- medicaid %\u0026gt;% mutate(race_f = as.factor(case_when(race == 0 ~ \u0026#34;Other\u0026#34;, race == 1 ~ \u0026#34;White\u0026#34;)), sex_f = as.factor(case_when(sex == 0 ~ \u0026#34;Male\u0026#34;, sex == 1 ~ \u0026#34;Female\u0026#34;)), maritalstat_f = as.factor(case_when(maritalstat == 0 ~ \u0026#34;Other\u0026#34;, maritalstat == 1 ~ \u0026#34;Married\u0026#34;)) ) %\u0026gt;% dplyr::select(-maritalstat, -sex, -race) medicaid_tibble \u0026lt;- as_tibble(medicaid) medicaid_tibble 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## # A tibble: 485 × 10 ## numvisits exposure children age income1000 access pc1times1000 race_f sex_f ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 0 100 1 24 14500 50 495 White Fema… ## 2 1 90 3 19 6000 17 520 White Fema… ## 3 0 106 4 17 8377 42 -1227 White Fema… ## 4 0 114 2 29 6000 33 -1524 White Fema… ## 5 11 115 1 26 8500 67 173 White Fema… ## 6 3 102 1 22 6000 25 -905 Other Fema… ## 7 0 92 2 24 4000 50 -1202 White Fema… ## 8 6 92 1 21 6000 67 656 White Fema… ## 9 1 117 1 21 6000 25 -1227 White Fema… ## 10 0 101 1 24 6000 67 -235 White Fema… ## # ℹ 475 more rows ## # ℹ 1 more variable: maritalstat_f \u0026lt;fct\u0026gt; Summary Statistics 1 sumtable(medicaid, out = \u0026#34;kable\u0026#34;) Table 1: Summary Statistics Variable N Mean Std. Dev. Min Pctl. 25 Pctl. 75 Max numvisits 485 1.6 3.3 0 0 2 48 exposure 485 104 10 32 98 112 120 children 485 2.3 1.3 1 1 3 9 age 485 31 8.9 16 24 36 64 income1000 485 8098 3242 500 6000 8500 17500 access 485 38 19 0 25 50 92 pc1times1000 485 -0.041 1434 -1524 -1066 657 7217 race_f 485 ... Other 159 33% ... White 326 67% sex_f 485 ... Female 469 97% ... Male 16 3% maritalstat_f 485 ... Married 75 15% ... Other 410 85% Regression Tree This data did not require much cleaning, so I\u0026rsquo;m going to go straight into the model fitting. First, I\u0026rsquo;m going to separate training and test data so that I can put my models to be able to calculate an error rate and see if fitting different types of models improve my error rate.\n1 2 3 4 set.seed(123) train \u0026lt;- sample(1:nrow(medicaid), nrow(medicaid) / 2) test \u0026lt;- medicaid[-train, ] y.test \u0026lt;- medicaid[-train, \u0026#34;numvisits\u0026#34;] Now I\u0026rsquo;m going to fit my first model, just a simple regression tree using the tree() function in the tree library.\n1 2 3 4 reg.tree \u0026lt;- tree(numvisits ~ ., data = medicaid, subset = train) summary(reg.tree) 1 2 3 4 5 6 7 8 9 10 ## ## Regression tree: ## tree(formula = numvisits ~ ., data = medicaid, subset = train) ## Variables actually used in tree construction: ## [1] \u0026#34;access\u0026#34; \u0026#34;pc1times1000\u0026#34; \u0026#34;age\u0026#34; ## Number of terminal nodes: 4 ## Residual mean deviance: 12.85 = 3058 / 238 ## Distribution of residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -10.8300 -1.3270 -0.9136 0.0000 0.6728 37.1700 Looks like a lot of variables were eliminated from the construction of the trees, I\u0026rsquo;m going to plot it.\n1 2 3 plot(reg.tree) text(reg.tree, pretty = 0) title(\u0026#34;Regression Tree for Medicaid 1986 Data\u0026#34;) Prune Tree This tree is already pretty minimal, but I\u0026rsquo;m going to go through the pruning process just to practice. First I\u0026rsquo;m going to plot the error as a function of size of the tree.\n1 2 cv.reg \u0026lt;- cv.tree(reg.tree) plot(cv.reg$size, cv.reg$dev, type = \u0026#34;b\u0026#34;) It looks like one is the best tree size for this particular dataset, but for the sake of being able to create a graph I\u0026rsquo;m going to set the best to two and see which variable was eliminated.\n1 2 3 4 prune.reg \u0026lt;- prune.tree(reg.tree, best = 2) plot(prune.reg) text(prune.reg, pretty = 0) title(\u0026#34;Pruned Regression Tree for Medicaid 1986 Data\u0026#34;) So age was the one variable that was eliminated, leaving access and pc1times1000. Now, I\u0026rsquo;m going to put this model to the test using mean square error for the test data.\n1 2 3 4 reg.pred \u0026lt;- predict(reg.tree, test) prune.reg.pred \u0026lt;- predict(prune.reg, test) # Unpruned Tree Test MSE mean((reg.pred - y.test)^2) 1 ## [1] 10.36599 1 2 # Pruned Tree Test MSE mean((prune.reg.pred - y.test)^2) 1 ## [1] 10.20865 The pruning does improve the prediction but very slightly bringing the test MSE from 10.37 to 10.21.\nBagging The next model that I am going to fit is a bagging model, I\u0026rsquo;m going to set mtry to equal the number of predictors as that is the key difference between bagging and random forests.\n1 2 3 4 5 6 set.seed(123) bag.tree \u0026lt;- randomForest(numvisits ~ ., data = medicaid, subset = train, mtry = 9, importance = TRUE) Next, I\u0026rsquo;m going to make a set of predicted data and use it to calculate the test MSE.\n1 2 bag.pred \u0026lt;- predict(bag.tree, test) mean((bag.pred - y.test)^2) 1 ## [1] 9.255999 As expected, this yields a better result than just doing a regression tree with no bagging as it reduces variance, now I\u0026rsquo;m going to try a Random forest approach by altering the model slightly.\nRandom Forest Here, the only thing that was changed between Bagging and Random Forest is that mtry was set to sqrt(p) instead of p itself.\n1 2 3 4 5 6 set.seed(123) rf.tree \u0026lt;- randomForest(numvisits ~ ., data = medicaid, subset = train, mtry = 3, importance = TRUE) Now, to predict and calculate our test MSE.\n1 2 rf.pred \u0026lt;- predict(rf.tree, test) mean((rf.pred - y.test)^2) 1 ## [1] 7.044376 Nice! This brings the test MSE down to 7.14, a pretty big improvement from my first model.\nImportance Plot I\u0026rsquo;m going to use a built in plot from the randomForest library to show the importance of the various predictors to take a look at which is the strongest variable in the model.\n1 varImpPlot(rf.tree) Boosting Next, I\u0026rsquo;m going to fit another model using decision trees, but this time I am going to use the boosting method that uses the gbm() function from the gbm library to fit the model. Also, when a summary of this model is printed it prints a plot of relative influence of the predictors.\n1 2 3 4 5 set.seed(123) boost.tree \u0026lt;- gbm(numvisits ~., data = medicaid, distribution = \u0026#34;gaussian\u0026#34;) summary(boost.tree) 1 2 3 4 5 6 7 8 9 10 ## var rel.inf ## pc1times1000 pc1times1000 67.629823 ## access access 15.745251 ## exposure exposure 5.430522 ## children children 4.713071 ## income1000 income1000 3.591793 ## age age 2.889541 ## race_f race_f 0.000000 ## sex_f sex_f 0.000000 ## maritalstat_f maritalstat_f 0.000000 This package also gives a built in plot of partial dependence. Here I integrate out all other variables and just take a look at the pc1times1000 variable. This variable represents the first principal component of three health status variables (functional limitations, acute conditions, and chronic conditions).\n1 plot(boost.tree, i = \u0026#34;pc1times1000\u0026#34;) And to check the test MSE and see how the model compares to the rest.\n1 boost.pred \u0026lt;- predict(boost.tree, test) 1 ## Using 100 trees... 1 mean((boost.pred - y.test)^2) 1 ## [1] 5.589588 Changing Number of Trees I want to see how the model is affected when I set n.trees equal to a thousand instead of the default of one hundred trees. Below, I added the n.trees specification to the model and calculated the MSE.\n1 2 3 4 5 6 7 set.seed(123) boost.tree.2 \u0026lt;- gbm(numvisits ~., data = medicaid, distribution = \u0026#34;gaussian\u0026#34;, n.trees = 1000) boost.pred.2 \u0026lt;- predict(boost.tree.2, test, n.trees = 1000) mean((boost.pred.2 - y.test)^2) 1 ## [1] 4.80569 This is the best so far! More than half of the error of the first model that I fit.\nBayesian Additive Regression Tree Here I create the components of the model from my test and training data in order to use the gbart() function in the BART package.\n1 2 3 4 5 6 x \u0026lt;- medicaid[, 2:10] y \u0026lt;- medicaid[, \u0026#34;numvisits\u0026#34;] x.train \u0026lt;- x[train, ] y.train \u0026lt;- y[train] x.test \u0026lt;- x[-train, ] y.test \u0026lt;- y[-train] And again, I fit the model, create some predictions off of the test data, and then calculate the test MSE, lets see if this final method improves the MSE further.\n1 2 set.seed(123) bart.tree \u0026lt;- gbart(x.train, y.train, x.test = x.test) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ## *****Calling gbart: type=1 ## *****Data: ## data:n,p,np: 242, 12, 243 ## y1,yn: -0.595041, -1.595041 ## x1,x[n*p]: 95.000000, 0.000000 ## xp1,xp[np*p]: 100.000000, 1.000000 ## *****Number of Trees: 200 ## *****Number of Cut Points: 32 ... 1 ## *****burn,nd,thin: 100,1000,1 ## *****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,0.848528,3,2.87615,1.59504 ## *****sigma: 3.842563 ## *****w (weights): 1.000000 ... 1.000000 ## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,12,0 ## *****printevery: 100 ## ## MCMC ## done 0 (out of 1100) ## done 100 (out of 1100) ## done 200 (out of 1100) ## done 300 (out of 1100) ## done 400 (out of 1100) ## done 500 (out of 1100) ## done 600 (out of 1100) ## done 700 (out of 1100) ## done 800 (out of 1100) ## done 900 (out of 1100) ## done 1000 (out of 1100) ## time: 6s ## trcnt,tecnt: 1000,1000 1 2 yhat \u0026lt;- bart.tree$yhat.test.mean mean((y.test - yhat)^2) 1 ## [1] 7.379971 7.41, this is a step back in terms of accurasy on test data from the previous model.\nConclusion In this case boosting with 1000 trees gives the best mean square error of 4.8 which means the model is off generally by 2.2 visits when used on data outside of the training set.\nDecision trees are definitely a departure to what I\u0026rsquo;ve been learning the last couple of weeks, which has mostly been regression methods of predicting, but it is good to have a whole arsenal of options when trying to do statistical learning.\n","date":"2024-02-23T00:00:00Z","image":"https://michelleyg1.github.io/p/decision-tree-medicaid/images/tree_hu5459c0360c2b0cb7a147d2df0eb350ca_2079033_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/decision-tree-medicaid/","title":"Doctor Visits Decision Trees"},{"content":"For this analysis I\u0026rsquo;m going to revisit my air quality analysis model in order to apply some new techniques that I\u0026rsquo;ve been learning. The main technique I wanted to practice was using cross validation and test mean square error in order to determine which model to select. I also learned about differnet types of model selection, such as subset selection methods, ridge regression, the lasso method, and dimension reduction methods.\nLoad Libraries 1 2 3 4 5 suppressPackageStartupMessages({ library(tidyverse) library(leaps) library(glmnet) }) Load Data 1 data(\u0026#34;airquality\u0026#34;) Clean Data Mean Imputation 1 2 3 4 5 6 7 8 9 10 11 12 airquality_mi \u0026lt;- airquality %\u0026gt;% mutate( ozone_mi = as.integer(if_else(is.na(Ozone), mean(Ozone, na.rm = TRUE), Ozone)), solar_mi = as.integer(if_else(is.na(Solar.R), mean(Solar.R, na.rm = TRUE), Solar.R)) ) %\u0026gt;% select(-1, -2) names(airquality_mi)[1:6] \u0026lt;- tolower(names(airquality_mi)[1:6]) Final Touches 1 2 3 4 airquality_mi \u0026lt;- airquality_mi %\u0026gt;% dplyr::select(month, day, wind, temp, ozone_mi, solar_mi) as_tibble(airquality_mi) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 153 × 6 ## month day wind temp ozone_mi solar_mi ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 5 1 7.4 67 41 190 ## 2 5 2 8 72 36 118 ## 3 5 3 12.6 74 12 149 ## 4 5 4 11.5 62 18 313 ## 5 5 5 14.3 56 42 185 ## 6 5 6 14.9 66 28 185 ## 7 5 7 8.6 65 23 299 ## 8 5 8 13.8 59 19 99 ## 9 5 9 20.1 61 8 19 ## 10 5 10 8.6 69 42 194 ## # ℹ 143 more rows Now that I have my data to where it was when I began my model fitting on the last analysis, I\u0026rsquo;m going to play around with some techniques that I learned. As this model does not have many different variables, I\u0026rsquo;m going to go through best subset selection and using adjustments in order to select a model pretty quickly.\nI did learn about forward and backward subset selection, but I\u0026rsquo;m going to save those for a more high dimensional data analysis in the future.\nBest Subset Selection First I\u0026rsquo;m going to fit the model using the regsubsets() function from the leaps library.\n1 2 3 best.subset \u0026lt;- regsubsets(ozone_mi ~ temp + wind + solar_mi, data = airquality_mi, nvmax = 3) And then use built in plots to visualize the R squared, adjusted R2, Cp, and BIC to see which variables make most sense to include in the model.\n1 plot(best.subset, scale = \u0026#34;r2\u0026#34;) 1 plot(best.subset, scale = \u0026#34;adjr2\u0026#34;) 1 plot(best.subset, scale = \u0026#34;Cp\u0026#34;) 1 plot(best.subset, scale = \u0026#34;bic\u0026#34;) Test MSE of Original Model Originally, I used R squared to test the fit of the model, however as I now know that only shows training error and tells us very little about how the model would work on data that it was not already fit on.\nHere I\u0026rsquo;m going to use test and training data to compute the test MSE so that I can see if these new methods that I learned improve the model that I used in my other blog post.\n1 2 3 4 5 6 7 8 9 10 11 12 13 set.seed(123) train \u0026lt;- sample(c(TRUE, FALSE), nrow(airquality_mi), replace = TRUE) test \u0026lt;- (!train) ozone.test \u0026lt;- airquality_mi$ozone_mi[test] aq.mv \u0026lt;- lm(ozone_mi ~ temp + I(temp^2) + wind + I(wind^2) + solar_mi, data = airquality_mi, subset = train) aq.mv.pred \u0026lt;- predict(aq.mv, airquality_mi[test, ]) mean((aq.mv.pred - ozone.test)^2) 1 ## [1] 528.7101 So 528.7 is the number to beat for my next models. Also, to fit on the whole dataset and check out the coefficients.\n1 2 3 aq.mv.whole \u0026lt;- lm(ozone_mi ~ temp + I(temp^2) + wind + I(wind^2) + solar_mi, data = airquality_mi) summary(aq.mv.whole) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## ## Call: ## lm(formula = ozone_mi ~ temp + I(temp^2) + wind + I(wind^2) + ## solar_mi, data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.293 -10.929 -3.289 8.684 89.513 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 321.25924 81.78038 3.928 0.000131 *** ## temp -7.28999 2.18385 -3.338 0.001069 ** ## I(temp^2) 0.05580 0.01431 3.898 0.000147 *** ## wind -11.10077 1.94439 -5.709 6.07e-08 *** ## I(wind^2) 0.39616 0.08865 4.469 1.56e-05 *** ## solar_mi 0.06206 0.01772 3.503 0.000610 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 18.46 on 147 degrees of freedom ## Multiple R-squared: 0.5998,\tAdjusted R-squared: 0.5862 ## F-statistic: 44.07 on 5 and 147 DF, p-value: \u0026lt; 2.2e-16 Shrinkage Methods Ridge Regression I\u0026rsquo;m going to use the glmnet package in order to fit these models, however I learned that the glmnet functions do not use normal model fitting syntax, but rather you have to create a model matrix first and then use the glmnet() formula.\n1 2 3 4 5 x \u0026lt;- model.matrix(ozone_mi ~ temp + wind + solar_mi, airquality_mi)[, -1] y \u0026lt;- airquality_mi$ozone_mi grid \u0026lt;- 10^seq(10, -2, length = 100) ridge \u0026lt;- glmnet(x, y, alpha = 0, lambda = grid) I\u0026rsquo;m going to use a test and training dataset to perform cross validation.\n1 2 3 4 set.seed(123) train.mat \u0026lt;- sample(1:nrow(x), nrow(x) / 2) test.mat \u0026lt;- (-train.mat) y.test \u0026lt;- y[test.mat] Next, I\u0026rsquo;m going to fit the model with the traiing data.\n1 2 3 4 5 ridge.train \u0026lt;- glmnet(x[train.mat, ], y[train.mat], alpha = 0, lambda = grid, thresh = 1e-12) Here I use the cv.glmnet() function in order to select the best value for the tuning parameter lambda.\n1 2 3 4 cv.out \u0026lt;- cv.glmnet(x[train.mat, ], y[train.mat], alpha = 0) lambda.ridge \u0026lt;- cv.out$lambda.min Lastly, I go through the process of computing a set of predicted values and computing the test Mean Squared Error in order to assess how this model compares to the original.\n1 2 3 4 ridge.pred \u0026lt;- predict(ridge.train, s = lambda.ridge, newx = x[test.mat, ]) mean((ridge.pred - y.test)^2) 1 ## [1] 363.6543 Alright, that\u0026rsquo;s better than the original. Now to fit on the whole dataset and check out the predicted coefficients.\n1 2 3 4 ridge.whole \u0026lt;- glmnet(x, y, alpha = 0) predict(ridge.whole, type = \u0026#34;coefficients\u0026#34;, s = lambda.ridge) 1 2 3 4 5 6 ## 4 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## s1 ## (Intercept) -29.47556219 ## temp 1.10854966 ## wind -2.47425857 ## solar_mi 0.05312106 The Lasso Yee haw lol. Okay, I\u0026rsquo;m going to repeat the proccess again, just using the alpha = 1 instead of alpha = 0.\n1 2 3 4 5 6 7 8 lasso.train \u0026lt;- glmnet(x[train.mat, ], y[train.mat], alpha = 1, lambda = grid) cv.out \u0026lt;- cv.glmnet(x[train.mat, ], y[train.mat], alpha = 1) lambda.lasso \u0026lt;- cv.out$lambda.min And to compute the test MSE again.\n1 2 3 4 lasso.pred \u0026lt;- predict(lasso.train, s = lambda.lasso, newx = x[test.mat, ]) mean((lasso.pred - y.test)^2) 1 ## [1] 364.4416 Not much different than the ridge, slightly worse actually, but that is to be expected really since there are not many variables. Now to check out the predicted coefficients when I fit the model on the whole dataset\n1 2 3 4 lasso.whole \u0026lt;- glmnet(x, y, alpha = 1) predict(lasso.whole, type = \u0026#34;coefficients\u0026#34;, s = lambda.lasso) 1 2 3 4 5 6 ## 4 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## s1 ## (Intercept) -37.9143506 ## temp 1.2367359 ## wind -2.6980120 ## solar_mi 0.0567973 They\u0026rsquo;re also pretty similar to each other in this case.\nConclusion This analysis helped me get a better grip on how to compute and use test mean squared error in order to use cross validation to put my model to the test. Originally, I did not know that there were so many different ways to do a regression but this textbook, Intro to Statistical Learning with Applications in R has helped me see beyond the basics and actually know whats going on within these statistical models instead of just taking shots in the dark.\nI also learned new and clever methods of seperating test and training data.\nI\u0026rsquo;m definintely still a beginner but I\u0026rsquo;m already using my new skills to improve my prior analyses and learn from my mistakes.\n","date":"2024-02-20T00:00:00Z","image":"https://michelleyg1.github.io/p/air-quality-cross-validation/images/cross_hu5459c0360c2b0cb7a147d2df0eb350ca_471906_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/air-quality-cross-validation/","title":"Air Quality Cross Validation"},{"content":"For this analysis I was looking for data that captured count data so that I could practice using a poisson regression. I came across the NYC Open Data\u0026rsquo;s Emergency Department Visits and Admissions for Influenza-like Illness and/or Pneumonia and decided to bring that into R to analyze.\nLoad Libraries 1 2 3 4 5 suppressPackageStartupMessages({ library(tidyverse) library(RSocrata) library(lubridate) }) Load Data I chose 10,000 rows from the most populous zip code in New York City, 11368 which is in Corona, Queens. I chose this as I had trouble using the read.socrata() function with larger amounts of data.\n1 resp \u0026lt;- read.socrata(\u0026#34;https://data.cityofnewyork.us/resource/2nwg-uqyg.json?mod_zcta=11368\u0026amp;$limit=10000\u0026#34;) Checking the different types of data that are stored in the different variables in the dataset.\n1 str(resp) 1 2 3 4 5 6 7 ## \u0026#39;data.frame\u0026#39;:\t10000 obs. of 6 variables: ## $ extract_date : POSIXct, format: \u0026#34;2021-09-27\u0026#34; \u0026#34;2021-07-08\u0026#34; ... ## $ date : POSIXct, format: \u0026#34;2021-01-28\u0026#34; \u0026#34;2021-02-20\u0026#34; ... ## $ mod_zcta : chr \u0026#34;11368\u0026#34; \u0026#34;11368\u0026#34; \u0026#34;11368\u0026#34; \u0026#34;11368\u0026#34; ... ## $ total_ed_visits : chr \u0026#34;122\u0026#34; \u0026#34;117\u0026#34; \u0026#34;96\u0026#34; \u0026#34;120\u0026#34; ... ## $ ili_pne_visits : chr \u0026#34;10\u0026#34; \u0026#34;4\u0026#34; \u0026#34;3\u0026#34; \u0026#34;6\u0026#34; ... ## $ ili_pne_admissions: chr \u0026#34;2\u0026#34; \u0026#34;1\u0026#34; \u0026#34;1\u0026#34; \u0026#34;1\u0026#34; ... Looks like the counts are stored as characters, and the dates are stored in POSIXct format. I\u0026rsquo;m going to need to convert the data types for the counts, and I\u0026rsquo;ll also drop the extract_date variable as that is not very useful to my analysis, as well as the mod_zcta variable since all of the values in this particular dataset are from the same zip code.\n1 2 3 4 5 6 7 8 resp \u0026lt;- resp %\u0026gt;% dplyr::select(-extract_date, -mod_zcta) %\u0026gt;% mutate( total_ed_visits = as.numeric(total_ed_visits), ili_pne_visits = as.numeric(ili_pne_visits), ili_pne_admissions = as.numeric(ili_pne_admissions), month = factor(month(ymd(resp$date))) ) Visualize Data First I\u0026rsquo;m going to make a long version of the data so that I can group them by outcome and have a legend that shows which line is which and in different colors, I used pivot_longer() to achieve this. I also converted the date column from datetime to just date, so that I can later use the scale_x_date function in ggplot to scale my x axis.\n1 2 3 4 5 6 7 resp_long \u0026lt;- resp %\u0026gt;% pivot_longer(cols = c(total_ed_visits, ili_pne_visits, ili_pne_admissions), names_to = \u0026#34;outcome\u0026#34;, values_to = \u0026#34;count\u0026#34;) %\u0026gt;% mutate( date = as.Date(date) ) Then, I used ggplot to visualize the data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ggplot(data = resp_long, aes(x = date, y = count, color = outcome)) + geom_smooth(se = FALSE) + scale_color_discrete(name = \u0026#34;Outcome\u0026#34;, labels = c(\u0026#34;Admissions for ILI*\u0026#34;, \u0026#34;ED Visits for ILI*\u0026#34;, \u0026#34;All ED Visits\u0026#34;)) + labs(caption = \u0026#34;*Influenza Like Illness\u0026#34;, x = \u0026#34;Date\u0026#34;, y = \u0026#34;Count\u0026#34;, title = \u0026#34;Count of Emergency Department Visits (All Cause and ILI*) and Hospital Admissions for ILI*\u0026#34;) + scale_x_date(date_breaks = \u0026#34;4 months\u0026#34;) + theme_light() + theme(plot.title = element_text(hjust=0.5)) Poisson Regression Fit Model 1 2 3 4 pois.resp\u0026lt;- glm(ili_pne_admissions ~ month, data = resp, family = poisson) summary(pois.resp) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ## ## Call: ## glm(formula = ili_pne_admissions ~ month, family = poisson, data = resp) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.0974 -1.2311 -0.3399 0.5260 7.9668 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 1.15670 0.02425 47.706 \u0026lt; 2e-16 *** ## month2 -0.05197 0.03821 -1.360 0.174 ## month3 1.40761 0.02553 55.145 \u0026lt; 2e-16 *** ## month4 1.31247 0.02572 51.025 \u0026lt; 2e-16 *** ## month5 -0.83506 0.03479 -24.005 \u0026lt; 2e-16 *** ## month6 -1.73316 0.04833 -35.860 \u0026lt; 2e-16 *** ## month7 -1.44372 0.04373 -33.014 \u0026lt; 2e-16 *** ## month8 -1.10669 0.04060 -27.259 \u0026lt; 2e-16 *** ## month9 -1.68666 0.05373 -31.393 \u0026lt; 2e-16 *** ## month10 -1.43405 0.04957 -28.932 \u0026lt; 2e-16 *** ## month11 -0.92730 0.04361 -21.263 \u0026lt; 2e-16 *** ## month12 -0.21770 0.03533 -6.161 7.21e-10 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 93222 on 9999 degrees of freedom ## Residual deviance: 44040 on 9988 degrees of freedom ## AIC: 64530 ## ## Number of Fisher Scoring iterations: 5 Visualize Coefficients 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 coef \u0026lt;- c(coef(pois.resp)) coef_df \u0026lt;- data.frame( month = c(1:12), coefficient = coef ) ggplot(data = coef_df, aes(x = month, y = coef)) + geom_line(color = \u0026#34;#619CFF\u0026#34;, linewidth = 1) + geom_point(color = \u0026#34;black\u0026#34;) + scale_x_continuous(n.breaks = 12) + labs(x = \u0026#34;Month\u0026#34;, y = \u0026#34;Coefficient\u0026#34;, title = \u0026#34;Coefficients by Month for Pois.Resp Model\u0026#34;) + theme_light() + theme(plot.title = element_text(hjust=0.5)) Predict I created a random small data frame of 3 different observations of month, and then used the predict() function to see what values for the response variable would be output. I also rounded it up to an interger as there can\u0026rsquo;t be 0.6 admissions to the hospital.\n1 2 3 4 5 6 7 new_data \u0026lt;- data.frame( month = factor(c(1, 10, 4)) ) round(predict(pois.resp, new_data, type = \u0026#34;response\u0026#34;)) 1 2 ## 1 2 3 ## 3 1 12 Conclusion This was a short and sweet analysis on real life data that reinforced what I\u0026rsquo;ve been learning about poisson regression. I wanted to make my model multivariate but there was not much information in this dataset as it was only for one specific purpose. One thing that I could have done to make this more complex was to add more variables such as air quality information from the days that the data were collected, but I decided that that would involve more work than I wanted to put into this specific post.\nI was thinking about putting general ED visits and Influenza like Illness ED visits as predictors in the model, but those are not really independent from the Influenza like Illness Admissions.\nIt would also be interesting to see this data in a \u0026ldquo;normal\u0026rdquo; year as COVID definitely had an impact on the data and which months were related to Influenza like Illness admissions.\n","date":"2024-02-13T00:00:00Z","image":"https://michelleyg1.github.io/p/influenza-like-illness-hospital-admissions-poisson-regression/images/pois_hu5459c0360c2b0cb7a147d2df0eb350ca_2464222_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/influenza-like-illness-hospital-admissions-poisson-regression/","title":"Influenza Like Illness Hospital Admissions Poisson Regression"},{"content":"Load Libraries 1 2 3 4 5 6 suppressPackageStartupMessages({ library(tidyverse) library(janitor) library(lubridate) library(e1071) }) Load and Explore Data I found this huge and interesting dataset on NYC\u0026rsquo;s Open Data portal and wanted to use it to practice with the new knowledge and skills that I learned recently. My goal was to create a classifier model that can predict if motor vehicle collisions would result in injuries or not to anyone involved in the accident based on several predictors.\nI saw there was a way to query the API directly and get the data into R without downloading it as a csv, and I did figure out how to do that for other open datasets that were smaller, but I had trouble with this large dataset. I ended up using the NYC Open Data\u0026rsquo;s query tool to filter the dataset beforehand and downloaded it from there. I chose the borough of Queens to work with as there are many more highways and potential for accidents resulting in injuries there, rather than Manhattan where vehicles move very slowly through the grid shaped streets. I also filtered it for the calendar year of 2023 to further reduce the number of observations.\n1 2 3 motor_raw \u0026lt;- read.csv(\u0026#34;/Users/michellegulotta/Desktop/motor_raw.csv\u0026#34;) motor_raw \u0026lt;- motor_raw %\u0026gt;% janitor::clean_names(., \u0026#34;snake\u0026#34;) I wanted to also minimize missing values to deal with, so for the sake of simplicity I only focused on the data in the first columns for contributing factor and vehicle type.\n1 2 motor_cat \u0026lt;- motor_raw %\u0026gt;% dplyr::select(crash_date, crash_time, collision_id, number_of_persons_injured, number_of_persons_killed, contributing_factor_vehicle_1, vehicle_type_code_1) Now to check out the variables remaining along with the data types associated with each column.\n1 str(motor_cat) 1 2 3 4 5 6 7 8 ## \u0026#39;data.frame\u0026#39;:\t17822 obs. of 7 variables: ## $ crash_date : chr \u0026#34;11/16/2023\u0026#34; \u0026#34;11/17/2023\u0026#34; \u0026#34;11/17/2023\u0026#34; \u0026#34;11/12/2023\u0026#34; ... ## $ crash_time : chr \u0026#34;15:45\u0026#34; \u0026#34;6:57\u0026#34; \u0026#34;14:30\u0026#34; \u0026#34;10:58\u0026#34; ... ## $ collision_id : int 4679634 4679868 4679994 4679991 4663636 4663437 4629913 4631543 4631801 4632069 ... ## $ number_of_persons_injured : int 1 0 0 0 0 0 0 4 0 1 ... ## $ number_of_persons_killed : int 0 0 0 0 0 0 0 0 0 0 ... ## $ contributing_factor_vehicle_1: chr \u0026#34;Driver Inexperience\u0026#34; \u0026#34;Other Vehicular\u0026#34; \u0026#34;Unspecified\u0026#34; \u0026#34;Unspecified\u0026#34; ... ## $ vehicle_type_code_1 : chr \u0026#34;Motorcycle\u0026#34; \u0026#34;Sedan\u0026#34; \u0026#34;Station Wagon/Sport Utility Vehicle\u0026#34; \u0026#34;Station Wagon/Sport Utility Vehicle\u0026#34; ... Clean and Manipulate Data Time is stored as character and is in the 24 hour format, I just want the hour as I am going to use that to create a time of day category to use as a predictor. I also made a month category using the lubridate package, but I did not end up using this as a predictor, it was interesting to see how the lubridate package can help me in future projects.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 motor_cat \u0026lt;- motor_cat %\u0026gt;% mutate(hour = substr(motor_cat$crash_time, start = 1, stop = 2), across( .cols = c(contributing_factor_vehicle_1, vehicle_type_code_1), .fns = ~if_else(.x == \u0026#34;\u0026#34;, NA, .x) ), month = as.Date(crash_date, format = \u0026#34;%m/%d/%Y\u0026#34;)) %\u0026gt;% rename(type = vehicle_type_code_1, contrib = contributing_factor_vehicle_1) motor_cat$hour \u0026lt;- str_replace_all(motor_cat$hour, \u0026#34;:\u0026#34;, \u0026#34;\u0026#34;) motor_cat$hour \u0026lt;- as.integer(motor_cat$hour) motor_cat$month \u0026lt;- month(ymd(motor_cat$month)) I\u0026rsquo;m going to come back to this later, I want to do all of the category creating at once so I\u0026rsquo;m going to work on the contributing factor and vehicle type variables. In the previous code I renamed these variables to make the name shorter as I\u0026rsquo;m going to be renaming a lot of stuff later as you\u0026rsquo;ll see.\n1 motor_cat %\u0026gt;% tabyl(contrib) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 ## contrib n percent ## Accelerator Defective 13 7.294355e-04 ## Aggressive Driving/Road Rage 145 8.136012e-03 ## Alcohol Involvement 408 2.289305e-02 ## Animals Action 14 7.855460e-04 ## Backing Unsafely 691 3.877230e-02 ## Brakes Defective 69 3.871619e-03 ## Cell Phone (hand-Held) 11 6.172147e-04 ## Cell Phone (hands-free) 1 5.611043e-05 ## Driver Inattention/Distraction 4204 2.358882e-01 ## Driver Inexperience 372 2.087308e-02 ## Driverless/Runaway Vehicle 24 1.346650e-03 ## Drugs (illegal) 11 6.172147e-04 ## Eating or Drinking 3 1.683313e-04 ## Failure to Keep Right 35 1.963865e-03 ## Failure to Yield Right-of-Way 1830 1.026821e-01 ## Fatigued/Drowsy 27 1.514981e-03 ## Fell Asleep 117 6.564920e-03 ## Following Too Closely 988 5.543710e-02 ## Glare 20 1.122209e-03 ## Headlights Defective 1 5.611043e-05 ## Illnes 36 2.019975e-03 ## Lane Marking Improper/Inadequate 7 3.927730e-04 ## Lost Consciousness 50 2.805521e-03 ## Obstruction/Debris 27 1.514981e-03 ## Other Electronic Device 3 1.683313e-04 ## Other Lighting Defects 1 5.611043e-05 ## Other Vehicular 362 2.031197e-02 ## Outside Car Distraction 47 2.637190e-03 ## Oversized Vehicle 33 1.851644e-03 ## Passenger Distraction 34 1.907754e-03 ## Passing Too Closely 656 3.680844e-02 ## Passing or Lane Usage Improper 959 5.380990e-02 ## Pavement Defective 14 7.855460e-04 ## Pavement Slippery 60 3.366626e-03 ## Pedestrian/Bicyclist/Other Pedestrian Error/Confusion 129 7.238245e-03 ## Physical Disability 6 3.366626e-04 ## Prescription Medication 2 1.122209e-04 ## Reaction to Uninvolved Vehicle 125 7.013803e-03 ## Shoulders Defective/Improper 1 5.611043e-05 ## Steering Failure 53 2.973853e-03 ## Tinted Windows 4 2.244417e-04 ## Tire Failure/Inadequate 30 1.683313e-03 ## Tow Hitch Defective 2 1.122209e-04 ## Traffic Control Device Improper/Non-Working 6 3.366626e-04 ## Traffic Control Disregarded 742 4.163394e-02 ## Turning Improperly 468 2.625968e-02 ## Unsafe Lane Changing 193 1.082931e-02 ## Unsafe Speed 770 4.320503e-02 ## Unspecified 3754 2.106385e-01 ## Using On Board Navigation Device 1 5.611043e-05 ## Vehicle Vandalism 5 2.805521e-04 ## View Obstructed/Limited 150 8.416564e-03 ## \u0026lt;NA\u0026gt; 108 6.059926e-03 ## valid_percent ## 7.338828e-04 ## 8.185616e-03 ## 2.303263e-02 ## 7.903353e-04 ## 3.900869e-02 ## 3.895224e-03 ## 6.209778e-04 ## 5.645252e-05 ## 2.373264e-01 ## 2.100034e-02 ## 1.354861e-03 ## 6.209778e-04 ## 1.693576e-04 ## 1.975838e-03 ## 1.033081e-01 ## 1.524218e-03 ## 6.604945e-03 ## 5.577509e-02 ## 1.129050e-03 ## 5.645252e-05 ## 2.032291e-03 ## 3.951677e-04 ## 2.822626e-03 ## 1.524218e-03 ## 1.693576e-04 ## 5.645252e-05 ## 2.043581e-02 ## 2.653269e-03 ## 1.862933e-03 ## 1.919386e-03 ## 3.703286e-02 ## 5.413797e-02 ## 7.903353e-04 ## 3.387151e-03 ## 7.282376e-03 ## 3.387151e-04 ## 1.129050e-04 ## 7.056565e-03 ## 5.645252e-05 ## 2.991984e-03 ## 2.258101e-04 ## 1.693576e-03 ## 1.129050e-04 ## 3.387151e-04 ## 4.188777e-02 ## 2.641978e-02 ## 1.089534e-02 ## 4.346844e-02 ## 2.119228e-01 ## 5.645252e-05 ## 2.822626e-04 ## 8.467879e-03 ## NA I\u0026rsquo;m going to categorize these into much fewer categories to make it easier to create a model, how about now with the type of car as the primary vehicle involved in the accident.\n1 motor_cat %\u0026gt;% tabyl(type) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 ## type n percent valid_percent ## 2 dr sedan 1 5.611043e-05 5.703205e-05 ## 3-Door 2 1.122209e-04 1.140641e-04 ## 4 dr sedan 11 6.172147e-04 6.273526e-04 ## AMBU 2 1.122209e-04 1.140641e-04 ## AMBULANCE 15 8.416564e-04 8.554808e-04 ## AMBULENCE 2 1.122209e-04 1.140641e-04 ## Ambulance 78 4.376613e-03 4.448500e-03 ## Armored Truck 4 2.244417e-04 2.281282e-04 ## BOOM MOPED 1 5.611043e-05 5.703205e-05 ## Beverage Truck 2 1.122209e-04 1.140641e-04 ## Bike 205 1.150264e-02 1.169157e-02 ## Box Truck 176 9.875435e-03 1.003764e-02 ## Bulk Agriculture 3 1.683313e-04 1.710962e-04 ## Bus 247 1.385928e-02 1.408692e-02 ## Carry All 7 3.927730e-04 3.992244e-04 ## Chassis Cab 6 3.366626e-04 3.421923e-04 ## Commercial 1 5.611043e-05 5.703205e-05 ## Concrete Mixer 3 1.683313e-04 1.710962e-04 ## Convertible 27 1.514981e-03 1.539865e-03 ## Courier 1 5.611043e-05 5.703205e-05 ## DELIVERY T 1 5.611043e-05 5.703205e-05 ## DUMP 1 5.611043e-05 5.703205e-05 ## DUMP TRUCK 1 5.611043e-05 5.703205e-05 ## Dump 36 2.019975e-03 2.053154e-03 ## E-Bike 154 8.641005e-03 8.782936e-03 ## E-Scooter 75 4.208282e-03 4.277404e-03 ## Electric m 1 5.611043e-05 5.703205e-05 ## FDNY 1 5.611043e-05 5.703205e-05 ## FDNY AMBUL 2 1.122209e-04 1.140641e-04 ## FDNY FIRE 1 5.611043e-05 5.703205e-05 ## FDNY truck 1 5.611043e-05 5.703205e-05 ## FIRE ENGIN 1 5.611043e-05 5.703205e-05 ## FIRE TRUCK 3 1.683313e-04 1.710962e-04 ## FIRETRUCK 3 1.683313e-04 1.710962e-04 ## FLYWING MO 1 5.611043e-05 5.703205e-05 ## FOOD CART 1 5.611043e-05 5.703205e-05 ## FORK LIFT 1 5.611043e-05 5.703205e-05 ## FREIGHT VA 1 5.611043e-05 5.703205e-05 ## Fire Truck 1 5.611043e-05 5.703205e-05 ## Fire engin 1 5.611043e-05 5.703205e-05 ## Fire truck 1 5.611043e-05 5.703205e-05 ## Firetruck 3 1.683313e-04 1.710962e-04 ## Flat Bed 26 1.458871e-03 1.482833e-03 ## Flat Rack 7 3.927730e-04 3.992244e-04 ## Ford FF 1 5.611043e-05 5.703205e-05 ## Forklift 1 5.611043e-05 5.703205e-05 ## GARBAGE TR 2 1.122209e-04 1.140641e-04 ## GAS MOPED 1 5.611043e-05 5.703205e-05 ## GOLFCART 1 5.611043e-05 5.703205e-05 ## Garbage or Refuse 24 1.346650e-03 1.368769e-03 ## Government 1 5.611043e-05 5.703205e-05 ## LIMO 3 1.683313e-04 1.710962e-04 ## Ladder 1 5.611043e-05 5.703205e-05 ## Ladder tru 1 5.611043e-05 5.703205e-05 ## Lift Boom 1 5.611043e-05 5.703205e-05 ## MC 1 5.611043e-05 5.703205e-05 ## METRO TRAN 1 5.611043e-05 5.703205e-05 ## MOPED 5 2.805521e-04 2.851603e-04 ## MOTOR SCOO 1 5.611043e-05 5.703205e-05 ## MOTORIZEDS 1 5.611043e-05 5.703205e-05 ## Minicycle 1 5.611043e-05 5.703205e-05 ## Moped 94 5.274380e-03 5.361013e-03 ## Motorbike 11 6.172147e-04 6.273526e-04 ## Motorcycle 188 1.054876e-02 1.072203e-02 ## Motorscooter 21 1.178319e-03 1.197673e-03 ## Mta bus 1 5.611043e-05 5.703205e-05 ## Multi-Wheeled Vehicle 1 5.611043e-05 5.703205e-05 ## NYC FIRE T 1 5.611043e-05 5.703205e-05 ## OMS 1 5.611043e-05 5.703205e-05 ## Omnibus 1 5.611043e-05 5.703205e-05 ## Open Body 1 5.611043e-05 5.703205e-05 ## PAS 2 1.122209e-04 1.140641e-04 ## PK 34 1.907754e-03 1.939090e-03 ## Pick-up Truck 428 2.401526e-02 2.440972e-02 ## RV 2 1.122209e-04 1.140641e-04 ## Red moped 1 5.611043e-05 5.703205e-05 ## Refrigerated Van 1 5.611043e-05 5.703205e-05 ## Road sweep 1 5.611043e-05 5.703205e-05 ## SCHOOL BUS 1 5.611043e-05 5.703205e-05 ## SCHOOLBUS 1 5.611043e-05 5.703205e-05 ## SCOOTER 1 5.611043e-05 5.703205e-05 ## SELF INSUR 1 5.611043e-05 5.703205e-05 ## SPC 2 1.122209e-04 1.140641e-04 ## Sanitation 1 5.611043e-05 5.703205e-05 ## School Bus 2 1.122209e-04 1.140641e-04 ## Sedan 8491 4.764336e-01 4.842592e-01 ## Sprinter v 1 5.611043e-05 5.703205e-05 ## Stake or Rack 1 5.611043e-05 5.703205e-05 ## Station Wagon/Sport Utility Vehicle 6625 3.717316e-01 3.778373e-01 ## TOW TRUCK 1 5.611043e-05 5.703205e-05 ## TRAILER 2 1.122209e-04 1.140641e-04 ## TRUCK 1 5.611043e-05 5.703205e-05 ## Tanker 8 4.488834e-04 4.562564e-04 ## Taxi 251 1.408372e-02 1.431505e-02 ## Tow Truck 2 1.122209e-04 1.140641e-04 ## Tow Truck / Wrecker 17 9.538772e-04 9.695449e-04 ## Tow truck 1 5.611043e-05 5.703205e-05 ## Tractor Tr 1 5.611043e-05 5.703205e-05 ## Tractor Truck Diesel 64 3.591067e-03 3.650051e-03 ## Tractor Truck Gasoline 13 7.294355e-04 7.414167e-04 ## Trailer 2 1.122209e-04 1.140641e-04 ## Truck 3 1.683313e-04 1.710962e-04 ## UHAUL 1 5.611043e-05 5.703205e-05 ## USPS 1 5.611043e-05 5.703205e-05 ## USPS VEHIC 1 5.611043e-05 5.703205e-05 ## UTILITY 1 5.611043e-05 5.703205e-05 ## Van 74 4.152171e-03 4.220372e-03 ## Van Camper 1 5.611043e-05 5.703205e-05 ## Waste truc 1 5.611043e-05 5.703205e-05 ## ambulance 1 5.611043e-05 5.703205e-05 ## commerical 1 5.611043e-05 5.703205e-05 ## delv 1 5.611043e-05 5.703205e-05 ## forklift 1 5.611043e-05 5.703205e-05 ## pick up tr 1 5.611043e-05 5.703205e-05 ## van 2 1.122209e-04 1.140641e-04 ## \u0026lt;NA\u0026gt; 288 1.615980e-02 NA Creating Categories This dataset is a mess, I\u0026rsquo;m going to clean it up here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 motor_cat \u0026lt;- motor_cat %\u0026gt;% mutate(contrib_cat = as.factor(case_when( contrib == \u0026#34;Accelerator Defective\u0026#34; | contrib == \u0026#34;Brakes Defective\u0026#34; | contrib == \u0026#34;Steering Failure\u0026#34; | contrib == \u0026#34;Headlights Defective\u0026#34; | contrib == \u0026#34;Other Lighting Defects\u0026#34; | contrib == \u0026#34;Tinted Windows\u0026#34; | contrib == \u0026#34;Tire Failure/Inadequate\u0026#34; | contrib == \u0026#34;Tow Hitch Defective\u0026#34; ~ \u0026#34;Car Defects\u0026#34;, contrib == \u0026#34;Backing Unsafely\u0026#34; | contrib == \u0026#34;Driver Inexperience\u0026#34; | contrib == \u0026#34;Failure to Keep Right\u0026#34; | contrib == \u0026#34;Failure to Yield Right-of-Way\u0026#34; | contrib == \u0026#34;Following Too Closely\u0026#34; | contrib == \u0026#34;Passing Too Closely\u0026#34; | contrib == \u0026#34;Passing or Lane Usage Improper\u0026#34; | contrib == \u0026#34;Turning Improperly\u0026#34; ~ \u0026#34;Driver Error\u0026#34;, contrib == \u0026#34;Alcohol Involvement\u0026#34; | contrib == \u0026#34;Drugs (illegal)\u0026#34; | contrib == \u0026#34;Prescription Medication\u0026#34; ~ \u0026#34;Substances\u0026#34;, contrib == \u0026#34;Animals Action\u0026#34; | contrib == \u0026#34;Driverless/Runaway Vehicle\u0026#34; | contrib == \u0026#34;Glare\u0026#34; | contrib == \u0026#34;Lane Marking Improper/Inadequate\u0026#34; | contrib == \u0026#34;Obstruction/Debris\u0026#34; | contrib == \u0026#34;Pavement Defective\u0026#34; | contrib == \u0026#34;Pavement Slippery\u0026#34; | contrib == \u0026#34;Pedestrian/Bicyclist/Other Pedestrian Error/Confusion\u0026#34; | contrib == \u0026#34;Reaction to Uninvolved Vehicle\u0026#34; | contrib == \u0026#34;Shoulders Defective/Improper\u0026#34; | contrib == \u0026#34;Traffic Control Device Improper/Non-Working\u0026#34; | contrib == \u0026#34;View Obstructed/Limited\u0026#34; | contrib == \u0026#34;Vehicle Vandalism\u0026#34; ~ \u0026#34;Road Conditions\u0026#34;, contrib == \u0026#34;Cell Phone (hand-Held)\u0026#34; | contrib == \u0026#34;Cell Phone (hands-free)\u0026#34; | contrib == \u0026#34;Driver Inattention/Distraction\u0026#34; | contrib == \u0026#34;Eating or Drinking\u0026#34; | contrib == \u0026#34;Other Electronic Device\u0026#34; | contrib == \u0026#34;Outside Car Distraction\u0026#34; | contrib == \u0026#34;Using Onboard Navigation Device\u0026#34; ~ \u0026#34;Distracted Driver\u0026#34;, contrib == \u0026#34;Fatigued/Drowsy\u0026#34; | contrib == \u0026#34;Fell Asleep\u0026#34; | contrib == \u0026#34;Illnes\u0026#34; | contrib == \u0026#34;Lost Consciousness\u0026#34; | contrib == \u0026#34;Physical Disability\u0026#34; ~ \u0026#34;Condition of Driver\u0026#34;, contrib == \u0026#34;Aggressive Driving/Road Rage\u0026#34; | contrib == \u0026#34;Traffic Control Disregarded\u0026#34; | contrib == \u0026#34;Unsafe Lane Change\u0026#34; | contrib == \u0026#34;Unsafe Speed\u0026#34; ~ \u0026#34;Dangerous Driving\u0026#34;, contrib == \u0026#34;Unspecified\u0026#34; | is.na(contrib) | contrib == \u0026#34;Other Vehicular\u0026#34; ~ NA )), type_cat = as.factor(case_when( type == \u0026#34;2 dr sedan\u0026#34; | type == \u0026#34;3-Door\u0026#34; | type == \u0026#34;4 dr sedan\u0026#34; | type == \u0026#34;Sedan\u0026#34; ~ \u0026#34;Sedan\u0026#34;, type == \u0026#34;Taxi\u0026#34; ~ \u0026#34;Taxi\u0026#34;, type == \u0026#34;Refrigerated Van\u0026#34; | type == \u0026#34;Van\u0026#34; | type == \u0026#34;Van Camper\u0026#34; | type == \u0026#34;van\u0026#34; ~ \u0026#34;Van\u0026#34;, type == \u0026#34;Station Wagon/Sport Utility Vehicle\u0026#34; ~ \u0026#34;SUV\u0026#34;, type == \u0026#34;AMBU\u0026#34; | type == \u0026#34;AMBULANCE\u0026#34; | type == \u0026#34;AMBULENCE\u0026#34; | type == \u0026#34;Ambulance\u0026#34; | type == \u0026#34;FDNY AMBUL\u0026#34; | type == \u0026#34;ambulance\u0026#34; ~ \u0026#34;Ambulance\u0026#34;, type == \u0026#34;Armored Truck\u0026#34; | type == \u0026#34;Beverage Truck\u0026#34; | type == \u0026#34;Box Truck\u0026#34; | type == \u0026#34;Concrete Mixer\u0026#34; | type == \u0026#34;DELIVERY T\u0026#34; | type == \u0026#34;DUMP\u0026#34; | type == \u0026#34;RV\u0026#34; | type == \u0026#34;DUMP TRUCK\u0026#34; | type == \u0026#34;Dump\u0026#34; | type == \u0026#34;Flat Bed\u0026#34; | type == \u0026#34;Flat Rack\u0026#34; | type == \u0026#34;GARBAGE TR\u0026#34; | type == \u0026#34;Garbage or Refuse\u0026#34; | type == \u0026#34;Road sweep\u0026#34; | type == \u0026#34;Sanitation\u0026#34; | type == \u0026#34;TOW TRUCK\u0026#34; | type == \u0026#34;TRAILER\u0026#34; | type == \u0026#34;TRUCK\u0026#34; | type == \u0026#34;Tanker\u0026#34; | type == \u0026#34;Tow Truck\u0026#34; | type == \u0026#34;Tow Truck / Wrecker\u0026#34; | type == \u0026#34;Tow truck\u0026#34; | type == \u0026#34;Tractor Tr\u0026#34; | type == \u0026#34;Tractor Truck Diesel\u0026#34; | type == \u0026#34;Tractor Truck Gasoline\u0026#34; | type == \u0026#34;Trailer\u0026#34; | type == \u0026#34;Truck\u0026#34; | type == \u0026#34;UHAUL\u0026#34; | type == \u0026#34;USPS\u0026#34; | type == \u0026#34;USPS VEHIC\u0026#34; | type == \u0026#34;UTILITY\u0026#34; | type == \u0026#34;Waste truc\u0026#34; ~ \u0026#34;Truck\u0026#34;, type == \u0026#34;Chassis Cab\u0026#34; | type == \u0026#34;Pick-up Truck\u0026#34; | type == \u0026#34;Pick up tr\u0026#34; ~ \u0026#34;Pick Up Truck\u0026#34;, type == \u0026#34;BOOM MOPED\u0026#34; | type == \u0026#34;GAS MOPED\u0026#34; | type == \u0026#34;MOPED\u0026#34; | type == \u0026#34;Moped\u0026#34;| type == \u0026#34;Motorbike\u0026#34; | type == \u0026#34;Motorcycle\u0026#34; | type == \u0026#34;Red moped\u0026#34; ~ \u0026#34;Motorcycle/Moped\u0026#34;, type == \u0026#34;Bike\u0026#34; | type == \u0026#34;E-Bike\u0026#34; | type == \u0026#34;Electric m\u0026#34; | type == \u0026#34;FLYWING MO\u0026#34; ~ \u0026#34;Bike/E-Bike\u0026#34;, type == \u0026#34;E-Scooter\u0026#34; | type == \u0026#34;MOTOR SCOO\u0026#34; | type == \u0026#34;MOTORIZEDS\u0026#34; | type == \u0026#34;Motorscooter\u0026#34; | type == \u0026#34;SCOOTER\u0026#34; ~ \u0026#34;Scooter/E-Scooter\u0026#34;, type == \u0026#34;Bus\u0026#34; | type == \u0026#34;Mta bus\u0026#34; | type == \u0026#34;Omnibus\u0026#34; | type == \u0026#34;SCHOOL BUS\u0026#34; | type == \u0026#34;SCHOOLBUS\u0026#34; | type == \u0026#34;School Bus\u0026#34; ~ \u0026#34;Bus\u0026#34;, type == \u0026#34;FDNY\u0026#34; | type == \u0026#34;FDNY FIRE\u0026#34; | type == \u0026#34;FDNY truck\u0026#34; | type == \u0026#34;FIRE ENGIN\u0026#34; | type == \u0026#34;FIRE TRUCK\u0026#34; | type == \u0026#34;FIRETRUCK\u0026#34; | type == \u0026#34;Fire Truck\u0026#34; | type == \u0026#34;Fire engin\u0026#34; | type == \u0026#34;Fire truck\u0026#34; | type == \u0026#34;Firetruck\u0026#34; | type == \u0026#34;Ladder\u0026#34; | type == \u0026#34;Ladder tru\u0026#34;| type == \u0026#34;NYC Fire T\u0026#34; ~ \u0026#34;Fire Truck\u0026#34;, is.na(type) ~ NA, TRUE ~ \u0026#34;Other\u0026#34; )), time_of_day = as.factor(case_when( hour \u0026lt; 6 | hour \u0026gt; 20 ~ \u0026#34;Night\u0026#34;, hour \u0026lt; 12 ~ \u0026#34;Morning\u0026#34;, hour \u0026lt; 18 ~ \u0026#34;Afternoon\u0026#34;, hour \u0026lt;= 20 ~ \u0026#34;Evening\u0026#34; )), fatal_cat = as.factor(case_when( number_of_persons_killed \u0026gt; 0 ~ \u0026#34;Fatal\u0026#34;, TRUE ~ \u0026#34;Non Fatal\u0026#34;) ), injured_cat = as.factor(case_when( number_of_persons_injured \u0026gt; 0 ~ \u0026#34;Injuries\u0026#34;, TRUE ~ \u0026#34;No Injuries\u0026#34;) )) Phew, that was a lot of work. Now I\u0026rsquo;m going to check to make sure that I didn\u0026rsquo;t leave anything out and everything was either categorized or caught by the catch all at the end of the new type of vehicle category variable.\n1 motor_cat %\u0026gt;% tabyl(contrib_cat) 1 2 3 4 5 6 7 8 9 ## contrib_cat n percent valid_percent ## Car Defects 173 0.009707104 0.01297143 ## Condition of Driver 236 0.013242060 0.01769513 ## Dangerous Driving 1657 0.092974975 0.12424083 ## Distracted Driver 4269 0.239535406 0.32008698 ## Driver Error 5999 0.336606441 0.44980130 ## Road Conditions 582 0.032656268 0.04363800 ## Substances 421 0.023622489 0.03156632 ## \u0026lt;NA\u0026gt; 4485 0.251655258 NA Looks good, now the type of vehicle category.\n1 motor_cat %\u0026gt;% tabyl(type_cat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## type_cat n percent valid_percent ## Ambulance 100 0.005611043 0.005703205 ## Bike/E-Bike 361 0.020255864 0.020588571 ## Bus 253 0.014195938 0.014429109 ## Fire Truck 18 0.001009988 0.001026577 ## Motorcycle/Moped 301 0.016889238 0.017166648 ## Other 102 0.005723263 0.005817269 ## Pick Up Truck 434 0.024351925 0.024751911 ## Scooter/E-Scooter 99 0.005554932 0.005646173 ## Sedan 8505 0.477219167 0.485057602 ## SUV 6625 0.371731568 0.377837345 ## Taxi 251 0.014083717 0.014315045 ## Truck 407 0.022836943 0.023212045 ## Van 78 0.004376613 0.004448500 ## \u0026lt;NA\u0026gt; 288 0.016159802 NA Here\u0026rsquo;s the final dataset, I generally display it in a tibble format as the blogdown format seems to display only the first few rows of tibbles which I like as it does not take up too much space on my blog post.\n1 2 3 4 5 6 motor_cat_df \u0026lt;- motor_cat %\u0026gt;% filter(!is.na(contrib_cat) \u0026amp; !is.na(type_cat)) %\u0026gt;% dplyr::select(collision_id, month, hour, contrib_cat, type_cat, time_of_day, injured_cat) motor_cat_final \u0026lt;- as_tibble(motor_cat_df) motor_cat_final 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 13,150 × 7 ## collision_id month hour contrib_cat type_cat time_of_day injured_cat ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 4679634 11 15 Driver Error Motorcycl… Afternoon Injuries ## 2 4663636 9 7 Distracted Driver Sedan Morning No Injuries ## 3 4663437 9 0 Driver Error Sedan Night No Injuries ## 4 4629913 5 15 Driver Error Sedan Afternoon No Injuries ## 5 4631543 5 16 Driver Error SUV Afternoon Injuries ## 6 4631801 5 14 Driver Error SUV Afternoon No Injuries ## 7 4632066 5 15 Distracted Driver Pick Up T… Afternoon No Injuries ## 8 4631365 5 9 Distracted Driver Truck Morning No Injuries ## 9 4680340 11 18 Dangerous Driving Motorcycl… Evening Injuries ## 10 4631597 5 15 Driver Error SUV Afternoon No Injuries ## # ℹ 13,140 more rows Data Visualization I wanted to visualize the predictors that I am going to use in the model just to see how everything is distributed and if there are any obvious differences that the model might pick up on.\nContributing Factor 1 2 3 4 5 6 7 8 9 ggplot(data = motor_cat_final, aes(y = contrib_cat, fill = injured_cat)) + geom_bar(color = \u0026#34;dimgray\u0026#34;, alpha = 0.8) + labs(title = \u0026#34;Count of Accidents and Their Outcomes by Contributing Factor\u0026#34;, y = \u0026#34;Primary Contributing Factor of Accident\u0026#34;, x = \u0026#34;Count\u0026#34;, caption = \u0026#34;Source: NYC Open Data Motor Vehicle Collisions- Crashes (January-December 2023)\u0026#34;, fill = \u0026#34;Injury Outcome\u0026#34;) + theme(plot.title = element_text(hjust=0.5)) + theme_minimal() The most common primary cause of the accident is distracted driving and driver error, this especially makes sense as distracted driving is made so easy these days by all the electronic devices that we have available. Type of Vehicle 1 2 3 4 5 6 7 8 9 ggplot(data = motor_cat_final, aes(y = type_cat, fill = injured_cat)) + geom_bar(color = \u0026#34;dimgray\u0026#34;, alpha = 0.8) + labs(title = \u0026#34;Count of Accidents and Their Outcomes by Type of Primary Vehicle\u0026#34;, y = \u0026#34;Primary Vehicle Involved in of Accident\u0026#34;, x = \u0026#34;Count\u0026#34;, caption = \u0026#34;Source: NYC Open Data Motor Vehicle Collisions- Crashes (January-December 2023)\u0026#34;, fill = \u0026#34;Injury Outcome\u0026#34;) + theme(plot.title = element_text(hjust=0.5)) + theme_minimal() One thing that stuck out to me, which is not very surprising now that I think about it, is the proportion of accidents that resulted in injuries when a bike/e-bike or motorcycle/moped is involved. This really emphasizes the importance of being safe and fully attentive when on the road not just for other drivers, but for bikes and motorcycles too, as well as the importance of bike infrastructure that would keep bikes seperate from other vehicles that can hurt them. Time of Day 1 2 3 4 5 6 7 8 9 ggplot(data = motor_cat_final, aes(x = time_of_day, fill = injured_cat)) + geom_bar(color = \u0026#34;dimgray\u0026#34;, alpha = 0.8) + labs(title = \u0026#34;Count of Accidents and Their Outcomes by Time of Day\u0026#34;, x = \u0026#34;Time of Day that Accident Occurred\u0026#34;, y = \u0026#34;Count\u0026#34;, caption = \u0026#34;Source: NYC Open Data Motor Vehicle Collisions- Crashes (January-December 2023)\u0026#34;, fill = \u0026#34;Injury Outcome\u0026#34;) + theme(plot.title = element_text(hjust=0.5)) + theme_minimal() Just inspecting this visually it does not look like there is that big of a difference between the different times of day and the proportion of accidents that result in injuries. Classification Models to Predict Injury Outcome of Motor Vehicle Collision Create Training and Test Data In order to create and test my model that classifies if an accident results in injuries or not for anyone involved, I\u0026rsquo;m going to seperate my training and test data by taking an arbitrary amount of observations to be considered test and training using the sample_frac() and anti_join() functions.\n1 2 3 4 5 6 7 8 set.seed(123) training \u0026lt;- motor_cat_df %\u0026gt;% sample_frac(.70) test \u0026lt;- motor_cat_df %\u0026gt;% anti_join(training, by = \u0026#34;collision_id\u0026#34;) training_injury_outcome \u0026lt;- training$injured_cat test_injury_outcome \u0026lt;- test$injured_cat Logistic Regression The first model that I wanted to fit was a logistic regression model, as there are only two different outcomes in the data (injuries vs. no injuries). Here I fitted that model with my predictors: contributing factor, type of vehicle, and time of day, along with my response variable which was if the accident resulted in injuries or not.\n1 2 3 4 glm.inj \u0026lt;- glm(injured_cat ~ contrib_cat + type_cat + time_of_day, data = training, family = binomial) summary(glm.inj) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 ## ## Call: ## glm(formula = injured_cat ~ contrib_cat + type_cat + time_of_day, ## family = binomial, data = training) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0102 -1.2497 0.8528 1.0539 2.3111 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 1.74107 0.48089 3.621 0.000294 *** ## contrib_catCondition of Driver -0.53689 0.24739 -2.170 0.029991 * ## contrib_catDangerous Driving -0.38954 0.19907 -1.957 0.050375 . ## contrib_catDistracted Driver 0.09706 0.19357 0.501 0.616072 ## contrib_catDriver Error -0.03027 0.19242 -0.157 0.874993 ## contrib_catRoad Conditions -0.53907 0.21675 -2.487 0.012880 * ## contrib_catSubstances 0.24526 0.22726 1.079 0.280488 ## type_catBike/E-Bike -3.93770 0.49501 -7.955 1.79e-15 *** ## type_catBus -1.76695 0.47378 -3.729 0.000192 *** ## type_catFire Truck -0.91710 0.94812 -0.967 0.333402 ## type_catMotorcycle/Moped -3.50744 0.49026 -7.154 8.42e-13 *** ## type_catOther -1.65302 0.50726 -3.259 0.001119 ** ## type_catPick Up Truck -1.30525 0.46299 -2.819 0.004814 ** ## type_catScooter/E-Scooter -3.41098 0.54718 -6.234 4.55e-10 *** ## type_catSedan -1.54048 0.44326 -3.475 0.000510 *** ## type_catSUV -1.66967 0.44347 -3.765 0.000167 *** ## type_catTaxi -2.37742 0.47385 -5.017 5.24e-07 *** ## type_catTruck -1.11660 0.46499 -2.401 0.016336 * ## type_catVan -1.29899 0.56568 -2.296 0.021658 * ## time_of_dayEvening -0.01271 0.06711 -0.189 0.849830 ## time_of_dayMorning 0.27367 0.05781 4.734 2.20e-06 *** ## time_of_dayNight 0.52662 0.05700 9.238 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 12644 on 9204 degrees of freedom ## Residual deviance: 12059 on 9183 degrees of freedom ## AIC: 12103 ## ## Number of Fisher Scoring iterations: 4 Then I used the predict() function to predict the outcome using my test data, and compared the real data along with the predicted data using a confusion matrix.\n1 2 3 4 5 6 7 inj.probs \u0026lt;- predict(glm.inj, test, type = \u0026#34;response\u0026#34;) inj.pred \u0026lt;- rep(\u0026#34;No Injuries\u0026#34;, length(inj.probs)) inj.pred[inj.probs \u0026gt; 0.5] \u0026lt;- \u0026#34;Injuries\u0026#34; table(inj.pred, test_injury_outcome) 1 2 3 4 ## test_injury_outcome ## inj.pred Injuries No Injuries ## Injuries 1304 1980 ## No Injuries 422 239 1 mean(inj.pred == test_injury_outcome) 1 ## [1] 0.391128 Yikes, that\u0026rsquo;s worse than just random guessing. I\u0026rsquo;m going to use a different model now and see how that compares to the logistic regression.\nNaive Bayes I chose to use the naive bayes model as it does not assume any particular distribution in the data. I fit the model using the naiveBayes() function from the e1071 package and displayed the output of the model object below.\n1 2 3 nb.inj \u0026lt;- naiveBayes(injured_cat ~ contrib_cat + type_cat + time_of_day, data = training) nb.inj 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## Injuries No Injuries ## 0.4437806 0.5562194 ## ## Conditional probabilities: ## contrib_cat ## Y Car Defects Condition of Driver Dangerous Driving ## Injuries 0.01126071 0.02105263 0.14369645 ## No Injuries 0.01445313 0.01484375 0.10761719 ## contrib_cat ## Y Distracted Driver Driver Error Road Conditions Substances ## Injuries 0.29987760 0.44504284 0.05483476 0.02423501 ## No Injuries 0.34003906 0.45058594 0.03339844 0.03906250 ## ## type_cat ## Y Ambulance Bike/E-Bike Bus Fire Truck ## Injuries 0.0014687882 0.0438188494 0.0166462668 0.0004895961 ## No Injuries 0.0074218750 0.0044921875 0.0142578125 0.0009765625 ## type_cat ## Y Motorcycle/Moped Other Pick Up Truck Scooter/E-Scooter ## Injuries 0.0337821297 0.0073439412 0.0205630355 0.0127294982 ## No Injuries 0.0052734375 0.0072265625 0.0292968750 0.0023437500 ## type_cat ## Y Sedan SUV Taxi Truck Van ## Injuries 0.4411260710 0.3801713586 0.0212974296 0.0173806610 0.0031823745 ## No Injuries 0.5011718750 0.3804687500 0.0117187500 0.0310546875 0.0042968750 ## ## time_of_day ## Y Afternoon Evening Morning Night ## Injuries 0.3618115 0.1703794 0.2296206 0.2381885 ## No Injuries 0.2933594 0.1333984 0.2527344 0.3205078 Now to check the model to see how it compares to my worse-than-random-guessing logistic regression model.\n1 2 nb.inj.pred \u0026lt;- predict(nb.inj, test) table(nb.inj.pred, test_injury_outcome) 1 2 3 4 ## test_injury_outcome ## nb.inj.pred Injuries No Injuries ## Injuries 461 263 ## No Injuries 1265 1956 1 mean(nb.inj.pred == test_injury_outcome) 1 ## [1] 0.6126743 Well that\u0026rsquo;s not great, but it\u0026rsquo;s definitely an improvement from the previous logistic regression model.\nImprove Logistic Regression Model It looks like the primary contributing factor does not really have much statistical significance if there are injuries in a motor vehicle accident or not, I\u0026rsquo;m going to remove that variable from each model and see how that changes things.\n1 2 3 4 glm.inj.new \u0026lt;- glm(injured_cat ~ type_cat + time_of_day, data = training, family = binomial) summary(glm.inj.new) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 ## ## Call: ## glm(formula = injured_cat ~ type_cat + time_of_day, family = binomial, ## data = training) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1353 -1.2387 0.9115 1.0588 2.1639 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 1.651322 0.441470 3.741 0.000184 *** ## type_catBike/E-Bike -3.888045 0.493421 -7.880 3.28e-15 *** ## type_catBus -1.732093 0.472146 -3.669 0.000244 *** ## type_catFire Truck -0.806790 0.946818 -0.852 0.394155 ## type_catMotorcycle/Moped -3.482104 0.488651 -7.126 1.03e-12 *** ## type_catOther -1.608261 0.505064 -3.184 0.001451 ** ## type_catPick Up Truck -1.283938 0.461426 -2.783 0.005393 ** ## type_catScooter/E-Scooter -3.353955 0.545475 -6.149 7.81e-10 *** ## type_catSedan -1.508263 0.441687 -3.415 0.000638 *** ## type_catSUV -1.633338 0.441928 -3.696 0.000219 *** ## type_catTaxi -2.305245 0.471983 -4.884 1.04e-06 *** ## type_catTruck -1.076075 0.463503 -2.322 0.020254 * ## type_catVan -1.271352 0.563454 -2.256 0.024048 * ## time_of_dayEvening -0.003336 0.066715 -0.050 0.960122 ## time_of_dayMorning 0.267486 0.057502 4.652 3.29e-06 *** ## time_of_dayNight 0.520432 0.055919 9.307 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 12644 on 9204 degrees of freedom ## Residual deviance: 12142 on 9189 degrees of freedom ## AIC: 12174 ## ## Number of Fisher Scoring iterations: 4 Now checking again\n1 2 3 4 5 6 7 inj.probs.new \u0026lt;- predict(glm.inj.new, test, type = \u0026#34;response\u0026#34;) inj.pred.new \u0026lt;- rep(\u0026#34;No Injuries\u0026#34;, length(inj.probs.new)) inj.pred.new[inj.probs.new \u0026gt; 0.5] \u0026lt;- \u0026#34;Injuries\u0026#34; table(inj.pred.new, test_injury_outcome) 1 2 3 4 ## test_injury_outcome ## inj.pred.new Injuries No Injuries ## Injuries 1524 2156 ## No Injuries 202 63 1 mean(inj.pred.new == test_injury_outcome) 1 ## [1] 0.4022814 Still worse than guessing even though this does result in a slight improvement, but the Naive Bayes model still remains on top.\nConclusion I learned so many new things trying to practice the skills that I\u0026rsquo;ve learned using classification models. I\u0026rsquo;m starting to become more familiar with the code on how to make predictions and check a classification model. Also, I learned a different way on how to seperate test and training data using the sample_frac() function in conjunction with the anti_join() function to create two different data frames that have a random fraction of observations in one data frame, and then the ones that were not selected in the other data frame.\nI also see how messy working with real life data rather than data from an R package or a textbook is, this dataset is so huge and I only used a very pared down version of it. The original has millions of observations! There are also a lot of instances of human error and just differences in recording observations in the contributing factor and type of vehicle columns.\n","date":"2024-02-12T00:00:00Z","image":"https://michelleyg1.github.io/p/accident-classification/images/bridge_hu5459c0360c2b0cb7a147d2df0eb350ca_3648849_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/accident-classification/","title":"NYC Open Data Motor Vehicle Collision Injury Outcome Classification"},{"content":"Currently, I\u0026rsquo;m working through this great book An Introduction to Statistical Learning with Applications in R, and I just finished the chapter on linear regression. One thing that I really like about this book is that there is a great balance of learning R and just learning statistics in a mathematical way which really shows the backbone of the R functions that I was just using without really knowing how they work.\nFor this analysis I used the built in dataset \u0026ldquo;airquality\u0026rdquo; in R which stores 153 observations of different air quality measures averaged out by day taken in New York City from May to September 1973. I used this to practice my skills in linear regression, both simple and multivariate.\nLoad Libraries 1 2 3 4 5 6 suppressPackageStartupMessages({ library(tidyverse) library(broom) library(car) library(ggthemes) }) Load Data 1 data(\u0026#34;airquality\u0026#34;) Explore and Clean Data 1 str(airquality) 1 2 3 4 5 6 7 ## \u0026#39;data.frame\u0026#39;:\t153 obs. of 6 variables: ## $ Ozone : int 41 36 12 18 NA 28 23 19 8 NA ... ## $ Solar.R: int 190 118 149 313 NA NA 299 99 19 194 ... ## $ Wind : num 7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ... ## $ Temp : int 67 72 74 62 56 66 65 59 61 69 ... ## $ Month : int 5 5 5 5 5 5 5 5 5 5 ... ## $ Day : int 1 2 3 4 5 6 7 8 9 10 ... Summary Statistics Creating a table of summary statistics that also doubles as a way to check for missing values, as since this dataset only has 153 observations so substantial amounts of missing data will affect the outcomes.\n1 2 3 4 airquality %\u0026gt;% dplyr::select(Ozone, Solar.R, Wind, Temp) %\u0026gt;% map_df(.f = ~broom::tidy(summary(.x)), .id = \u0026#34;variable\u0026#34;) 1 2 3 4 5 6 7 ## # A tibble: 4 × 8 ## variable minimum q1 median mean q3 maximum na ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Ozone 1 18 31.5 42.1 63.2 168 37 ## 2 Solar.R 7 116. 205 186. 259. 334 7 ## 3 Wind 1.7 7.4 9.7 9.96 11.5 20.7 NA ## 4 Temp 56 72 79 77.9 85 97 NA So it looks like there are a decent amount of missing values for the ozone variable, I\u0026rsquo;m going to use mean imputation to deal with this.\nMean Imputation for Missing Values 1 2 3 4 5 6 7 8 9 10 airquality_mi \u0026lt;- airquality %\u0026gt;% mutate( ozone_mi = as.integer(if_else(is.na(Ozone), mean(Ozone, na.rm = TRUE), Ozone)), solar_mi = as.integer(if_else(is.na(Solar.R), mean(Solar.R, na.rm = TRUE), Solar.R)) ) %\u0026gt;% select(-1, -2) Clean Up and Print 1 2 3 4 5 6 names(airquality_mi)[1:6] \u0026lt;- tolower(names(airquality_mi)[1:6]) airquality_mi \u0026lt;- airquality_mi %\u0026gt;% select(month, day, wind, temp, ozone_mi, solar_mi) as_tibble(airquality_mi) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 153 × 6 ## month day wind temp ozone_mi solar_mi ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 5 1 7.4 67 41 190 ## 2 5 2 8 72 36 118 ## 3 5 3 12.6 74 12 149 ## 4 5 4 11.5 62 18 313 ## 5 5 5 14.3 56 42 185 ## 6 5 6 14.9 66 28 185 ## 7 5 7 8.6 65 23 299 ## 8 5 8 13.8 59 19 99 ## 9 5 9 20.1 61 8 19 ## 10 5 10 8.6 69 42 194 ## # ℹ 143 more rows Simple Regression Visualize Variables of Interest 1 2 3 4 5 6 7 8 9 10 ggplot(data = airquality_mi, aes(x = temp, y = ozone_mi)) + geom_jitter() + labs( x = \u0026#34;Temperature Measured in Fahrenheit\u0026#34;, y = \u0026#34;Mean Ozone per Day in Parts Per Billion\u0026#34;, title = \u0026#34;Ozone Concentration vs. Temperature in New York City from May to September 1973\u0026#34; ) + theme_stata() + theme(plot.title = element_text(hjust = 0.5)) Fit Model 1 2 aq.simple \u0026lt;- lm(ozone_mi ~ temp, data = airquality_mi) summary(aq.simple) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## ## Call: ## lm(formula = ozone_mi ~ temp, data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.006 -15.851 -2.160 8.469 120.149 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -101.6222 15.3547 -6.618 5.96e-10 *** ## temp 1.8454 0.1957 9.428 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 22.84 on 151 degrees of freedom ## Multiple R-squared: 0.3706,\tAdjusted R-squared: 0.3664 ## F-statistic: 88.9 on 1 and 151 DF, p-value: \u0026lt; 2.2e-16 Looking at the summary of this model its already pretty evident that there are some issues, the residual standard error is pretty high and the R squared shows that a majority of variation cannot be explained by the model. I\u0026rsquo;m also going to run some diagnostic plots to see if there are other issues.\nDiagnostics 1 2 par(mfrow = c(2, 2)) plot(aq.simple) These plots suggest that there may be a non linear relationship between the variables, so I'm going to apply a transformation and see if that helps. Polynomial Transformation 1 2 aq.simple.trans \u0026lt;- lm(ozone_mi ~ temp + I(temp^2), data = airquality_mi) summary(aq.simple.trans) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## ## Call: ## lm(formula = ozone_mi ~ temp + I(temp^2), data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.463 -13.131 -2.735 9.869 124.916 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 296.43794 94.87393 3.125 0.002138 ** ## temp -8.78275 2.50999 -3.499 0.000615 *** ## I(temp^2) 0.06981 0.01644 4.246 3.8e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 21.65 on 150 degrees of freedom ## Multiple R-squared: 0.4381,\tAdjusted R-squared: 0.4306 ## F-statistic: 58.47 on 2 and 150 DF, p-value: \u0026lt; 2.2e-16 So that\u0026rsquo;s a bit better in terms of R squared, I\u0026rsquo;m going to re-check the diagnostic plots.\n1 2 par(mfrow = c(2, 2)) plot(aq.simple.trans) This is not the best model for the data, but being able to recognize that is also important. I\u0026rsquo;m going to visualize the regression line on my previous plot for the sake of practice.\nVisualize with Least Squares Regression Line 1 2 3 4 5 6 7 8 9 10 11 12 ggplot(data = airquality_mi, aes(x = temp, y = ozone_mi)) + geom_jitter() + stat_smooth(method = \u0026#34;lm\u0026#34;, formula = y ~ poly(x, 2, raw = TRUE)) + labs( x = \u0026#34;Temperature Measured in Fahrenheit\u0026#34;, y = \u0026#34;Mean Ozone per Day in Parts Per Billion\u0026#34;, title = \u0026#34;Ozone Concentration vs. Temperature in New York City from May to September 1973\u0026#34; ) + theme_stata() + theme(plot.title = element_text(hjust = 0.5)) Multivariate Regression I also want to do a multivariate regression with this data to see if I can come up with a better model and be able to practice making predictions using a regression model.\nFit Model 1 2 aq.mv \u0026lt;- lm(ozone_mi ~ temp + wind + solar_mi, data = airquality_mi) summary(aq.mv) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## ## Call: ## lm(formula = ozone_mi ~ temp + wind + solar_mi, data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.541 -14.590 -5.148 12.172 101.198 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -38.20875 18.88354 -2.023 0.04482 * ## temp 1.24096 0.20907 5.935 1.97e-08 *** ## wind -2.71862 0.54280 -5.009 1.53e-06 *** ## solar_mi 0.05772 0.02003 2.881 0.00454 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 20.9 on 149 degrees of freedom ## Multiple R-squared: 0.48,\tAdjusted R-squared: 0.4696 ## F-statistic: 45.85 on 3 and 149 DF, p-value: \u0026lt; 2.2e-16 Diagnostics The first thing I want to check since I have multiple predictor variables in my new model is colinearity. I\u0026rsquo;m going to use the vif function to calculate the variance inflation factor to see if there are any variables that are correlated too strongly.\n1 vif(aq.mv) 1 2 ## temp wind solar_mi ## 1.363082 1.272795 1.080454 Looks good! Moving on to normal diagnostic plots\n1 2 par(mfrow = c(2, 2)) plot(aq.mv) This model could use some work to get it to fit the data and assumptions a bit better, I'm going to see if doing some transformations of the predictor helps. Transformation First I\u0026rsquo;m going to visualize my individual predictor variables with the response variables just to see what I\u0026rsquo;m working with and which transformations might help me.\nI already visualized ozone and temperature in the simple regression, so I\u0026rsquo;m going to visualize ozone and wind.\n1 2 3 4 5 6 7 8 9 10 ggplot(data = airquality_mi, aes(x = wind, y = ozone_mi)) + geom_jitter() + labs( x = \u0026#34;Average Wind Speed Measured in Miles Per Hour\u0026#34;, y = \u0026#34;Mean Ozone per Day in Parts Per Billion\u0026#34;, title = \u0026#34;Ozone Concentration vs. Wind in New York City from May to September 1973\u0026#34; ) + theme_stata() + theme(plot.title = element_text(hjust = 0.5)) And ozone and solar.\n1 2 3 4 5 6 7 8 9 10 ggplot(data = airquality_mi, aes(x = solar_mi, y = ozone_mi)) + geom_jitter() + labs( x = \u0026#34;Solar Radiation Measured in Langleys\u0026#34;, y = \u0026#34;Mean Ozone per Day in Parts Per Billion\u0026#34;, title = \u0026#34;Ozone Concentration vs. Solar Radiation in New York City from May to September 1973\u0026#34; ) + theme_stata() + theme(plot.title = element_text(hjust = 0.5)) I\u0026rsquo;m going to apply polynomial transformations to two out of the three predictor variables.\n1 2 aq.mv.trans \u0026lt;- lm(ozone_mi ~ temp + I(temp^2) + wind + I(wind^2) + solar_mi, data = airquality_mi) summary(aq.mv.trans) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## ## Call: ## lm(formula = ozone_mi ~ temp + I(temp^2) + wind + I(wind^2) + ## solar_mi, data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.293 -10.929 -3.289 8.684 89.513 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 321.25924 81.78038 3.928 0.000131 *** ## temp -7.28999 2.18385 -3.338 0.001069 ** ## I(temp^2) 0.05580 0.01431 3.898 0.000147 *** ## wind -11.10077 1.94439 -5.709 6.07e-08 *** ## I(wind^2) 0.39616 0.08865 4.469 1.56e-05 *** ## solar_mi 0.06206 0.01772 3.503 0.000610 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 18.46 on 147 degrees of freedom ## Multiple R-squared: 0.5998,\tAdjusted R-squared: 0.5862 ## F-statistic: 44.07 on 5 and 147 DF, p-value: \u0026lt; 2.2e-16 This R2 and RSE is definitely better than the simple regression model, as now almost 60% of the variation in the data can be explained by the model rather than being closer to 40%, still not the best but definitely better. Now I\u0026rsquo;m going to re-check the diagnostics to see if this satisfies the assumptions a bit better.\n1 2 par(mfrow = c(2, 2)) plot(aq.mv.trans) This definitely helped deal with the pattern in the residuals vs. fitted plot and helps satisfy the linearity assumption of the model. Make Predictions I\u0026rsquo;m going to use this model to make a couple predictions about ozone levels when given information about temperature, solar radiation, and wind speeds, along with confidence intervals.\n1 2 3 4 5 6 7 8 9 10 11 new_data_point \u0026lt;- data.frame( solar_mi = c(250, 120, 320), temp = c(63, 74, 82), wind = c(10.2, 9.5, 7.0) ) predict( aq.mv.trans, new_data_point, interval = \u0026#34;confidence\u0026#34; ) 1 2 3 4 ## fit lwr upr ## 1 26.96636 19.31629 34.61643 ## 2 25.10863 20.28017 29.93708 ## 3 60.25022 54.08080 66.41964 Conclusion I\u0026rsquo;ve tried to perform linear regressions before, as it is very simple to do in R but I really didn\u0026rsquo;t understand any of the outputs of the model or what constitutes as a good model, but the Linear Regression chapter of Intro to Statistical Learning along with running this analysis helped me understand those concepts better.\nI also, until then, didn\u0026rsquo;t really understand the importance of linear regression and thought that it was just similar to correlation, but we can actually use linear regression in order to input new data to get a prediction for the response variable, in this case the amount of ozone in the atmosphere.\nIn working on this analysis I also learned about mean imputation when dealing with missing values, which has its advantages and disadvantages compared to just removing rows with missing values.\n","date":"2024-02-02T00:00:00Z","image":"https://michelleyg1.github.io/p/airquality/images/smoke_hu5459c0360c2b0cb7a147d2df0eb350ca_571128_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/airquality/","title":"Air Quality Regression"},{"content":"One thing that I’ve learned since I’ve been looking into working with large complex survey data like the CDC’s NHANES is about survey weights and how to use them in your analysis. In this analysis I tried to use the survey package in R to use the survey design to analyze the data, as there are certain demographics that were oversampled and undersampled as a part of the survey design.\nLoad Packages 1 2 3 4 5 6 7 suppressPackageStartupMessages({ library(tidyverse) library(nhanesA) library(janitor) library(survey) library(gtsummary) }) Load Data 1 2 demo \u0026lt;- nhanes(\u0026#34;P_DEMO\u0026#34;) demo \u0026lt;- nhanesTranslate(\u0026#34;P_DEMO\u0026#34;, names(demo), data = demo) ## Translated columns: RIDSTATR RIAGENDR RIDRETH1 RIDRETH3 RIDEXMON DMDBORN4 DMDYRUSZ DMDEDUC2 DMDMARTZ RIDEXPRG SIALANG SIAPROXY SIAINTRP FIALANG FIAPROXY FIAINTRP MIALANG MIAPROXY MIAINTRP AIALANGA 1 2 exam \u0026lt;- nhanes(\u0026#34;P_BMX\u0026#34;) exam \u0026lt;- nhanesTranslate(\u0026#34;P_BMX\u0026#34;, names(exam), data = exam) ## Translated columns: BMDSTATS BMIWT BMIHT BMDBMIC Retain Useful Variables 1 2 3 4 5 demo_select \u0026lt;- demo %\u0026gt;% select(SEQN, RIAGENDR, RIDAGEYR, RIDRETH3, SDMVPSU, SDMVSTRA, WTMECPRP) exam_select \u0026lt;- exam %\u0026gt;% select(SEQN, BMXBMI) Merge Data 1 2 3 merged_data \u0026lt;- merge(demo_select, exam_select, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) merged_data$SEQN \u0026lt;- NULL Clean Dataset to Make Analysis Easier First I wanted to see which race categories there are so that I can capture all of them when I recode them into more simple categories.\n1 merged_data %\u0026gt;% tabyl(RIDRETH3) ## RIDRETH3 n percent ## Mexican American 1990 0.12789203 ## Other Hispanic 1544 0.09922879 ## Non-Hispanic White 5271 0.33875321 ## Non-Hispanic Black 4098 0.26336761 ## Non-Hispanic Asian 1638 0.10526992 ## Other Race - Including Multi-Racial 1019 0.06548843 Now, I’m going to rename some variables, and use case_when() to make the race_cat and bmi_cat columns, as well as factoring these columns so that they can be used in my analysis and are not just recognized as character strings by R.\nI also added at the end to output a tibble of the raw NHANES data without the weights to print out how my clean data looks and make sure that I didn’t miss any columns in the renaming process.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 nhanes \u0026lt;- merged_data %\u0026gt;% rename( gender = RIAGENDR, age = RIDAGEYR, bmi = BMXBMI, psu = SDMVPSU, strata = SDMVSTRA, weight = WTMECPRP) %\u0026gt;% mutate( race_cat = case_when(RIDRETH3 == \u0026#34;Mexican American\u0026#34; | RIDRETH3 == \u0026#34;Other Hispanic\u0026#34; ~ \u0026#34;Hispanic\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic White\u0026#34; ~ \u0026#34;White\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic Black\u0026#34; ~ \u0026#34;Black\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic Asian\u0026#34; ~ \u0026#34;Asian\u0026#34;, RIDRETH3 == \u0026#34;Other Race - Including Multi-Racial\u0026#34; ~ \u0026#34;Other\u0026#34;), bmi_cat = case_when(bmi \u0026lt; 18.5 ~ \u0026#34;Underweight\u0026#34;, bmi \u0026lt; 25 ~ \u0026#34;Normal\u0026#34;, bmi \u0026lt; 30 ~ \u0026#34;Overweight\u0026#34;, bmi \u0026gt;= 30 ~ \u0026#34;Obese\u0026#34;) ) %\u0026gt;% filter(!is.na(bmi)) %\u0026gt;% mutate_if(., is.character, as.factor) %\u0026gt;% select(-RIDRETH3) # Unweighted nhanes_tibble \u0026lt;- as_tibble(nhanes) %\u0026gt;% filter(age \u0026gt;= 18) %\u0026gt;% select(-psu, -strata, -weight) %\u0026gt;% print() ## # A tibble: 8,790 × 5 ## gender age bmi race_cat bmi_cat ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Female 29 37.8 Asian Obese ## 2 Male 49 29.7 White Overweight ## 3 Male 36 21.9 White Normal ## 4 Male 68 30.2 Other Obese ## 5 Male 76 26.6 White Overweight ## 6 Female 44 39.1 Hispanic Obese ## 7 Female 33 28.9 Asian Overweight ## 8 Female 68 28.1 Black Overweight ## 9 Female 42 31.3 Asian Obese ## 10 Male 58 30.5 Hispanic Obese ## # ℹ 8,780 more rows Refactor Order of BMI Categories R shows factored variables autmatically in alphabetical order, but this did not make much sense for my chart as BMI categories make more sense from underweight to obese, so I used the factor() function to remedy this before I make my table.\n1 2 nhanes$bmi_cat \u0026lt;- factor(nhanes$bmi_cat, levels = c(\u0026#34;Underweight\u0026#34;, \u0026#34;Normal\u0026#34;, \u0026#34;Overweight\u0026#34;, \u0026#34;Obese\u0026#34;)) Survey Design I learned that in order to properly analyze large survey data, and to make it representative of the population at large, you have to create a survey design object and use that in your analysis.\n1 2 3 4 5 nhanes_design \u0026lt;- svydesign(id = ~psu, strata = ~strata, weights = ~weight, nest = TRUE, data = nhanes) Apply Eligibility Criteria It is also important that the data is subset in this manner when working with survey data.\n1 nhanes_adult \u0026lt;- subset(nhanes_design, age \u0026gt;= 18) Summary Statistics BMI Categories by Race/Hispanic Origin, Gender, and Average Age Weighted 1 2 3 4 5 6 7 8 tbl_svysummary(nhanes_adult, by = bmi_cat, include = c(gender, age, race_cat), label = list(gender ~ \u0026#34;Gender\u0026#34;, age ~ \u0026#34;Age\u0026#34;, race_cat ~ \u0026#34;Race/Hispanic Origin\u0026#34;), statistic = list(age ~ \u0026#34;{mean} ({sd})\u0026#34;) ) \u0026#10; Characteristic Underweight, N = 3,797,8841 Normal, N = 62,452,8711 Overweight, N = 76,984,8391 Obese, N = 101,342,0061 Gender Male 1,148,289 (30%) 27,234,973 (44%) 41,256,750 (54%) 48,289,482 (48%) Female 2,649,595 (70%) 35,217,899 (56%) 35,728,089 (46%) 53,052,524 (52%) Age 37 (17) 44 (19) 50 (17) 48 (17) Race/Hispanic Origin Asian 385,874 (10%) 6,348,964 (10%) 5,473,194 (7.1%) 2,321,160 (2.3%) Black 613,071 (16%) 6,036,949 (9.7%) 7,381,673 (9.6%) 13,643,866 (13%) Hispanic 440,910 (12%) 7,245,902 (12%) 14,181,024 (18%) 17,625,000 (17%) Other 106,290 (2.8%) 2,276,951 (3.6%) 2,692,021 (3.5%) 4,801,387 (4.7%) White 2,251,739 (59%) 40,544,106 (65%) 47,256,927 (61%) 62,950,593 (62%) \u0026#10; 1 n (%); Mean (SD) BMI Categories by Race/Hispanic Origin, Gender, and Average Age Unweighted 1 2 3 4 5 6 7 8 tbl_summary(nhanes_tibble, by = bmi_cat, include = c(gender, age, race_cat), label = list(gender ~ \u0026#34;Gender\u0026#34;, age ~ \u0026#34;Age\u0026#34;, race_cat ~ \u0026#34;Race/Hispanic Origin\u0026#34;), statistic = list(age ~ \u0026#34;{mean} ({sd})\u0026#34;) ) \u0026#10; Characteristic Normal, N = 2,1851 Obese, N = 3,6881 Overweight, N = 2,7671 Underweight, N = 1501 Gender Male 1,044 (48%) 1,644 (45%) 1,521 (55%) 62 (41%) Female 1,141 (52%) 2,044 (55%) 1,246 (45%) 88 (59%) Age 46 (20) 50 (17) 52 (18) 39 (20) Race/Hispanic Origin Asian 478 (22%) 163 (4.4%) 403 (15%) 27 (18%) Black 494 (23%) 1,175 (32%) 609 (22%) 48 (32%) Hispanic 341 (16%) 879 (24%) 701 (25%) 19 (13%) Other 110 (5.0%) 198 (5.4%) 113 (4.1%) 6 (4.0%) White 762 (35%) 1,273 (35%) 941 (34%) 50 (33%) \u0026#10; 1 n (%); Mean (SD) Gender vs BMI T-Test Normality Assumption 1 svyqqmath(~bmi, design = nhanes_adult) This deviates from the normal distribution, so I applied a log transformation to the BMI variable to remedy this deviation from the normal distribution at the tail end of the data.\n1 svyqqmath(~log10(bmi), design = nhanes_adult) That looks a lot better, now time to visualize the data and run the analysis. Boxplot Visualization 1 2 3 4 5 svyboxplot(log10(bmi) ~ gender, design = nhanes_adult, xlab = \u0026#34;Gender\u0026#34;, main = \u0026#34;Weighted Boxplot of Mean Log Transformed BMI by Gender\u0026#34;, ylab = \u0026#34;Log Transformed BMI\u0026#34;) T-Test 1 svyttest(log10(bmi) ~ gender, nhanes_adult) ## ## Design-based t-test ## ## data: log10(bmi) ~ gender ## t = 0.46451, df = 24, p-value = 0.6465 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -0.005848037 0.009244940 ## sample estimates: ## difference in mean ## 0.001698451 Race/Hispanic Origin vs. BMI ANOVA 1 2 3 4 5 6 svyboxplot(log10(bmi) ~ race_cat, design = nhanes_adult, xlab = \u0026#34;Race/Hispanic Origin\u0026#34;, ylab = \u0026#34;Log Transformed BMI\u0026#34;, main = \u0026#34;Weighted Boxplot of Mean Log Transformed BMI by Race/Hispanic Origin\u0026#34;) In visually inspecting the boxplot, it looks like there is a difference between the various race/hispanic origin groups and BMI, but lets see. Fit Model 1 race.bmi.glm \u0026lt;- svyglm(log10(bmi) ~ race_cat, nhanes_adult) ANOVA A barrier that I ran into was that the survey package in R does not seen to have an ANOVA function, and when you run the survey GLM through the regular ANOVA function it does not work as I thought it would. I did some researching around and I found that this is a way to assess this for complex surveys using a Wald test.\n1 2 options(scipen = 999) regTermTest(race.bmi.glm, ~race_cat) ## Wald test for race_cat ## in svyglm(formula = log10(bmi) ~ race_cat, design = nhanes_adult) ## F = 184.1178 on 4 and 21 df: p= 0.00000000000000050064 So it looks like there is a statistically significant difference between the various Race/Hispanic Origin categories and BMI, as hypothesized by the boxplot.\nConclusion While the NHANES dataset is still very useful, I did have to dig a lot deeper and do more research to find out how I can properly use this data.\nAs I am also working through another book on statistics, I found out that there is not just parametric statistics if your data is normal, and non parmetric statistics if it isn’t, but that you’re allowed to transform your data to help fit better into the normal distribution so that you can use parametric statistics like T-Tests with more confidence.\n","date":"2024-01-20T00:00:00Z","image":"https://michelleyg1.github.io/p/nhanes-bmi-analysis-using-survey-package/images/green_hu5459c0360c2b0cb7a147d2df0eb350ca_2075947_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/nhanes-bmi-analysis-using-survey-package/","title":"NHANES BMI Analysis Using Survey Package"},{"content":"For this analysis I decided to use the CDC NHANES data (2017 - pre pandemic 2020). I found this guide on importing and cleaning NHANES data using the nhanesA package, and found it very useful in this project.\nNHANES is a very large survey conducted by the CDC that is a crucial resource for public health data analysis, and is all available for free public use. As this was my first time using this dataset, I decided to keep it simple, but I will definitely be coming back to it as there is just so much information collected that can be used for all types of public health analysis.\nLoad Packages 1 2 3 4 5 6 7 suppressPackageStartupMessages({ library(tidyverse) library(nhanesA) library(tableone) library(arsenal) library(agricolae) }) Grab Datasets that I Need Here I used the nhanesA package to get the datasets (or Data Files as they are called on the NHANES site), the two that I needed for this analysis were the demographics dataset\n1 2 demo \u0026lt;- nhanes(\u0026#34;P_DEMO\u0026#34;) demo_translate \u0026lt;- nhanesTranslate(\u0026#34;P_DEMO\u0026#34;, names(demo), data = demo) 1 ## Translated columns: RIDSTATR RIAGENDR RIDRETH1 RIDRETH3 RIDEXMON DMDBORN4 DMDYRUSZ DMDEDUC2 DMDMARTZ RIDEXPRG SIALANG SIAPROXY SIAINTRP FIALANG FIAPROXY FIAINTRP MIALANG MIAPROXY MIAINTRP AIALANGA and the total cholesterol one\n1 2 exam \u0026lt;- nhanes(\u0026#34;P_TCHOL\u0026#34;) exam_translate \u0026lt;- nhanesTranslate(\u0026#34;P_TCHOL\u0026#34;, names(exam), data = exam) 1 2 ## Warning in nhanesTranslate(\u0026#34;P_TCHOL\u0026#34;, names(exam), data = exam): No columns ## were translated Retain Useful Variables Here I am only retaining the variables that I want to bring into my final dataset.\nFrom the demographics file, I decided on SEQN which is the sequence number that will help us merge the two datasets, RIDEXPRG which indicates if the respondent is pregnant, RIAGENDR which stores the participants gender, RIDAGEYR which stores the participant\u0026rsquo;s age in years, and lastly RIDRETH3 which categorizes the participant\u0026rsquo;s race.\nFrom the exam file I only extracted the sequence number, and lab value for total cholesterol.\n1 2 3 4 5 demo_select \u0026lt;- demo_translate %\u0026gt;% select(SEQN, RIDEXPRG, RIAGENDR, RIDAGEYR, RIDRETH3) exam_select \u0026lt;- exam_translate %\u0026gt;% select(SEQN, LBXTC) Merge the Data Using SEQN to Match Values The structure of the NHANES database makes it easy to match which lab values belong to which patient in including the SEQN column on the seperate datasets. I also went ahead and dropped the SEQN column when I was done merging as I only needed it for that task.\n1 2 3 merged_data \u0026lt;- merge(demo_select, exam_select, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) merged_data$SEQN \u0026lt;- NULL Initial Investigaton of the Data Here I want to see the different categories and how much we have of each so that I can apply the eligibility criteria and recode the values properly.\n1 2 3 4 initial_table \u0026lt;- CreateTableOne(data = merged_data, includeNA = TRUE) print(initial_table, showAllLevels = TRUE) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## ## level ## n ## RIDEXPRG (%) Yes, positive lab pregnancy test or self-reported pregnant at exam ## The participant was not pregnant at exam ## Cannot ascertain if the participant is pregnant at exam ## \u0026lt;NA\u0026gt; ## RIAGENDR (%) Male ## Female ## RIDAGEYR (mean (SD)) ## RIDRETH3 (%) Mexican American ## Other Hispanic ## Non-Hispanic White ## Non-Hispanic Black ## Non-Hispanic Asian ## Other Race - Including Multi-Racial ## LBXTC (mean (SD)) ## ## Overall ## n 15560 ## RIDEXPRG (%) 87 ( 0.6) ## 1604 (10.3) ## 183 ( 1.2) ## 13686 (88.0) ## RIAGENDR (%) 7721 (49.6) ## 7839 (50.4) ## RIDAGEYR (mean (SD)) 33.74 (25.32) ## RIDRETH3 (%) 1990 (12.8) ## 1544 ( 9.9) ## 5271 (33.9) ## 4098 (26.3) ## 1638 (10.5) ## 1019 ( 6.5) ## LBXTC (mean (SD)) 177.46 (40.36) Apply Eligibility Criteria I decided for the eligibility criteria to exclude pregnant women, as well as those under the age of 20, I did that using dplyr\u0026rsquo;s filter() function, I also filtered out the columns where there was no total cholesterol lab value as that is not useful to my analysis.\n1 2 3 4 analytic_data \u0026lt;- merged_data %\u0026gt;% filter(!is.na(LBXTC), RIDAGEYR \u0026gt;= 20, RIDEXPRG != \u0026#34;Yes, positive lab pregnancy test or self-reported pregnant at exam\u0026#34; | is.na(RIDEXPRG)) Recode and Make Categories In this section, I created several new variables using dplyr\u0026rsquo;s mutate() function. I created the age_cat variable that groups the participants by age, the total_cholesterol_cat variable that groups the lab values by their normal, borderline and high ranges, and I also simplified the race data into more concise and broad categories.\nI also renamed the remaining variables to match the naming conventions in the other newly created variables, also to give some more sense to them as NHANES is great but the variables all look like keyboard smashes to me. Their variable search tool helps with that.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 category_data \u0026lt;- analytic_data %\u0026gt;% mutate( age_cat = cut(analytic_data$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE), total_cholesterol_cat = cut(analytic_data$LBXTC, c(0, 200, 240, Inf), labels = c(\u0026#34;Normal\u0026#34;, \u0026#34;Borderline\u0026#34;, \u0026#34;High\u0026#34;), right = FALSE), race = car::recode(analytic_data$RIDRETH3, \u0026#34;c(\u0026#39;Mexican American\u0026#39;, \u0026#39;Other Hispanic\u0026#39;) = \u0026#39;Hispanic\u0026#39;; \u0026#39;Non-Hispanic White\u0026#39; = \u0026#39;White\u0026#39;; \u0026#39;Non-Hispanic Black\u0026#39; = \u0026#39;Black\u0026#39;; \u0026#39;Non-Hispanic Asian\u0026#39; = \u0026#39;Asian\u0026#39;; \u0026#39;Other Race - Including Multi-Racial\u0026#39; = \u0026#39;Other\u0026#39;; else = NA\u0026#34;) ) %\u0026gt;% rename( pregnancy_status = RIDEXPRG, gender = RIAGENDR, age = RIDAGEYR, total_cholesterol = LBXTC ) %\u0026gt;% select(-RIDRETH3, -pregnancy_status) 1 2 cholesterol \u0026lt;- as_tibble(category_data) cholesterol 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 7,845 × 6 ## gender age total_cholesterol age_cat total_cholesterol_cat race ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Female 29 195 [20,40) Normal Asian ## 2 Male 49 147 [40,60) Normal White ## 3 Male 36 164 [20,40) Normal White ## 4 Male 68 105 [60,Inf) Normal Other ## 5 Male 76 233 [60,Inf) Borderline White ## 6 Female 44 212 [40,60) Borderline Hispanic ## 7 Female 68 165 [60,Inf) Normal Black ## 8 Female 42 229 [40,60) Borderline Asian ## 9 Male 58 172 [40,60) Normal Hispanic ## 10 Male 44 189 [40,60) Normal White ## # ℹ 7,835 more rows Table of Total Cholesterol Categories Here I used the arsenal package to make a table displaying how the different total cholesterol categories look within our study sample.\n1 2 3 labels \u0026lt;- list(age_cat = \u0026#34;Age Range\u0026#34;, gender = \u0026#34;Gender\u0026#34;, race = \u0026#34;Race\u0026#34;) tab \u0026lt;- arsenal::tableby(total_cholesterol_cat ~ age_cat + gender + race, data = cholesterol, test = FALSE) summary(tab, labelTranslations = labels, text=TRUE) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## ## ## | | Normal (N=5236) | Borderline (N=1873) | High (N=736) | Total (N=7845) | ## |:-----------|:---------------:|:-------------------:|:------------:|:--------------:| ## |Age Range | | | | | ## |- [20,40) | 1789 (34.2%) | 412 (22.0%) | 117 (15.9%) | 2318 (29.5%) | ## |- [40,60) | 1499 (28.6%) | 789 (42.1%) | 338 (45.9%) | 2626 (33.5%) | ## |- [60,Inf) | 1948 (37.2%) | 672 (35.9%) | 281 (38.2%) | 2901 (37.0%) | ## |Gender | | | | | ## |- Male | 2659 (50.8%) | 850 (45.4%) | 323 (43.9%) | 3832 (48.8%) | ## |- Female | 2577 (49.2%) | 1023 (54.6%) | 413 (56.1%) | 4013 (51.2%) | ## |Race | | | | | ## |- Asian | 559 (10.7%) | 271 (14.5%) | 108 (14.7%) | 938 (12.0%) | ## |- Black | 1430 (27.3%) | 400 (21.4%) | 162 (22.0%) | 1992 (25.4%) | ## |- Hispanic | 1169 (22.3%) | 432 (23.1%) | 164 (22.3%) | 1765 (22.5%) | ## |- Other | 245 (4.7%) | 96 (5.1%) | 36 (4.9%) | 377 (4.8%) | ## |- White | 1833 (35.0%) | 674 (36.0%) | 266 (36.1%) | 2773 (35.3%) | Checking Assumptions Assumption of Normal Distribution Next before I look and see if there are any statistically significant differences between the groups of my choice, I have to look and see the distribution of our data to see what kind of assumptions we can make when running the stats.\n1 2 3 4 5 6 7 ggplot(cholesterol, aes(x = total_cholesterol)) + geom_histogram(binwidth = 15, color = \u0026#34;black\u0026#34;, fill = \u0026#34;#bbbbff\u0026#34;) + labs(x = \u0026#34;Total Cholesterol (mg/dL)\u0026#34;, y = \u0026#34;Count\u0026#34;, title = \u0026#34;Total Cholesterol (mg/dL) Distribution\u0026#34;, caption = \u0026#34;Source: CDC NHANES 2017 - Pre Pandemic 2020\u0026#34;) + theme(plot.title = element_text(hjust = 0.5)) This follows a generally normal distribution, but I\u0026rsquo;m going to take a closer look as I do see outliers towards the end of our bell curve.\n1 2 cholesterol_model \u0026lt;- lm(total_cholesterol ~ race, data = cholesterol) plot(cholesterol_model, which = 2) It seems like there is a systematic deviation from the expected relationship if the data were to be normally distributed. The data is not normally distributed. Assumption of Constant Variance 1 plot(cholesterol_model, which = 3) There is constant variance throughout the data, however since our assumption of normal distribution was violated I am going to check one more assumption that would be needed to run a non parametric test assessing the differences between the central tendency of our chosen groups. Assumption of Similar Skewness for Each Category 1 2 3 4 5 6 7 8 9 ggplot(cholesterol, aes(x = total_cholesterol, fill = race)) + geom_histogram(binwidth = 15, color = \u0026#34;black\u0026#34;) + labs(x = \u0026#34;Total Cholesterol (mg/dL)\u0026#34;, y = \u0026#34;Count\u0026#34;, title = \u0026#34;Total Cholesterol (mg/dL) Distribution by Race/Hispanic Origin\u0026#34;, caption = \u0026#34;Source: CDC NHANES 2017 - Pre Pandemic 2020\u0026#34;) + theme(plot.title = element_text(hjust = 0.5)) + facet_wrap(~race) + scale_fill_manual(values = c(\u0026#34;#E6FFFD\u0026#34;, \u0026#34;#AEE2FF\u0026#34;, \u0026#34;#ACBCFF\u0026#34;, \u0026#34;#B799FF\u0026#34;, \u0026#34;#AA77FF\u0026#34;)) Looks like each category of Race/Hispanic origin does have a similar skewness. Statistical Testing Visualize Data with a Boxplot 1 2 3 4 5 6 7 8 9 10 11 ggplot(cholesterol, aes(x = race, y = total_cholesterol, fill = race)) + geom_boxplot() + scale_fill_manual(values = c(\u0026#34;#E6FFFD\u0026#34;, \u0026#34;#AEE2FF\u0026#34;, \u0026#34;#ACBCFF\u0026#34;, \u0026#34;#B799FF\u0026#34;, \u0026#34;#AA77FF\u0026#34;)) + labs( x = \u0026#34;Race/Hispanic Origin\u0026#34;, y = \u0026#34;Total Cholesterol (mg/dL)\u0026#34;, title = \u0026#34;Total Cholesterol (mg/dL) by Race/Hispanic Origin of Participant\u0026#34;, caption = \u0026#34;Source: CDC NHANES 2017 - Pre Pandemic 2020\u0026#34; ) + theme(legend.position = \u0026#34;none\u0026#34;, plot.title = element_text(hjust = 0.5)) Kruskal Wallis Test Since I checked various assumptions, and the assumption of normality was violated, I decided to go with a non parametric Kruskal Wallis Test to test my hypothesis that there is a different mean total cholesterol value between the 5 different racial/hispanic origin categories.\n1 2 options(scipen = 999) kruskal.test(total_cholesterol ~ race, data = cholesterol) 1 2 3 4 5 6 ## ## Kruskal-Wallis rank sum test ## ## data: total_cholesterol by race ## Kruskal-Wallis chi-squared = 60.582, df = 4, p-value = ## 0.000000000002188 The p value is very very small, meaning there is one or more categories that have a mean total cholesterol that differs from the other. To check which groups are causing there to be a statisticially signifigant difference, I\u0026rsquo;ll run a pairwise comparison.\nPairwise Comparison Test 1 2 pairwise.wilcox.test(cholesterol$total_cholesterol, cholesterol$race, p.adjust.method = \u0026#34;BH\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 ## ## Pairwise comparisons using Wilcoxon rank sum test with continuity correction ## ## data: cholesterol$total_cholesterol and cholesterol$race ## ## Asian Black Hispanic Other ## Black 0.0000000000011 - - - ## Hispanic 0.00096 0.0000031636325 - - ## Other 0.01722 0.01722 0.77812 - ## White 0.0000031636325 0.00069 0.12485 0.61744 ## ## P value adjustment method: BH Looks like there are multiple groups in which a statistically significant difference can be observed, in fact it is present in all except Other and Hispanic, White and Hispanic, and Other and White.\nConclusion I was always hesitant to use the CDC\u0026rsquo;s NHANES dataset as it is so big and there are so many different variables but I\u0026rsquo;m glad that I found the nhanesA package as that made it so easy to do this analysis, and I\u0026rsquo;ll definitely be using it again to avoid having to dig for datasets and also having to download gigantic ones to my poor old computer lol.\nI also learned some other things during this analysis, in terms of R skills I learned about the arsenal package that builds tables to display the various categories that were pre-existing in the data, as well as the categories that I created using the cut() function based on the typical ranges used in medicine for total cholesterol.\nIn terms of statistics, I still have some confusion about the central limit theorem and normal distributions and if you can actually use parametric tests on non normal distributions, as there seems to be a lot of heated debate over this on the various statistics forums that I visited in hope of getting an answer to this question. I decided to play it safe and use a non parametric test, but I do want to learn more about how and when you can violate assumptions if you should at all.\n","date":"2024-01-12T00:00:00Z","image":"https://michelleyg1.github.io/p/total-cholesterol-cdc-nhanes-analysis/images/purple_hu3d03a01dcc18bc5be0e67db3d8d209a6_1915810_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/total-cholesterol-cdc-nhanes-analysis/","title":"Total Cholesterol CDC NHANES Analysis"},{"content":"I found this dataset by filtering through the various settings that they have on the National Agriculture Statistics Service quick stats tool to see the condition of blueberries by year and week in the state of New Jersey.\nA lot of people think that New Jersey is only the city and the shore, but its not called the Garden State for nothing! For its small size, New Jersey punches above its weight class in producing and exporting various agricultural products.\nOne crop that New Jersey is known for is its blueberries.\nLoad Package 1 library(tidyverse) 1 2 3 4 5 6 7 8 9 10 ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors 1 library(janitor) 1 2 3 4 5 6 ## ## Attaching package: \u0026#39;janitor\u0026#39; ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## chisq.test, fisher.test Import Dataset 1 blueberries \u0026lt;- read_csv(\u0026#34;/Users/michellegulotta/Desktop/my_first_project/blueberries/blueberry.csv\u0026#34;) 1 2 3 4 5 6 7 8 9 10 ## Rows: 225 Columns: 21 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026#34;,\u0026#34; ## chr (9): Program, Period, Geo Level, State, watershed_code, Commodity, Data... ## dbl (3): Year, State ANSI, Value ## lgl (8): Ag District, Ag District Code, County, County ANSI, Zip Code, Regi... ## date (1): Week Ending ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 1 2 3 4 5 6 7 8 blueberries \u0026lt;- blueberries %\u0026gt;% janitor::clean_names(., \u0026#34;snake\u0026#34;) %\u0026gt;% select(year, period, week_ending, data_item, value) %\u0026gt;% rename( condition = data_item, percent = value, week_number = period ) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 225 × 5 ## year week_number week_ending condition percent ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## 2 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## 3 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 100 ## 4 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## 5 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## 6 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 18 ## 7 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 19 ## 8 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 44 ## 9 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 19 ## 10 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## # ℹ 215 more rows Cleaning Data Clean up Condition and Week Number Column I wanted to clean up the condition column as I noticed it was pretty redundant, and all we really needed was the condition. This would also make it eaiser as this is going to be the legend on our graph. Another thing that I wanted to do was to make the week number column more simple, as the name of the variable already tells us that this is the week number, so all we really need in the observation is the number.\n1 2 blueberries$condition \u0026lt;- str_replace_all(blueberries$condition, \u0026#34;BLUEBERRIES, TAME - CONDITION, MEASURED IN PCT\u0026#34;, \u0026#34;\u0026#34;) blueberries$week_number \u0026lt;- str_replace_all(blueberries$week_number, \u0026#34;WEEK #\u0026#34;, \u0026#34;\u0026#34;) Checking to make sure that the different conditions look good\n1 unique(blueberries$condition) 1 ## [1] \u0026#34; EXCELLENT\u0026#34; \u0026#34; FAIR\u0026#34; \u0026#34; GOOD\u0026#34; \u0026#34; POOR\u0026#34; \u0026#34; VERY POOR\u0026#34; It looks like there\u0026rsquo;s some random leading and trailing white space, so I\u0026rsquo;m going to clean that up using the trimws() function, as well as the str_to_title() function to make it more readable and look nicer on our graph\n1 2 blueberries$condition \u0026lt;- trimws(blueberries$condition) blueberries$condition \u0026lt;- str_to_title(blueberries$condition) Checking the conditions again\n1 unique(blueberries$condition) 1 ## [1] \u0026#34;Excellent\u0026#34; \u0026#34;Fair\u0026#34; \u0026#34;Good\u0026#34; \u0026#34;Poor\u0026#34; \u0026#34;Very Poor\u0026#34; Looks good! Time to make a data visualization\nData Visualization 1 2 3 4 5 6 blueberries_2021 \u0026lt;- blueberries %\u0026gt;% filter(year == 2021) %\u0026gt;% mutate( condition_f = factor(condition) ) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 55 × 6 ## year week_number week_ending condition percent condition_f ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 2021 25 2021-06-27 Excellent 25 Excellent ## 2 2021 25 2021-06-27 Fair 25 Fair ## 3 2021 25 2021-06-27 Good 50 Good ## 4 2021 25 2021-06-27 Poor 0 Poor ## 5 2021 25 2021-06-27 Very Poor 0 Very Poor ## 6 2021 26 2021-07-04 Excellent 84 Excellent ## 7 2021 26 2021-07-04 Fair 8 Fair ## 8 2021 26 2021-07-04 Good 8 Good ## 9 2021 26 2021-07-04 Poor 0 Poor ## 10 2021 26 2021-07-04 Very Poor 0 Very Poor ## # ℹ 45 more rows Factor to Change Order of Legend One of the problems that I came across while trying to make this graph was that the legend would appear in alphabetical order rather than in the order that made sense from excellent to very poor. After some trying and researching solutions, I realized that I could use the factor() function to change the order of the levels.\n1 2 blueberries_2021$condition_f \u0026lt;- factor(blueberries_2021$condition_f, levels = c(\u0026#34;Excellent\u0026#34;, \u0026#34;Good\u0026#34;, \u0026#34;Fair\u0026#34;, \u0026#34;Poor\u0026#34;, \u0026#34;Very Poor\u0026#34;)) Make Plot 1 2 3 4 5 6 7 8 9 10 11 ggplot(blueberries_2021, aes(x = week_ending, y = percent, fill = condition_f)) + geom_area(alpha = 0.5, position = \u0026#34;identity\u0026#34;) + scale_fill_manual(values = c(\u0026#34;#785EF0\u0026#34;, \u0026#34;#009E73\u0026#34;, \u0026#34;#FFB000\u0026#34;, \u0026#34;#FE6100\u0026#34;, \u0026#34;#DC267F\u0026#34;)) + labs(title = \u0026#34;Condition of Blueberries Measured in Percent in New Jersey in 2021\u0026#34;, x = \u0026#34;Week\u0026#34;, y = \u0026#34;Percent\u0026#34;, fill = \u0026#34;Condition\u0026#34;, caption = \u0026#34;Source: USDA National Agrigultural Statistics Service\u0026#34;) + theme_light() Conclusion Now it\u0026rsquo;s the beginning of January but this has me looking forward to the beginning of July!\nWorking on this specific visualization helped me work on my data cleaning skills. I also learned more about how factors work and how R automatically puts a character vector in alphabetical order, and that in order to get it to appear in the order you want on a legend, you have to change the order of the factor variable. I didn\u0026rsquo;t think about that before I started but it definitely makes sense, R doesn\u0026rsquo;t just know how humans rank the quality of blueberries.\nI also learned about geom_area(), I first tried to use geom_line() and was able to make that pretty easily, but when I tried to change it to geom_area() there was a lot that I had to change in order to get the effect that I wanted on my graph.\n","date":"2024-01-07T00:00:00Z","image":"https://michelleyg1.github.io/p/jersey-blueberries/images/blueberry_hu3d03a01dcc18bc5be0e67db3d8d209a6_1579336_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/jersey-blueberries/","title":"Jersey Blueberries"},{"content":"I found this interesting dataset on Kaggle and wanted to do an exploratory analysis as I also have had asthma since I was a kid and wanted to see if I could find any interesting patterns within the data.\nLoad Packages 1 2 3 4 suppressPackageStartupMessages({ library(tidyverse) library(janitor) }) Import Dataset 1 2 3 asthma \u0026lt;- read.csv(\u0026#34;/Users/michellegulotta/Desktop/my_first_project/asthma/CDIAsthmaByStateTransposed2010-2020.csv\u0026#34;) asthma \u0026lt;- asthma %\u0026gt;% janitor::clean_names(., \u0026#34;snake\u0026#34;) Taking a look at the first few rows of the data to see what kind of information this dataset is providing\n1 head(asthma) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## geo_loc year state pop_est ## 1 POINT (-86.63186076199969 32.84057112200048) 2010 Alabama 4785514 ## 2 POINT (-147.72205903599973 64.84507995700051) 2010 Alaska 713982 ## 3 POINT (-111.76381127699972 34.865970280000454) 2010 Arizona 6407342 ## 4 POINT (-92.27449074299966 34.74865012400045) 2010 Arkansas 2921998 ## 5 POINT (-120.99999953799971 37.63864012300047) 2010 California 37319550 ## 6 POINT (-106.13361092099967 38.843840757000464) 2010 Colorado 5047539 ## f_fatal m_fatal o_fatal f_er m_er o_er f_hosp m_hosp o_hosp ## 1 44 17 61 NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA ## 3 31 27 58 22280 18335 40650 4717 3352 8073 ## 4 29 8 37 NA NA NA 1876 962 2841 ## 5 251 152 403 116571 96999 217637 19399 12958 34583 ## 6 33 15 48 NA NA NA 2385 1942 4336 Asthma Fatalities by Gender in New Jersey Data Visualization Clean Up Data Since its my home state, I decided to narrow the data that I wanted to look at down a bit to just New Jersey. I also only was interested in looking at fatalities for this particular graph as it is the most severe type of asthma incident recorded in this data.\nThe first thing that I needed to do was to pivot the data so that gender was its own observation, I originally missed out on this step when I was trying to make the graph and had a hard time coming up with the ggplot code as I was just repeating adding a different shape to my graph for each column. After a bit of trial and error I realized there was probably a way to do it where I wouldn’t have to add the columns individually as their own geoms.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 nj_asthma_fatalities \u0026lt;- asthma %\u0026gt;% filter(state == \u0026#34;New Jersey\u0026#34;) %\u0026gt;% select(year, ends_with(\u0026#34;_fatal\u0026#34;)) %\u0026gt;% pivot_longer( cols = ends_with(\u0026#34;_fatal\u0026#34;), names_to = \u0026#34;gender\u0026#34;, values_to = \u0026#34;fatalities\u0026#34;, names_pattern = \u0026#34;(.*)_fatal\u0026#34; ) %\u0026gt;% mutate( gender = factor(gender, c(\u0026#34;f\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;o\u0026#34;), c(\u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;, \u0026#34;Overall\u0026#34;)) ) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 33 × 3 ## year gender fatalities ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Female 49 ## 2 2010 Male 31 ## 3 2010 Overall 80 ## 4 2011 Female 62 ## 5 2011 Male 32 ## 6 2011 Overall 94 ## 7 2012 Female 63 ## 8 2012 Male 40 ## 9 2012 Overall 103 ## 10 2013 Female 73 ## # ℹ 23 more rows Create a Graph Now to take my newly pivoted data and create a graph using the ggplot2 package:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ggplot(data = nj_asthma_fatalities) + geom_smooth(mapping = aes( x = year, y = fatalities, group = gender, color = gender), se = FALSE) + scale_color_manual(values = c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;black\u0026#34;)) + scale_x_continuous(n.breaks = 10) + labs(title = \u0026#34;Asthma Fatalities in New Jersey by Gender from 2010 to 2020\u0026#34;, x = \u0026#34;Years\u0026#34;, y = \u0026#34;Number of Fatalities\u0026#34;) + theme(plot.title = element_text(hjust=0.5)) + theme_bw() 1 ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula = \u0026#39;y ~ x\u0026#39; This graph shows an interesting trend that at least in the state of New Jersey over the 2010s decade asthma fatality incidents increased and then decreased, just to increase once again around 2017. Also, another trend that caught my eye was that that females made up the majority of overall fatalities that were recorded.\nAs these are not proportional to the population, they do not tell us the whole story. I’m interested in seeing if the mortality rates show the same pattern as anyone living in New Jersey can tell you that the population has increased over this past decade just from the traffic alone, does the population change account for the increase in asthma fatality incidents?\nCalculate Mortality Rate per 100,000 People Column I calculated the cause specific mortality rate per 100,000 people in the whole population of the state.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 nj_asthma_mortality_rate \u0026lt;- asthma %\u0026gt;% filter(state == \u0026#34;New Jersey\u0026#34;) %\u0026gt;% select(year, pop_est, ends_with(\u0026#34;_fatal\u0026#34;)) %\u0026gt;% pivot_longer( cols = ends_with(\u0026#34;_fatal\u0026#34;), names_to = \u0026#34;gender\u0026#34;, values_to = \u0026#34;fatalities\u0026#34;, names_pattern = \u0026#34;(.*)_fatal\u0026#34; ) %\u0026gt;% mutate( gender = factor(gender, c(\u0026#34;f\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;o\u0026#34;), c(\u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;, \u0026#34;Overall\u0026#34;)), mortality_rate = round(((fatalities / pop_est) * 100000), 2) ) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 33 × 5 ## year pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 8799451 Female 49 0.56 ## 2 2010 8799451 Male 31 0.35 ## 3 2010 8799451 Overall 80 0.91 ## 4 2011 8828552 Female 62 0.7 ## 5 2011 8828552 Male 32 0.36 ## 6 2011 8828552 Overall 94 1.06 ## 7 2012 8845671 Female 63 0.71 ## 8 2012 8845671 Male 40 0.45 ## 9 2012 8845671 Overall 103 1.16 ## 10 2013 8857821 Female 73 0.82 ## # ℹ 23 more rows Create a Graph of Asthma Mortality Rate per 100,000 People and Group By Gender Then I used ggplot2 to graph this new column to compare the mortality rate per 100,000 people over the decade of 2010 to 2020 to see if the same trend emerges:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ggplot(data = nj_asthma_mortality_rate) + geom_smooth(mapping = aes(x = year, y = mortality_rate, group = gender, color = gender), se = FALSE) + scale_color_manual(values = c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;black\u0026#34;)) + scale_x_continuous(n.breaks = 10) + labs(title = \u0026#34;Asthma Mortality Rate in New Jersey by Gender from 2010 to 2020\u0026#34;, x = \u0026#34;Years\u0026#34;, y = \u0026#34;Number of Fatalities\u0026#34;) + theme(plot.title = element_text(hjust=0.5)) + theme_bw() 1 ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula = \u0026#39;y ~ x\u0026#39; Interesting, the graphs look pretty much the same to me so the population increase is not the reason for the increase in asthma fatality incidents as the mortality rate from asthma follows the same pattern as the fatality incidents over time, as well as the gender disparity.\nNationwide Analysis of Gender Differences In Asthma Mortality Rate Clean Up Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 asthma_mortality \u0026lt;- asthma %\u0026gt;% select(year, state, pop_est, f_fatal, m_fatal) %\u0026gt;% pivot_longer( cols = ends_with(\u0026#34;_fatal\u0026#34;), names_to = \u0026#34;gender\u0026#34;, values_to = \u0026#34;fatalities\u0026#34;, names_pattern = \u0026#34;(.*)_fatal\u0026#34; ) %\u0026gt;% mutate( gender = factor(gender, c(\u0026#34;f\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;o\u0026#34;), c(\u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;, \u0026#34;Overall\u0026#34;)), mortality_rate = round(((fatalities / pop_est) * 100000), 2) ) %\u0026gt;% filter(gender != \u0026#34;Overall\u0026#34;) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 1,122 × 6 ## year state pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Alabama 4785514 Female 44 0.92 ## 2 2010 Alabama 4785514 Male 17 0.36 ## 3 2010 Alaska 713982 Female NA NA ## 4 2010 Alaska 713982 Male NA NA ## 5 2010 Arizona 6407342 Female 31 0.48 ## 6 2010 Arizona 6407342 Male 27 0.42 ## 7 2010 Arkansas 2921998 Female 29 0.99 ## 8 2010 Arkansas 2921998 Male 8 0.27 ## 9 2010 California 37319550 Female 251 0.67 ## 10 2010 California 37319550 Male 152 0.41 ## # ℹ 1,112 more rows I noticed there were a decent amount of missing values just by glancing at this lets see how many exactly\n1 sum(is.na(asthma_mortality$mortality_rate)) 1 ## [1] 264 Hm, 264/1122 that is not that bad, also it does look like they\u0026rsquo;re states with limited populations, so that might be the reason is that there just weren\u0026rsquo;t any fatal asthma incidents in that particular year.\nI\u0026rsquo;m going to remove the missing values for our next step in this analysis\n1 2 3 asthma_mortality_no_miss \u0026lt;- asthma_mortality %\u0026gt;% drop_na(mortality_rate) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 858 × 6 ## year state pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Alabama 4785514 Female 44 0.92 ## 2 2010 Alabama 4785514 Male 17 0.36 ## 3 2010 Arizona 6407342 Female 31 0.48 ## 4 2010 Arizona 6407342 Male 27 0.42 ## 5 2010 Arkansas 2921998 Female 29 0.99 ## 6 2010 Arkansas 2921998 Male 8 0.27 ## 7 2010 California 37319550 Female 251 0.67 ## 8 2010 California 37319550 Male 152 0.41 ## 9 2010 Colorado 5047539 Female 33 0.65 ## 10 2010 Colorado 5047539 Male 15 0.3 ## # ℹ 848 more rows Create a Histogram of Fatal Asthma Incidents for Each Gender I then decided to create a histogram to look at how these asthma fatalities are distributed.\nI\u0026rsquo;m going to use ggplot2 to make a histogram comparing the distribution of mortality rates between the two populations\n1 2 3 4 5 6 ggplot(data = asthma_mortality_no_miss, aes(x = mortality_rate, fill = gender)) + geom_histogram(color = \u0026#34;black\u0026#34;) + scale_fill_manual(values=c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;)) + labs(title = \u0026#34;Asthma Mortality Rate by Gender in the USA from 2010 to 2020\u0026#34;, x = \u0026#34;Mortality Rate\u0026#34;, y= \u0026#34;Number of Observations\u0026#34;) 1 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. It looks like there are some outliers, let me take a look at the data sorted by mortality rate descending\n1 asthma_mortality_no_miss[order(asthma_mortality_no_miss$mortality_rate, decreasing = TRUE),] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 858 × 6 ## year state pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2020 New Mexico 211839 Female 26 12.3 ## 2 2020 Pennsylvania 1299444 Female 97 7.46 ## 3 2020 Illinois 1278658 Male 89 6.96 ## 4 2020 Illinois 1278658 Female 88 6.88 ## 5 2020 New Mexico 211839 Male 13 6.14 ## 6 2020 Pennsylvania 1299444 Male 69 5.31 ## 7 2014 Mississippi 2991892 Female 39 1.3 ## 8 2016 Mississippi 2990595 Female 38 1.27 ## 9 2014 Oregon 3965447 Female 50 1.26 ## 10 2014 Iowa 3110643 Female 38 1.22 ## # ℹ 848 more rows Wow, it looks like in 2020 there was a huge increase due to the pandemic most likely. I\u0026rsquo;m going to take a look at the data from 2010-2019 to get a closer look at the distribution.\n1 2 3 asthma_mortality_drop_2020 \u0026lt;- asthma_mortality_no_miss %\u0026gt;% filter(year != 2020) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 780 × 6 ## year state pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Alabama 4785514 Female 44 0.92 ## 2 2010 Alabama 4785514 Male 17 0.36 ## 3 2010 Arizona 6407342 Female 31 0.48 ## 4 2010 Arizona 6407342 Male 27 0.42 ## 5 2010 Arkansas 2921998 Female 29 0.99 ## 6 2010 Arkansas 2921998 Male 8 0.27 ## 7 2010 California 37319550 Female 251 0.67 ## 8 2010 California 37319550 Male 152 0.41 ## 9 2010 Colorado 5047539 Female 33 0.65 ## 10 2010 Colorado 5047539 Male 15 0.3 ## # ℹ 770 more rows And repeat the histogram process with the 2010 to 2019 data\n1 2 3 4 5 6 ggplot(data = asthma_mortality_drop_2020, aes(x = mortality_rate, fill = gender)) + geom_histogram(color = \u0026#34;black\u0026#34;) + scale_fill_manual(values=c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;)) + labs(title = \u0026#34;Asthma Mortality Rate by Gender in the USA from 2010 to 2019\u0026#34;, x = \u0026#34;Mortality Rate\u0026#34;, y= \u0026#34;Number of Observations\u0026#34;) 1 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. I want to compare the means of these populations to see if there is a statistically significant difference in fatal asthma incidence between the two genders throughout the whole USA.\nStatistical Testing From the histogram, these don\u0026rsquo;t really seem to fit a normal distribution, but rather a skewed distribution, but let\u0026rsquo;s run the Shapiro-Wilk test to be sure instead of just eyeballing it and assuming.\n1 2 options(scipen = 999) shapiro.test(asthma_mortality_no_miss$mortality_rate) 1 2 3 4 5 ## ## Shapiro-Wilk normality test ## ## data: asthma_mortality_no_miss$mortality_rate ## W = 0.27179, p-value \u0026lt; 0.00000000000000022 Okay, since the p value is not greater than 0.05, I can\u0026rsquo;t use a t-test as that assumes normal distribution.\nI\u0026rsquo;m going to go for a non-parametric test since we\u0026rsquo;re not assuming any particular distribution here to test my hypothesis that when it comes to asthma fatalities females have a higher mortality rate than males. Since I have two unpaired samples and I want to test how their values compare, I\u0026rsquo;m going to use the Mann-Whitney test.\n1 wilcox.test(asthma_mortality_no_miss$mortality_rate) 1 2 3 4 5 6 ## ## Wilcoxon signed rank test with continuity correction ## ## data: asthma_mortality_no_miss$mortality_rate ## V = 368511, p-value \u0026lt; 0.00000000000000022 ## alternative hypothesis: true location is not equal to 0 With these results we can reject the null hypothesis, and say within this data there is a statistically significant difference between the two populations.\nConclusion Before this analysis I had no idea of the gender differences that arise in asthma incidents, upon looking into this further after completing this analysis I came across a paper that discusses the gender differences in asthma prevalence and severity on a biological level.\nAccording to the Asthma and Allergy Network, In people under 18, it is more common for boys to have asthma than girls, but this prevalence switches when analyzing adult populations. The fact that women have a higher risk of death from asthma when compared to men is also confirmed by this source.\n","date":"2024-01-03T00:00:00Z","image":"https://michelleyg1.github.io/p/asthma-incidents-analysis/images/featured_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/asthma-incidents-analysis/","title":"Asthma Incidents Analysis"}]