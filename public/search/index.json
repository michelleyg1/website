[{"content":"For this analysis I was looking for data that captured count data so that I could practice using a poisson regression. I came across the NYC Open Data\u0026rsquo;s Emergency Department Visits and Admissions for Influenza-like Illness and/or Pneumonia and decided to bring that into R to analyze.\nLoad Libraries 1 2 3 4 5 suppressPackageStartupMessages({ library(tidyverse) library(RSocrata) library(lubridate) }) Load Data I chose 10,000 rows from the most populous zip code in New York City, 11368 which is in Corona, Queens. I chose this as I had trouble using the read.socrata() function with larger amounts of data.\n1 resp \u0026lt;- read.socrata(\u0026#34;https://data.cityofnewyork.us/resource/2nwg-uqyg.json?mod_zcta=11368\u0026amp;$limit=10000\u0026#34;) Checking the different types of data that are stored in the differnet variables in the dataset.\n1 str(resp) 1 2 3 4 5 6 7 ## \u0026#39;data.frame\u0026#39;:\t10000 obs. of 6 variables: ## $ extract_date : POSIXct, format: \u0026#34;2021-09-27\u0026#34; \u0026#34;2021-07-08\u0026#34; ... ## $ date : POSIXct, format: \u0026#34;2021-01-28\u0026#34; \u0026#34;2021-02-20\u0026#34; ... ## $ mod_zcta : chr \u0026#34;11368\u0026#34; \u0026#34;11368\u0026#34; \u0026#34;11368\u0026#34; \u0026#34;11368\u0026#34; ... ## $ total_ed_visits : chr \u0026#34;122\u0026#34; \u0026#34;117\u0026#34; \u0026#34;96\u0026#34; \u0026#34;120\u0026#34; ... ## $ ili_pne_visits : chr \u0026#34;10\u0026#34; \u0026#34;4\u0026#34; \u0026#34;3\u0026#34; \u0026#34;6\u0026#34; ... ## $ ili_pne_admissions: chr \u0026#34;2\u0026#34; \u0026#34;1\u0026#34; \u0026#34;1\u0026#34; \u0026#34;1\u0026#34; ... Looks like the counts are stored as characters, and the dates are stored in POSIXct format. I\u0026rsquo;m going to need to convert the data types for the counts, and I\u0026rsquo;ll also drop the extract_date variable as that is not very useful to my analysis, as well as the mod_zcta variable since all of the values in this particular dataset are from that zip code.\n1 2 3 4 5 6 7 8 resp \u0026lt;- resp %\u0026gt;% dplyr::select(-extract_date, -mod_zcta) %\u0026gt;% mutate( total_ed_visits = as.numeric(total_ed_visits), ili_pne_visits = as.numeric(ili_pne_visits), ili_pne_admissions = as.numeric(ili_pne_admissions), month = factor(month(ymd(resp$date))) ) Visualize Data First I\u0026rsquo;m going to make a long version of the data so that I can group them by outcome and have a legend that shows which line is which and in different colors, I used pivot_longer() to achieve this. I also converted the date column from datetime to just date, so that I can later use the scale_x_date function in ggplot to scale my x axis.\n1 2 3 4 5 6 7 resp_long \u0026lt;- resp %\u0026gt;% pivot_longer(cols = c(total_ed_visits, ili_pne_visits, ili_pne_admissions), names_to = \u0026#34;outcome\u0026#34;, values_to = \u0026#34;count\u0026#34;) %\u0026gt;% mutate( date = as.Date(date) ) Then, I used ggplot to visualize the data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ggplot(data = resp_long, aes(x = date, y = count, color = outcome)) + geom_smooth(se = FALSE) + scale_color_discrete(name = \u0026#34;Outcome\u0026#34;, labels = c(\u0026#34;Admissions for ILI*\u0026#34;, \u0026#34;ED Visits for ILI*\u0026#34;, \u0026#34;All ED Visits\u0026#34;)) + labs(caption = \u0026#34;*Influenza Like Illness\u0026#34;, x = \u0026#34;Date\u0026#34;, y = \u0026#34;Count\u0026#34;, title = \u0026#34;Count of Emergency Department Visits (All Cause and ILI*) and Hospital Admissions for ILI*\u0026#34;) + scale_x_date(date_breaks = \u0026#34;4 months\u0026#34;) + theme_light() + theme(plot.title = element_text(hjust=0.5)) 1 ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula = \u0026#39;y ~ s(x, bs = \u0026#34;cs\u0026#34;)\u0026#39; Poisson Regression Fit Model 1 2 3 4 pois.resp\u0026lt;- glm(ili_pne_admissions ~ month, data = resp, family = poisson) summary(pois.resp) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ## ## Call: ## glm(formula = ili_pne_admissions ~ month, family = poisson, data = resp) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.0974 -1.2311 -0.3399 0.5260 7.9668 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 1.15670 0.02425 47.706 \u0026lt; 2e-16 *** ## month2 -0.05197 0.03821 -1.360 0.174 ## month3 1.40761 0.02553 55.145 \u0026lt; 2e-16 *** ## month4 1.31247 0.02572 51.025 \u0026lt; 2e-16 *** ## month5 -0.83506 0.03479 -24.005 \u0026lt; 2e-16 *** ## month6 -1.73316 0.04833 -35.860 \u0026lt; 2e-16 *** ## month7 -1.44372 0.04373 -33.014 \u0026lt; 2e-16 *** ## month8 -1.10669 0.04060 -27.259 \u0026lt; 2e-16 *** ## month9 -1.68666 0.05373 -31.393 \u0026lt; 2e-16 *** ## month10 -1.43405 0.04957 -28.932 \u0026lt; 2e-16 *** ## month11 -0.92730 0.04361 -21.263 \u0026lt; 2e-16 *** ## month12 -0.21770 0.03533 -6.161 7.21e-10 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 93222 on 9999 degrees of freedom ## Residual deviance: 44040 on 9988 degrees of freedom ## AIC: 64530 ## ## Number of Fisher Scoring iterations: 5 Visualize Coefficents 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 coef \u0026lt;- c(coef(pois.resp)) coef_df \u0026lt;- data.frame( month = c(1:12), coefficent = coef ) ggplot(data = coef_df, aes(x = month, y = coef)) + geom_line(color = \u0026#34;#619CFF\u0026#34;, size = 1) + geom_point(color = \u0026#34;black\u0026#34;) + scale_x_continuous(n.breaks = 12) + labs(x = \u0026#34;Month\u0026#34;, y = \u0026#34;Coefficient\u0026#34;, title = \u0026#34;Poisson Model Coefficients by Month for Pois.Resp Model\u0026#34;) + theme_light() + theme(plot.title = element_text(hjust=0.5)) 1 2 3 4 5 ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Predict I created a random small data frame of 3 different observations of month, and then used the predict() function to see what values for the response variable would be output. I also rounded it up to an interger as there can\u0026rsquo;t be 0.6 admissions to the hospital.\n1 2 3 4 5 6 7 new_data \u0026lt;- data.frame( month = factor(c(1, 10, 4)) ) round(predict(pois.resp, new_data, type = \u0026#34;response\u0026#34;)) 1 2 ## 1 2 3 ## 3 1 12 Conclusion This was a short and sweet analysis on real life data that reinforced what I\u0026rsquo;ve been learning about poisson regression. I wanted to make my model multivariate but there was not much information in this dataset as it was only for one specific purpose. One thing that I could have done to make this more complex was to add more variables such as air quality information from the days that the data were collected, but I decided that that would involve more work than I wanted to put into this specific post.\nI was thinking about putting general ED visits and Influenza like Illness ED visits as predictors in the model, but those are not really independent from the Influenza like Illness Admissions.\nIt would also be interesting to see this data in a \u0026ldquo;normal\u0026rdquo; year as COVID definitely had an impact on the data and which months were related to Influenza like Illness admissions.\n","date":"2024-02-13T00:00:00Z","image":"https://michelleyg1.github.io/p/influenza-like-illness-hospital-admissions-poisson-regression/images/pois_hu5459c0360c2b0cb7a147d2df0eb350ca_2464222_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/influenza-like-illness-hospital-admissions-poisson-regression/","title":"Influenza Like Illness Hospital Admissions Poisson Regression"},{"content":"Load Libraries 1 2 3 4 5 6 suppressPackageStartupMessages({ library(tidyverse) library(janitor) library(lubridate) library(e1071) }) Load and Explore Data I found this huge and interesting dataset on NYC\u0026rsquo;s Open Data portal and wanted to use it to practice with the new knowledge and skills that I learned recently. My goal was to create a classifier model that can predict if motor vehicle collisions would result in injuries or not to anyone involved in the accident based on several predictors.\nI saw there was a way to query the API directly and get the data into R without downloading it as a csv, and I did figure out how to do that for other open datasets that were smaller, but I had trouble with this large dataset. I ended up using the NYC Open Data\u0026rsquo;s query tool to filter the dataset beforehand and downloaded it from there. I chose the borough of Queens to work with as there are many more highways and potential for accidents resulting in injuries there, rather than Manhattan where vehicles move very slowly through the grid shaped streets. I also filtered it for the calendar year of 2023 to further reduce the number of observations.\n1 2 3 motor_raw \u0026lt;- read.csv(\u0026#34;/Users/michellegulotta/Desktop/motor_raw.csv\u0026#34;) motor_raw \u0026lt;- motor_raw %\u0026gt;% janitor::clean_names(., \u0026#34;snake\u0026#34;) I wanted to also minimize missing values to deal with, so for the sake of simplicity I only focused on the data in the first columns for contributing factor and vehicle type.\n1 2 motor_cat \u0026lt;- motor_raw %\u0026gt;% dplyr::select(crash_date, crash_time, collision_id, number_of_persons_injured, number_of_persons_killed, contributing_factor_vehicle_1, vehicle_type_code_1) Now to check out the variables remaining along with the data types associated with each column.\n1 str(motor_cat) 1 2 3 4 5 6 7 8 ## \u0026#39;data.frame\u0026#39;:\t17822 obs. of 7 variables: ## $ crash_date : chr \u0026#34;11/16/2023\u0026#34; \u0026#34;11/17/2023\u0026#34; \u0026#34;11/17/2023\u0026#34; \u0026#34;11/12/2023\u0026#34; ... ## $ crash_time : chr \u0026#34;15:45\u0026#34; \u0026#34;6:57\u0026#34; \u0026#34;14:30\u0026#34; \u0026#34;10:58\u0026#34; ... ## $ collision_id : int 4679634 4679868 4679994 4679991 4663636 4663437 4629913 4631543 4631801 4632069 ... ## $ number_of_persons_injured : int 1 0 0 0 0 0 0 4 0 1 ... ## $ number_of_persons_killed : int 0 0 0 0 0 0 0 0 0 0 ... ## $ contributing_factor_vehicle_1: chr \u0026#34;Driver Inexperience\u0026#34; \u0026#34;Other Vehicular\u0026#34; \u0026#34;Unspecified\u0026#34; \u0026#34;Unspecified\u0026#34; ... ## $ vehicle_type_code_1 : chr \u0026#34;Motorcycle\u0026#34; \u0026#34;Sedan\u0026#34; \u0026#34;Station Wagon/Sport Utility Vehicle\u0026#34; \u0026#34;Station Wagon/Sport Utility Vehicle\u0026#34; ... Clean and Manipulate Data Time is stored as character and is in the 24 hour format, I just want the hour as I am going to use that to create a time of day category to use as a predictor. I also made a month category using the lubridate package, but I did not end up using this as a predictor, it was interesting to see how the lubridate package can help me in future projects.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 motor_cat \u0026lt;- motor_cat %\u0026gt;% mutate(hour = substr(motor_cat$crash_time, start = 1, stop = 2), across( .cols = c(contributing_factor_vehicle_1, vehicle_type_code_1), .fns = ~if_else(.x == \u0026#34;\u0026#34;, NA, .x) ), month = as.Date(crash_date, format = \u0026#34;%m/%d/%Y\u0026#34;)) %\u0026gt;% rename(type = vehicle_type_code_1, contrib = contributing_factor_vehicle_1) motor_cat$hour \u0026lt;- str_replace_all(motor_cat$hour, \u0026#34;:\u0026#34;, \u0026#34;\u0026#34;) motor_cat$hour \u0026lt;- as.integer(motor_cat$hour) motor_cat$month \u0026lt;- month(ymd(motor_cat$month)) I\u0026rsquo;m going to come back to this later, I want to do all of the category creating at once so I\u0026rsquo;m going to work on the contributing factor and vehicle type variables. In the previous code I renamed these variables to make the name shorter as I\u0026rsquo;m going to be renaming a lot of stuff later as you\u0026rsquo;ll see.\n1 motor_cat %\u0026gt;% tabyl(contrib) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 ## contrib n percent ## Accelerator Defective 13 7.294355e-04 ## Aggressive Driving/Road Rage 145 8.136012e-03 ## Alcohol Involvement 408 2.289305e-02 ## Animals Action 14 7.855460e-04 ## Backing Unsafely 691 3.877230e-02 ## Brakes Defective 69 3.871619e-03 ## Cell Phone (hand-Held) 11 6.172147e-04 ## Cell Phone (hands-free) 1 5.611043e-05 ## Driver Inattention/Distraction 4204 2.358882e-01 ## Driver Inexperience 372 2.087308e-02 ## Driverless/Runaway Vehicle 24 1.346650e-03 ## Drugs (illegal) 11 6.172147e-04 ## Eating or Drinking 3 1.683313e-04 ## Failure to Keep Right 35 1.963865e-03 ## Failure to Yield Right-of-Way 1830 1.026821e-01 ## Fatigued/Drowsy 27 1.514981e-03 ## Fell Asleep 117 6.564920e-03 ## Following Too Closely 988 5.543710e-02 ## Glare 20 1.122209e-03 ## Headlights Defective 1 5.611043e-05 ## Illnes 36 2.019975e-03 ## Lane Marking Improper/Inadequate 7 3.927730e-04 ## Lost Consciousness 50 2.805521e-03 ## Obstruction/Debris 27 1.514981e-03 ## Other Electronic Device 3 1.683313e-04 ## Other Lighting Defects 1 5.611043e-05 ## Other Vehicular 362 2.031197e-02 ## Outside Car Distraction 47 2.637190e-03 ## Oversized Vehicle 33 1.851644e-03 ## Passenger Distraction 34 1.907754e-03 ## Passing Too Closely 656 3.680844e-02 ## Passing or Lane Usage Improper 959 5.380990e-02 ## Pavement Defective 14 7.855460e-04 ## Pavement Slippery 60 3.366626e-03 ## Pedestrian/Bicyclist/Other Pedestrian Error/Confusion 129 7.238245e-03 ## Physical Disability 6 3.366626e-04 ## Prescription Medication 2 1.122209e-04 ## Reaction to Uninvolved Vehicle 125 7.013803e-03 ## Shoulders Defective/Improper 1 5.611043e-05 ## Steering Failure 53 2.973853e-03 ## Tinted Windows 4 2.244417e-04 ## Tire Failure/Inadequate 30 1.683313e-03 ## Tow Hitch Defective 2 1.122209e-04 ## Traffic Control Device Improper/Non-Working 6 3.366626e-04 ## Traffic Control Disregarded 742 4.163394e-02 ## Turning Improperly 468 2.625968e-02 ## Unsafe Lane Changing 193 1.082931e-02 ## Unsafe Speed 770 4.320503e-02 ## Unspecified 3754 2.106385e-01 ## Using On Board Navigation Device 1 5.611043e-05 ## Vehicle Vandalism 5 2.805521e-04 ## View Obstructed/Limited 150 8.416564e-03 ## \u0026lt;NA\u0026gt; 108 6.059926e-03 ## valid_percent ## 7.338828e-04 ## 8.185616e-03 ## 2.303263e-02 ## 7.903353e-04 ## 3.900869e-02 ## 3.895224e-03 ## 6.209778e-04 ## 5.645252e-05 ## 2.373264e-01 ## 2.100034e-02 ## 1.354861e-03 ## 6.209778e-04 ## 1.693576e-04 ## 1.975838e-03 ## 1.033081e-01 ## 1.524218e-03 ## 6.604945e-03 ## 5.577509e-02 ## 1.129050e-03 ## 5.645252e-05 ## 2.032291e-03 ## 3.951677e-04 ## 2.822626e-03 ## 1.524218e-03 ## 1.693576e-04 ## 5.645252e-05 ## 2.043581e-02 ## 2.653269e-03 ## 1.862933e-03 ## 1.919386e-03 ## 3.703286e-02 ## 5.413797e-02 ## 7.903353e-04 ## 3.387151e-03 ## 7.282376e-03 ## 3.387151e-04 ## 1.129050e-04 ## 7.056565e-03 ## 5.645252e-05 ## 2.991984e-03 ## 2.258101e-04 ## 1.693576e-03 ## 1.129050e-04 ## 3.387151e-04 ## 4.188777e-02 ## 2.641978e-02 ## 1.089534e-02 ## 4.346844e-02 ## 2.119228e-01 ## 5.645252e-05 ## 2.822626e-04 ## 8.467879e-03 ## NA I\u0026rsquo;m going to categorize these into much fewer categories to make it easier to create a model, how about now with the type of car as the primary vehicle involved in the accident.\n1 motor_cat %\u0026gt;% tabyl(type) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 ## type n percent valid_percent ## 2 dr sedan 1 5.611043e-05 5.703205e-05 ## 3-Door 2 1.122209e-04 1.140641e-04 ## 4 dr sedan 11 6.172147e-04 6.273526e-04 ## AMBU 2 1.122209e-04 1.140641e-04 ## AMBULANCE 15 8.416564e-04 8.554808e-04 ## AMBULENCE 2 1.122209e-04 1.140641e-04 ## Ambulance 78 4.376613e-03 4.448500e-03 ## Armored Truck 4 2.244417e-04 2.281282e-04 ## BOOM MOPED 1 5.611043e-05 5.703205e-05 ## Beverage Truck 2 1.122209e-04 1.140641e-04 ## Bike 205 1.150264e-02 1.169157e-02 ## Box Truck 176 9.875435e-03 1.003764e-02 ## Bulk Agriculture 3 1.683313e-04 1.710962e-04 ## Bus 247 1.385928e-02 1.408692e-02 ## Carry All 7 3.927730e-04 3.992244e-04 ## Chassis Cab 6 3.366626e-04 3.421923e-04 ## Commercial 1 5.611043e-05 5.703205e-05 ## Concrete Mixer 3 1.683313e-04 1.710962e-04 ## Convertible 27 1.514981e-03 1.539865e-03 ## Courier 1 5.611043e-05 5.703205e-05 ## DELIVERY T 1 5.611043e-05 5.703205e-05 ## DUMP 1 5.611043e-05 5.703205e-05 ## DUMP TRUCK 1 5.611043e-05 5.703205e-05 ## Dump 36 2.019975e-03 2.053154e-03 ## E-Bike 154 8.641005e-03 8.782936e-03 ## E-Scooter 75 4.208282e-03 4.277404e-03 ## Electric m 1 5.611043e-05 5.703205e-05 ## FDNY 1 5.611043e-05 5.703205e-05 ## FDNY AMBUL 2 1.122209e-04 1.140641e-04 ## FDNY FIRE 1 5.611043e-05 5.703205e-05 ## FDNY truck 1 5.611043e-05 5.703205e-05 ## FIRE ENGIN 1 5.611043e-05 5.703205e-05 ## FIRE TRUCK 3 1.683313e-04 1.710962e-04 ## FIRETRUCK 3 1.683313e-04 1.710962e-04 ## FLYWING MO 1 5.611043e-05 5.703205e-05 ## FOOD CART 1 5.611043e-05 5.703205e-05 ## FORK LIFT 1 5.611043e-05 5.703205e-05 ## FREIGHT VA 1 5.611043e-05 5.703205e-05 ## Fire Truck 1 5.611043e-05 5.703205e-05 ## Fire engin 1 5.611043e-05 5.703205e-05 ## Fire truck 1 5.611043e-05 5.703205e-05 ## Firetruck 3 1.683313e-04 1.710962e-04 ## Flat Bed 26 1.458871e-03 1.482833e-03 ## Flat Rack 7 3.927730e-04 3.992244e-04 ## Ford FF 1 5.611043e-05 5.703205e-05 ## Forklift 1 5.611043e-05 5.703205e-05 ## GARBAGE TR 2 1.122209e-04 1.140641e-04 ## GAS MOPED 1 5.611043e-05 5.703205e-05 ## GOLFCART 1 5.611043e-05 5.703205e-05 ## Garbage or Refuse 24 1.346650e-03 1.368769e-03 ## Government 1 5.611043e-05 5.703205e-05 ## LIMO 3 1.683313e-04 1.710962e-04 ## Ladder 1 5.611043e-05 5.703205e-05 ## Ladder tru 1 5.611043e-05 5.703205e-05 ## Lift Boom 1 5.611043e-05 5.703205e-05 ## MC 1 5.611043e-05 5.703205e-05 ## METRO TRAN 1 5.611043e-05 5.703205e-05 ## MOPED 5 2.805521e-04 2.851603e-04 ## MOTOR SCOO 1 5.611043e-05 5.703205e-05 ## MOTORIZEDS 1 5.611043e-05 5.703205e-05 ## Minicycle 1 5.611043e-05 5.703205e-05 ## Moped 94 5.274380e-03 5.361013e-03 ## Motorbike 11 6.172147e-04 6.273526e-04 ## Motorcycle 188 1.054876e-02 1.072203e-02 ## Motorscooter 21 1.178319e-03 1.197673e-03 ## Mta bus 1 5.611043e-05 5.703205e-05 ## Multi-Wheeled Vehicle 1 5.611043e-05 5.703205e-05 ## NYC FIRE T 1 5.611043e-05 5.703205e-05 ## OMS 1 5.611043e-05 5.703205e-05 ## Omnibus 1 5.611043e-05 5.703205e-05 ## Open Body 1 5.611043e-05 5.703205e-05 ## PAS 2 1.122209e-04 1.140641e-04 ## PK 34 1.907754e-03 1.939090e-03 ## Pick-up Truck 428 2.401526e-02 2.440972e-02 ## RV 2 1.122209e-04 1.140641e-04 ## Red moped 1 5.611043e-05 5.703205e-05 ## Refrigerated Van 1 5.611043e-05 5.703205e-05 ## Road sweep 1 5.611043e-05 5.703205e-05 ## SCHOOL BUS 1 5.611043e-05 5.703205e-05 ## SCHOOLBUS 1 5.611043e-05 5.703205e-05 ## SCOOTER 1 5.611043e-05 5.703205e-05 ## SELF INSUR 1 5.611043e-05 5.703205e-05 ## SPC 2 1.122209e-04 1.140641e-04 ## Sanitation 1 5.611043e-05 5.703205e-05 ## School Bus 2 1.122209e-04 1.140641e-04 ## Sedan 8491 4.764336e-01 4.842592e-01 ## Sprinter v 1 5.611043e-05 5.703205e-05 ## Stake or Rack 1 5.611043e-05 5.703205e-05 ## Station Wagon/Sport Utility Vehicle 6625 3.717316e-01 3.778373e-01 ## TOW TRUCK 1 5.611043e-05 5.703205e-05 ## TRAILER 2 1.122209e-04 1.140641e-04 ## TRUCK 1 5.611043e-05 5.703205e-05 ## Tanker 8 4.488834e-04 4.562564e-04 ## Taxi 251 1.408372e-02 1.431505e-02 ## Tow Truck 2 1.122209e-04 1.140641e-04 ## Tow Truck / Wrecker 17 9.538772e-04 9.695449e-04 ## Tow truck 1 5.611043e-05 5.703205e-05 ## Tractor Tr 1 5.611043e-05 5.703205e-05 ## Tractor Truck Diesel 64 3.591067e-03 3.650051e-03 ## Tractor Truck Gasoline 13 7.294355e-04 7.414167e-04 ## Trailer 2 1.122209e-04 1.140641e-04 ## Truck 3 1.683313e-04 1.710962e-04 ## UHAUL 1 5.611043e-05 5.703205e-05 ## USPS 1 5.611043e-05 5.703205e-05 ## USPS VEHIC 1 5.611043e-05 5.703205e-05 ## UTILITY 1 5.611043e-05 5.703205e-05 ## Van 74 4.152171e-03 4.220372e-03 ## Van Camper 1 5.611043e-05 5.703205e-05 ## Waste truc 1 5.611043e-05 5.703205e-05 ## ambulance 1 5.611043e-05 5.703205e-05 ## commerical 1 5.611043e-05 5.703205e-05 ## delv 1 5.611043e-05 5.703205e-05 ## forklift 1 5.611043e-05 5.703205e-05 ## pick up tr 1 5.611043e-05 5.703205e-05 ## van 2 1.122209e-04 1.140641e-04 ## \u0026lt;NA\u0026gt; 288 1.615980e-02 NA Creating Categories This dataset is a mess, I\u0026rsquo;m going to clean it up here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 motor_cat \u0026lt;- motor_cat %\u0026gt;% mutate(contrib_cat = as.factor(case_when( contrib == \u0026#34;Accelerator Defective\u0026#34; | contrib == \u0026#34;Brakes Defective\u0026#34; | contrib == \u0026#34;Steering Failure\u0026#34; | contrib == \u0026#34;Headlights Defective\u0026#34; | contrib == \u0026#34;Other Lighting Defects\u0026#34; | contrib == \u0026#34;Tinted Windows\u0026#34; | contrib == \u0026#34;Tire Failure/Inadequate\u0026#34; | contrib == \u0026#34;Tow Hitch Defective\u0026#34; ~ \u0026#34;Car Defects\u0026#34;, contrib == \u0026#34;Backing Unsafely\u0026#34; | contrib == \u0026#34;Driver Inexperience\u0026#34; | contrib == \u0026#34;Failure to Keep Right\u0026#34; | contrib == \u0026#34;Failure to Yield Right-of-Way\u0026#34; | contrib == \u0026#34;Following Too Closely\u0026#34; | contrib == \u0026#34;Passing Too Closely\u0026#34; | contrib == \u0026#34;Passing or Lane Usage Improper\u0026#34; | contrib == \u0026#34;Turning Improperly\u0026#34; ~ \u0026#34;Driver Error\u0026#34;, contrib == \u0026#34;Alcohol Involvement\u0026#34; | contrib == \u0026#34;Drugs (illegal)\u0026#34; | contrib == \u0026#34;Prescription Medication\u0026#34; ~ \u0026#34;Substances\u0026#34;, contrib == \u0026#34;Animals Action\u0026#34; | contrib == \u0026#34;Driverless/Runaway Vehicle\u0026#34; | contrib == \u0026#34;Glare\u0026#34; | contrib == \u0026#34;Lane Marking Improper/Inadequate\u0026#34; | contrib == \u0026#34;Obstruction/Debris\u0026#34; | contrib == \u0026#34;Pavement Defective\u0026#34; | contrib == \u0026#34;Pavement Slippery\u0026#34; | contrib == \u0026#34;Pedestrian/Bicyclist/Other Pedestrian Error/Confusion\u0026#34; | contrib == \u0026#34;Reaction to Uninvolved Vehicle\u0026#34; | contrib == \u0026#34;Shoulders Defective/Improper\u0026#34; | contrib == \u0026#34;Traffic Control Device Improper/Non-Working\u0026#34; | contrib == \u0026#34;View Obstructed/Limited\u0026#34; | contrib == \u0026#34;Vehicle Vandalism\u0026#34; ~ \u0026#34;Road Conditions\u0026#34;, contrib == \u0026#34;Cell Phone (hand-Held)\u0026#34; | contrib == \u0026#34;Cell Phone (hands-free)\u0026#34; | contrib == \u0026#34;Driver Inattention/Distraction\u0026#34; | contrib == \u0026#34;Eating or Drinking\u0026#34; | contrib == \u0026#34;Other Electronic Device\u0026#34; | contrib == \u0026#34;Outside Car Distraction\u0026#34; | contrib == \u0026#34;Using Onboard Navigation Device\u0026#34; ~ \u0026#34;Distracted Driver\u0026#34;, contrib == \u0026#34;Fatigued/Drowsy\u0026#34; | contrib == \u0026#34;Fell Asleep\u0026#34; | contrib == \u0026#34;Illnes\u0026#34; | contrib == \u0026#34;Lost Consciousness\u0026#34; | contrib == \u0026#34;Physical Disability\u0026#34; ~ \u0026#34;Condition of Driver\u0026#34;, contrib == \u0026#34;Aggressive Driving/Road Rage\u0026#34; | contrib == \u0026#34;Traffic Control Disregarded\u0026#34; | contrib == \u0026#34;Unsafe Lane Change\u0026#34; | contrib == \u0026#34;Unsafe Speed\u0026#34; ~ \u0026#34;Dangerous Driving\u0026#34;, contrib == \u0026#34;Unspecified\u0026#34; | is.na(contrib) | contrib == \u0026#34;Other Vehicular\u0026#34; ~ NA )), type_cat = as.factor(case_when( type == \u0026#34;2 dr sedan\u0026#34; | type == \u0026#34;3-Door\u0026#34; | type == \u0026#34;4 dr sedan\u0026#34; | type == \u0026#34;Sedan\u0026#34; ~ \u0026#34;Sedan\u0026#34;, type == \u0026#34;Taxi\u0026#34; ~ \u0026#34;Taxi\u0026#34;, type == \u0026#34;Refrigerated Van\u0026#34; | type == \u0026#34;Van\u0026#34; | type == \u0026#34;Van Camper\u0026#34; | type == \u0026#34;van\u0026#34; ~ \u0026#34;Van\u0026#34;, type == \u0026#34;Station Wagon/Sport Utility Vehicle\u0026#34; ~ \u0026#34;SUV\u0026#34;, type == \u0026#34;AMBU\u0026#34; | type == \u0026#34;AMBULANCE\u0026#34; | type == \u0026#34;AMBULENCE\u0026#34; | type == \u0026#34;Ambulance\u0026#34; | type == \u0026#34;FDNY AMBUL\u0026#34; | type == \u0026#34;ambulance\u0026#34; ~ \u0026#34;Ambulance\u0026#34;, type == \u0026#34;Armored Truck\u0026#34; | type == \u0026#34;Beverage Truck\u0026#34; | type == \u0026#34;Box Truck\u0026#34; | type == \u0026#34;Concrete Mixer\u0026#34; | type == \u0026#34;DELIVERY T\u0026#34; | type == \u0026#34;DUMP\u0026#34; | type == \u0026#34;RV\u0026#34; | type == \u0026#34;DUMP TRUCK\u0026#34; | type == \u0026#34;Dump\u0026#34; | type == \u0026#34;Flat Bed\u0026#34; | type == \u0026#34;Flat Rack\u0026#34; | type == \u0026#34;GARBAGE TR\u0026#34; | type == \u0026#34;Garbage or Refuse\u0026#34; | type == \u0026#34;Road sweep\u0026#34; | type == \u0026#34;Sanitation\u0026#34; | type == \u0026#34;TOW TRUCK\u0026#34; | type == \u0026#34;TRAILER\u0026#34; | type == \u0026#34;TRUCK\u0026#34; | type == \u0026#34;Tanker\u0026#34; | type == \u0026#34;Tow Truck\u0026#34; | type == \u0026#34;Tow Truck / Wrecker\u0026#34; | type == \u0026#34;Tow truck\u0026#34; | type == \u0026#34;Tractor Tr\u0026#34; | type == \u0026#34;Tractor Truck Diesel\u0026#34; | type == \u0026#34;Tractor Truck Gasoline\u0026#34; | type == \u0026#34;Trailer\u0026#34; | type == \u0026#34;Truck\u0026#34; | type == \u0026#34;UHAUL\u0026#34; | type == \u0026#34;USPS\u0026#34; | type == \u0026#34;USPS VEHIC\u0026#34; | type == \u0026#34;UTILITY\u0026#34; | type == \u0026#34;Waste truc\u0026#34; ~ \u0026#34;Truck\u0026#34;, type == \u0026#34;Chassis Cab\u0026#34; | type == \u0026#34;Pick-up Truck\u0026#34; | type == \u0026#34;Pick up tr\u0026#34; ~ \u0026#34;Pick Up Truck\u0026#34;, type == \u0026#34;BOOM MOPED\u0026#34; | type == \u0026#34;GAS MOPED\u0026#34; | type == \u0026#34;MOPED\u0026#34; | type == \u0026#34;Moped\u0026#34;| type == \u0026#34;Motorbike\u0026#34; | type == \u0026#34;Motorcycle\u0026#34; | type == \u0026#34;Red moped\u0026#34; ~ \u0026#34;Motorcycle/Moped\u0026#34;, type == \u0026#34;Bike\u0026#34; | type == \u0026#34;E-Bike\u0026#34; | type == \u0026#34;Electric m\u0026#34; | type == \u0026#34;FLYWING MO\u0026#34; ~ \u0026#34;Bike/E-Bike\u0026#34;, type == \u0026#34;E-Scooter\u0026#34; | type == \u0026#34;MOTOR SCOO\u0026#34; | type == \u0026#34;MOTORIZEDS\u0026#34; | type == \u0026#34;Motorscooter\u0026#34; | type == \u0026#34;SCOOTER\u0026#34; ~ \u0026#34;Scooter/E-Scooter\u0026#34;, type == \u0026#34;Bus\u0026#34; | type == \u0026#34;Mta bus\u0026#34; | type == \u0026#34;Omnibus\u0026#34; | type == \u0026#34;SCHOOL BUS\u0026#34; | type == \u0026#34;SCHOOLBUS\u0026#34; | type == \u0026#34;School Bus\u0026#34; ~ \u0026#34;Bus\u0026#34;, type == \u0026#34;FDNY\u0026#34; | type == \u0026#34;FDNY FIRE\u0026#34; | type == \u0026#34;FDNY truck\u0026#34; | type == \u0026#34;FIRE ENGIN\u0026#34; | type == \u0026#34;FIRE TRUCK\u0026#34; | type == \u0026#34;FIRETRUCK\u0026#34; | type == \u0026#34;Fire Truck\u0026#34; | type == \u0026#34;Fire engin\u0026#34; | type == \u0026#34;Fire truck\u0026#34; | type == \u0026#34;Firetruck\u0026#34; | type == \u0026#34;Ladder\u0026#34; | type == \u0026#34;Ladder tru\u0026#34;| type == \u0026#34;NYC Fire T\u0026#34; ~ \u0026#34;Fire Truck\u0026#34;, is.na(type) ~ NA, TRUE ~ \u0026#34;Other\u0026#34; )), time_of_day = as.factor(case_when( hour \u0026lt; 6 | hour \u0026gt; 20 ~ \u0026#34;Night\u0026#34;, hour \u0026lt; 12 ~ \u0026#34;Morning\u0026#34;, hour \u0026lt; 18 ~ \u0026#34;Afternoon\u0026#34;, hour \u0026lt;= 20 ~ \u0026#34;Evening\u0026#34; )), fatal_cat = as.factor(case_when( number_of_persons_killed \u0026gt; 0 ~ \u0026#34;Fatal\u0026#34;, TRUE ~ \u0026#34;Non Fatal\u0026#34;) ), injured_cat = as.factor(case_when( number_of_persons_injured \u0026gt; 0 ~ \u0026#34;Injuries\u0026#34;, TRUE ~ \u0026#34;No Injuries\u0026#34;) )) Phew, that was a lot of work. Now I\u0026rsquo;m going to check to make sure that I didn\u0026rsquo;t leave anything out and everything was either categorized or caught by the catch all at the end of the new type of vehicle category variable.\n1 motor_cat %\u0026gt;% tabyl(contrib_cat) 1 2 3 4 5 6 7 8 9 ## contrib_cat n percent valid_percent ## Car Defects 173 0.009707104 0.01297143 ## Condition of Driver 236 0.013242060 0.01769513 ## Dangerous Driving 1657 0.092974975 0.12424083 ## Distracted Driver 4269 0.239535406 0.32008698 ## Driver Error 5999 0.336606441 0.44980130 ## Road Conditions 582 0.032656268 0.04363800 ## Substances 421 0.023622489 0.03156632 ## \u0026lt;NA\u0026gt; 4485 0.251655258 NA Looks good, now the type of vehicle category.\n1 motor_cat %\u0026gt;% tabyl(type_cat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## type_cat n percent valid_percent ## Ambulance 100 0.005611043 0.005703205 ## Bike/E-Bike 361 0.020255864 0.020588571 ## Bus 253 0.014195938 0.014429109 ## Fire Truck 18 0.001009988 0.001026577 ## Motorcycle/Moped 301 0.016889238 0.017166648 ## Other 102 0.005723263 0.005817269 ## Pick Up Truck 434 0.024351925 0.024751911 ## Scooter/E-Scooter 99 0.005554932 0.005646173 ## Sedan 8505 0.477219167 0.485057602 ## SUV 6625 0.371731568 0.377837345 ## Taxi 251 0.014083717 0.014315045 ## Truck 407 0.022836943 0.023212045 ## Van 78 0.004376613 0.004448500 ## \u0026lt;NA\u0026gt; 288 0.016159802 NA Here\u0026rsquo;s the final dataset, I generally display it in a tibble format as the blogdown format seems to display only the first few rows of tibbles which I like as it does not take up too much space on my blog post.\n1 2 3 4 5 6 motor_cat_df \u0026lt;- motor_cat %\u0026gt;% filter(!is.na(contrib_cat) \u0026amp; !is.na(type_cat)) %\u0026gt;% dplyr::select(collision_id, month, hour, contrib_cat, type_cat, time_of_day, injured_cat) motor_cat_final \u0026lt;- as_tibble(motor_cat_df) motor_cat_final 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 13,150 × 7 ## collision_id month hour contrib_cat type_cat time_of_day injured_cat ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 4679634 11 15 Driver Error Motorcycl… Afternoon Injuries ## 2 4663636 9 7 Distracted Driver Sedan Morning No Injuries ## 3 4663437 9 0 Driver Error Sedan Night No Injuries ## 4 4629913 5 15 Driver Error Sedan Afternoon No Injuries ## 5 4631543 5 16 Driver Error SUV Afternoon Injuries ## 6 4631801 5 14 Driver Error SUV Afternoon No Injuries ## 7 4632066 5 15 Distracted Driver Pick Up T… Afternoon No Injuries ## 8 4631365 5 9 Distracted Driver Truck Morning No Injuries ## 9 4680340 11 18 Dangerous Driving Motorcycl… Evening Injuries ## 10 4631597 5 15 Driver Error SUV Afternoon No Injuries ## # ℹ 13,140 more rows Data Visualization I wanted to visualize the predictors that I am going to use in the model just to see how everything is distributed and if there are any obvious differences that the model might pick up on.\nContributing Factor 1 2 3 4 5 6 7 8 9 ggplot(data = motor_cat_final, aes(y = contrib_cat, fill = injured_cat)) + geom_bar(color = \u0026#34;dimgray\u0026#34;, alpha = 0.8) + labs(title = \u0026#34;Count of Accidents and Their Outcomes by Contributing Factor\u0026#34;, y = \u0026#34;Primary Contributing Factor of Accident\u0026#34;, x = \u0026#34;Count\u0026#34;, caption = \u0026#34;Source: NYC Open Data Motor Vehicle Collisions- Crashes (January-December 2023)\u0026#34;, fill = \u0026#34;Injury Outcome\u0026#34;) + theme(plot.title = element_text(hjust=0.5)) + theme_minimal() The most common primary cause of the accident is distracted driving and driver error, this especially makes sense as distracted driving is made so easy these days by all the electronic devices that we have available. Type of Vehicle 1 2 3 4 5 6 7 8 9 ggplot(data = motor_cat_final, aes(y = type_cat, fill = injured_cat)) + geom_bar(color = \u0026#34;dimgray\u0026#34;, alpha = 0.8) + labs(title = \u0026#34;Count of Accidents and Their Outcomes by Type of Primary Vehicle\u0026#34;, y = \u0026#34;Primary Vehicle Involved in of Accident\u0026#34;, x = \u0026#34;Count\u0026#34;, caption = \u0026#34;Source: NYC Open Data Motor Vehicle Collisions- Crashes (January-December 2023)\u0026#34;, fill = \u0026#34;Injury Outcome\u0026#34;) + theme(plot.title = element_text(hjust=0.5)) + theme_minimal() One thing that stuck out to me, which is not very surprising now that I think about it, is the proportion of accidents that resulted in injuries when a bike/e-bike or motorcycle/moped is involved. This really emphasizes the importance of being safe and fully attentive when on the road not just for other drivers, but for bikes and motorcycles too, as well as the importance of bike infrastructure that would keep bikes seperate from other vehicles that can hurt them. Time of Day 1 2 3 4 5 6 7 8 9 ggplot(data = motor_cat_final, aes(x = time_of_day, fill = injured_cat)) + geom_bar(color = \u0026#34;dimgray\u0026#34;, alpha = 0.8) + labs(title = \u0026#34;Count of Accidents and Their Outcomes by Time of Day\u0026#34;, x = \u0026#34;Time of Day that Accident Occurred\u0026#34;, y = \u0026#34;Count\u0026#34;, caption = \u0026#34;Source: NYC Open Data Motor Vehicle Collisions- Crashes (January-December 2023)\u0026#34;, fill = \u0026#34;Injury Outcome\u0026#34;) + theme(plot.title = element_text(hjust=0.5)) + theme_minimal() Just inspecting this visually it does not look like there is that big of a difference between the different times of day and the proportion of accidents that result in injuries. Classification Models to Predict Injury Outcome of Motor Vehicle Collision Create Training and Test Data In order to create and test my model that classifies if an accident results in injuries or not for anyone involved, I\u0026rsquo;m going to seperate my training and test data by taking an arbitrary amount of observations to be considered test and training using the sample_frac() and anti_join() functions.\n1 2 3 4 5 6 7 8 set.seed(123) training \u0026lt;- motor_cat_df %\u0026gt;% sample_frac(.70) test \u0026lt;- motor_cat_df %\u0026gt;% anti_join(training, by = \u0026#34;collision_id\u0026#34;) training_injury_outcome \u0026lt;- training$injured_cat test_injury_outcome \u0026lt;- test$injured_cat Logistic Regression The first model that I wanted to fit was a logistic regression model, as there are only two different outcomes in the data (injuries vs. no injuries). Here I fitted that model with my predictors: contributing factor, type of vehicle, and time of day, along with my response variable which was if the accident resulted in injuries or not.\n1 2 3 4 glm.inj \u0026lt;- glm(injured_cat ~ contrib_cat + type_cat + time_of_day, data = training, family = binomial) summary(glm.inj) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 ## ## Call: ## glm(formula = injured_cat ~ contrib_cat + type_cat + time_of_day, ## family = binomial, data = training) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0102 -1.2497 0.8528 1.0539 2.3111 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 1.74107 0.48089 3.621 0.000294 *** ## contrib_catCondition of Driver -0.53689 0.24739 -2.170 0.029991 * ## contrib_catDangerous Driving -0.38954 0.19907 -1.957 0.050375 . ## contrib_catDistracted Driver 0.09706 0.19357 0.501 0.616072 ## contrib_catDriver Error -0.03027 0.19242 -0.157 0.874993 ## contrib_catRoad Conditions -0.53907 0.21675 -2.487 0.012880 * ## contrib_catSubstances 0.24526 0.22726 1.079 0.280488 ## type_catBike/E-Bike -3.93770 0.49501 -7.955 1.79e-15 *** ## type_catBus -1.76695 0.47378 -3.729 0.000192 *** ## type_catFire Truck -0.91710 0.94812 -0.967 0.333402 ## type_catMotorcycle/Moped -3.50744 0.49026 -7.154 8.42e-13 *** ## type_catOther -1.65302 0.50726 -3.259 0.001119 ** ## type_catPick Up Truck -1.30525 0.46299 -2.819 0.004814 ** ## type_catScooter/E-Scooter -3.41098 0.54718 -6.234 4.55e-10 *** ## type_catSedan -1.54048 0.44326 -3.475 0.000510 *** ## type_catSUV -1.66967 0.44347 -3.765 0.000167 *** ## type_catTaxi -2.37742 0.47385 -5.017 5.24e-07 *** ## type_catTruck -1.11660 0.46499 -2.401 0.016336 * ## type_catVan -1.29899 0.56568 -2.296 0.021658 * ## time_of_dayEvening -0.01271 0.06711 -0.189 0.849830 ## time_of_dayMorning 0.27367 0.05781 4.734 2.20e-06 *** ## time_of_dayNight 0.52662 0.05700 9.238 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 12644 on 9204 degrees of freedom ## Residual deviance: 12059 on 9183 degrees of freedom ## AIC: 12103 ## ## Number of Fisher Scoring iterations: 4 Then I used the predict() function to predict the outcome using my test data, and compared the real data along with the predicted data using a confusion matrix.\n1 2 3 4 5 6 7 inj.probs \u0026lt;- predict(glm.inj, test, type = \u0026#34;response\u0026#34;) inj.pred \u0026lt;- rep(\u0026#34;No Injuries\u0026#34;, length(inj.probs)) inj.pred[inj.probs \u0026gt; 0.5] \u0026lt;- \u0026#34;Injuries\u0026#34; table(inj.pred, test_injury_outcome) 1 2 3 4 ## test_injury_outcome ## inj.pred Injuries No Injuries ## Injuries 1304 1980 ## No Injuries 422 239 1 mean(inj.pred == test_injury_outcome) 1 ## [1] 0.391128 Yikes, that\u0026rsquo;s worse than just random guessing. I\u0026rsquo;m going to use a different model now and see how that compares to the logistic regression.\nNaive Bayes I chose to use the naive bayes model as it does not assume any particular distribution in the data. I fit the model using the naiveBayes() function from the e1071 package and displayed the output of the model object below.\n1 2 3 nb.inj \u0026lt;- naiveBayes(injured_cat ~ contrib_cat + type_cat + time_of_day, data = training) nb.inj 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## Injuries No Injuries ## 0.4437806 0.5562194 ## ## Conditional probabilities: ## contrib_cat ## Y Car Defects Condition of Driver Dangerous Driving ## Injuries 0.01126071 0.02105263 0.14369645 ## No Injuries 0.01445313 0.01484375 0.10761719 ## contrib_cat ## Y Distracted Driver Driver Error Road Conditions Substances ## Injuries 0.29987760 0.44504284 0.05483476 0.02423501 ## No Injuries 0.34003906 0.45058594 0.03339844 0.03906250 ## ## type_cat ## Y Ambulance Bike/E-Bike Bus Fire Truck ## Injuries 0.0014687882 0.0438188494 0.0166462668 0.0004895961 ## No Injuries 0.0074218750 0.0044921875 0.0142578125 0.0009765625 ## type_cat ## Y Motorcycle/Moped Other Pick Up Truck Scooter/E-Scooter ## Injuries 0.0337821297 0.0073439412 0.0205630355 0.0127294982 ## No Injuries 0.0052734375 0.0072265625 0.0292968750 0.0023437500 ## type_cat ## Y Sedan SUV Taxi Truck Van ## Injuries 0.4411260710 0.3801713586 0.0212974296 0.0173806610 0.0031823745 ## No Injuries 0.5011718750 0.3804687500 0.0117187500 0.0310546875 0.0042968750 ## ## time_of_day ## Y Afternoon Evening Morning Night ## Injuries 0.3618115 0.1703794 0.2296206 0.2381885 ## No Injuries 0.2933594 0.1333984 0.2527344 0.3205078 Now to check the model to see how it compares to my worse-than-random-guessing logistic regression model.\n1 2 nb.inj.pred \u0026lt;- predict(nb.inj, test) table(nb.inj.pred, test_injury_outcome) 1 2 3 4 ## test_injury_outcome ## nb.inj.pred Injuries No Injuries ## Injuries 461 263 ## No Injuries 1265 1956 1 mean(nb.inj.pred == test_injury_outcome) 1 ## [1] 0.6126743 Well that\u0026rsquo;s not great, but it\u0026rsquo;s definitely an improvement from the previous logistic regression model.\nImprove Logistic Regression Model It looks like the primary contributing factor does not really have much statistical significance if there are injuries in a motor vehicle accident or not, I\u0026rsquo;m going to remove that variable from each model and see how that changes things.\n1 2 3 4 glm.inj.new \u0026lt;- glm(injured_cat ~ type_cat + time_of_day, data = training, family = binomial) summary(glm.inj.new) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 ## ## Call: ## glm(formula = injured_cat ~ type_cat + time_of_day, family = binomial, ## data = training) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1353 -1.2387 0.9115 1.0588 2.1639 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 1.651322 0.441470 3.741 0.000184 *** ## type_catBike/E-Bike -3.888045 0.493421 -7.880 3.28e-15 *** ## type_catBus -1.732093 0.472146 -3.669 0.000244 *** ## type_catFire Truck -0.806790 0.946818 -0.852 0.394155 ## type_catMotorcycle/Moped -3.482104 0.488651 -7.126 1.03e-12 *** ## type_catOther -1.608261 0.505064 -3.184 0.001451 ** ## type_catPick Up Truck -1.283938 0.461426 -2.783 0.005393 ** ## type_catScooter/E-Scooter -3.353955 0.545475 -6.149 7.81e-10 *** ## type_catSedan -1.508263 0.441687 -3.415 0.000638 *** ## type_catSUV -1.633338 0.441928 -3.696 0.000219 *** ## type_catTaxi -2.305245 0.471983 -4.884 1.04e-06 *** ## type_catTruck -1.076075 0.463503 -2.322 0.020254 * ## type_catVan -1.271352 0.563454 -2.256 0.024048 * ## time_of_dayEvening -0.003336 0.066715 -0.050 0.960122 ## time_of_dayMorning 0.267486 0.057502 4.652 3.29e-06 *** ## time_of_dayNight 0.520432 0.055919 9.307 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 12644 on 9204 degrees of freedom ## Residual deviance: 12142 on 9189 degrees of freedom ## AIC: 12174 ## ## Number of Fisher Scoring iterations: 4 Now checking again\n1 2 3 4 5 6 7 inj.probs.new \u0026lt;- predict(glm.inj.new, test, type = \u0026#34;response\u0026#34;) inj.pred.new \u0026lt;- rep(\u0026#34;No Injuries\u0026#34;, length(inj.probs.new)) inj.pred.new[inj.probs.new \u0026gt; 0.5] \u0026lt;- \u0026#34;Injuries\u0026#34; table(inj.pred.new, test_injury_outcome) 1 2 3 4 ## test_injury_outcome ## inj.pred.new Injuries No Injuries ## Injuries 1524 2156 ## No Injuries 202 63 1 mean(inj.pred.new == test_injury_outcome) 1 ## [1] 0.4022814 Still worse than guessing even though this does result in a slight improvement, but the Naive Bayes model still remains on top.\nConclusion I learned so many new things trying to practice the skills that I\u0026rsquo;ve learned using classification models. I\u0026rsquo;m starting to become more familiar with the code on how to make predictions and check a classification model. Also, I learned a different way on how to seperate test and training data using the sample_frac() function in conjunction with the anti_join() function to create two different data frames that have a random fraction of observations in one data frame, and then the ones that were not selected in the other data frame.\nI also see how messy working with real life data rather than data from an R package or a textbook is, this dataset is so huge and I only used a very pared down version of it. The original has millions of observations! There are also a lot of instances of human error and just differences in recording observations in the contributing factor and type of vehicle columns.\n","date":"2024-02-12T00:00:00Z","image":"https://michelleyg1.github.io/p/accident-classification/images/bridge_hu5459c0360c2b0cb7a147d2df0eb350ca_3648849_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/accident-classification/","title":"NYC Open Data Motor Vehicle Collision Injury Outcome Classification"},{"content":"Currently, I\u0026rsquo;m working through this great book An Introduction to Statistical Learning with Applications in R, and I just finished the chapter on linear regression. One thing that I really like about this book is that there is a great balance of learning R and just learning statistics in a mathematical way which really shows the backbone of the R functions that I was just using without really knowing how they work.\nFor this analysis I used the built in dataset \u0026ldquo;airquality\u0026rdquo; in R which stores 153 observations of different air quality measures averaged out by day taken in New York City from May to September 1973. I used this to practice my skills in linear regression, both simple and multivariate.\nLoad Libraries 1 2 3 4 5 6 suppressPackageStartupMessages({ library(tidyverse) library(broom) library(car) library(ggthemes) }) Load Data 1 data(\u0026#34;airquality\u0026#34;) Explore and Clean Data 1 str(airquality) 1 2 3 4 5 6 7 ## \u0026#39;data.frame\u0026#39;:\t153 obs. of 6 variables: ## $ Ozone : int 41 36 12 18 NA 28 23 19 8 NA ... ## $ Solar.R: int 190 118 149 313 NA NA 299 99 19 194 ... ## $ Wind : num 7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ... ## $ Temp : int 67 72 74 62 56 66 65 59 61 69 ... ## $ Month : int 5 5 5 5 5 5 5 5 5 5 ... ## $ Day : int 1 2 3 4 5 6 7 8 9 10 ... Summary Statistics Creating a table of summary statistics that also doubles as a way to check for missing values, as since this dataset only has 153 observations so substantial amounts of missing data will affect the outcomes.\n1 2 3 4 airquality %\u0026gt;% dplyr::select(Ozone, Solar.R, Wind, Temp) %\u0026gt;% map_df(.f = ~broom::tidy(summary(.x)), .id = \u0026#34;variable\u0026#34;) 1 2 3 4 5 6 7 ## # A tibble: 4 × 8 ## variable minimum q1 median mean q3 maximum na ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Ozone 1 18 31.5 42.1 63.2 168 37 ## 2 Solar.R 7 116. 205 186. 259. 334 7 ## 3 Wind 1.7 7.4 9.7 9.96 11.5 20.7 NA ## 4 Temp 56 72 79 77.9 85 97 NA So it looks like there are a decent amount of missing values for the ozone variable, I\u0026rsquo;m going to use mean imputation to deal with this.\nMean Imputation for Missing Values 1 2 3 4 5 6 7 8 9 10 airquality_mi \u0026lt;- airquality %\u0026gt;% mutate( ozone_mi = as.integer(if_else(is.na(Ozone), mean(Ozone, na.rm = TRUE), Ozone)), solar_mi = as.integer(if_else(is.na(Solar.R), mean(Solar.R, na.rm = TRUE), Solar.R)) ) %\u0026gt;% select(-1, -2) Clean Up and Print 1 2 3 4 5 6 names(airquality_mi)[1:6] \u0026lt;- tolower(names(airquality_mi)[1:6]) airquality_mi \u0026lt;- airquality_mi %\u0026gt;% select(month, day, wind, temp, ozone_mi, solar_mi) as_tibble(airquality_mi) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 153 × 6 ## month day wind temp ozone_mi solar_mi ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 5 1 7.4 67 41 190 ## 2 5 2 8 72 36 118 ## 3 5 3 12.6 74 12 149 ## 4 5 4 11.5 62 18 313 ## 5 5 5 14.3 56 42 185 ## 6 5 6 14.9 66 28 185 ## 7 5 7 8.6 65 23 299 ## 8 5 8 13.8 59 19 99 ## 9 5 9 20.1 61 8 19 ## 10 5 10 8.6 69 42 194 ## # ℹ 143 more rows Simple Regression Visualize Variables of Interest 1 2 3 4 5 6 7 8 9 10 ggplot(data = airquality_mi, aes(x = temp, y = ozone_mi)) + geom_jitter() + labs( x = \u0026#34;Temperature Measured in Fahrenheit\u0026#34;, y = \u0026#34;Mean Ozone per Day in Parts Per Billion\u0026#34;, title = \u0026#34;Ozone Concentration vs. Temperature in New York City from May to September 1973\u0026#34; ) + theme_stata() + theme(plot.title = element_text(hjust = 0.5)) Fit Model 1 2 aq.simple \u0026lt;- lm(ozone_mi ~ temp, data = airquality_mi) summary(aq.simple) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## ## Call: ## lm(formula = ozone_mi ~ temp, data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.006 -15.851 -2.160 8.469 120.149 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -101.6222 15.3547 -6.618 5.96e-10 *** ## temp 1.8454 0.1957 9.428 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 22.84 on 151 degrees of freedom ## Multiple R-squared: 0.3706,\tAdjusted R-squared: 0.3664 ## F-statistic: 88.9 on 1 and 151 DF, p-value: \u0026lt; 2.2e-16 Looking at the summary of this model its already pretty evident that there are some issues, the residual standard error is pretty high and the R squared shows that a majority of variation cannot be explained by the model. I\u0026rsquo;m also going to run some diagnostic plots to see if there are other issues.\nDiagnostics 1 2 par(mfrow = c(2, 2)) plot(aq.simple) These plots suggest that there may be a non linear relationship between the variables, so I'm going to apply a transformation and see if that helps. Polynomial Transformation 1 2 aq.simple.trans \u0026lt;- lm(ozone_mi ~ temp + I(temp^2), data = airquality_mi) summary(aq.simple.trans) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## ## Call: ## lm(formula = ozone_mi ~ temp + I(temp^2), data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.463 -13.131 -2.735 9.869 124.916 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 296.43794 94.87393 3.125 0.002138 ** ## temp -8.78275 2.50999 -3.499 0.000615 *** ## I(temp^2) 0.06981 0.01644 4.246 3.8e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 21.65 on 150 degrees of freedom ## Multiple R-squared: 0.4381,\tAdjusted R-squared: 0.4306 ## F-statistic: 58.47 on 2 and 150 DF, p-value: \u0026lt; 2.2e-16 So that\u0026rsquo;s a bit better in terms of R squared, I\u0026rsquo;m going to re-check the diagnostic plots.\n1 2 par(mfrow = c(2, 2)) plot(aq.simple.trans) This is not the best model for the data, but being able to recognize that is also important. I\u0026rsquo;m going to visualize the regression line on my previous plot for the sake of practice.\nVisualize with Least Squares Regression Line 1 2 3 4 5 6 7 8 9 10 11 12 ggplot(data = airquality_mi, aes(x = temp, y = ozone_mi)) + geom_jitter() + stat_smooth(method = \u0026#34;lm\u0026#34;, formula = y ~ poly(x, 2, raw = TRUE)) + labs( x = \u0026#34;Temperature Measured in Fahrenheit\u0026#34;, y = \u0026#34;Mean Ozone per Day in Parts Per Billion\u0026#34;, title = \u0026#34;Ozone Concentration vs. Temperature in New York City from May to September 1973\u0026#34; ) + theme_stata() + theme(plot.title = element_text(hjust = 0.5)) Multivariate Regression I also want to do a multivariate regression with this data to see if I can come up with a better model and be able to practice making predictions using a regression model.\nFit Model 1 2 aq.mv \u0026lt;- lm(ozone_mi ~ temp + wind + solar_mi, data = airquality_mi) summary(aq.mv) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## ## Call: ## lm(formula = ozone_mi ~ temp + wind + solar_mi, data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.541 -14.590 -5.148 12.172 101.198 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -38.20875 18.88354 -2.023 0.04482 * ## temp 1.24096 0.20907 5.935 1.97e-08 *** ## wind -2.71862 0.54280 -5.009 1.53e-06 *** ## solar_mi 0.05772 0.02003 2.881 0.00454 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 20.9 on 149 degrees of freedom ## Multiple R-squared: 0.48,\tAdjusted R-squared: 0.4696 ## F-statistic: 45.85 on 3 and 149 DF, p-value: \u0026lt; 2.2e-16 Diagnostics The first thing I want to check since I have multiple predictor variables in my new model is colinearity. I\u0026rsquo;m going to use the vif function to calculate the variance inflation factor to see if there are any variables that are correlated too strongly.\n1 vif(aq.mv) 1 2 ## temp wind solar_mi ## 1.363082 1.272795 1.080454 Looks good! Moving on to normal diagnostic plots\n1 2 par(mfrow = c(2, 2)) plot(aq.mv) This model could use some work to get it to fit the data and assumptions a bit better, I'm going to see if doing some transformations of the predictor helps. Transformation First I\u0026rsquo;m going to visualize my individual predictor variables with the response variables just to see what I\u0026rsquo;m working with and which transformations might help me.\nI already visualized ozone and temperature in the simple regression, so I\u0026rsquo;m going to visualize ozone and wind.\n1 2 3 4 5 6 7 8 9 10 ggplot(data = airquality_mi, aes(x = wind, y = ozone_mi)) + geom_jitter() + labs( x = \u0026#34;Average Wind Speed Measured in Miles Per Hour\u0026#34;, y = \u0026#34;Mean Ozone per Day in Parts Per Billion\u0026#34;, title = \u0026#34;Ozone Concentration vs. Wind in New York City from May to September 1973\u0026#34; ) + theme_stata() + theme(plot.title = element_text(hjust = 0.5)) And ozone and solar.\n1 2 3 4 5 6 7 8 9 10 ggplot(data = airquality_mi, aes(x = solar_mi, y = ozone_mi)) + geom_jitter() + labs( x = \u0026#34;Solar Radiation Measured in Langleys\u0026#34;, y = \u0026#34;Mean Ozone per Day in Parts Per Billion\u0026#34;, title = \u0026#34;Ozone Concentration vs. Solar Radiation in New York City from May to September 1973\u0026#34; ) + theme_stata() + theme(plot.title = element_text(hjust = 0.5)) I\u0026rsquo;m going to apply polynomial transformations to two out of the three predictor variables.\n1 2 aq.mv.trans \u0026lt;- lm(ozone_mi ~ temp + I(temp^2) + wind + I(wind^2) + solar_mi, data = airquality_mi) summary(aq.mv.trans) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## ## Call: ## lm(formula = ozone_mi ~ temp + I(temp^2) + wind + I(wind^2) + ## solar_mi, data = airquality_mi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.293 -10.929 -3.289 8.684 89.513 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 321.25924 81.78038 3.928 0.000131 *** ## temp -7.28999 2.18385 -3.338 0.001069 ** ## I(temp^2) 0.05580 0.01431 3.898 0.000147 *** ## wind -11.10077 1.94439 -5.709 6.07e-08 *** ## I(wind^2) 0.39616 0.08865 4.469 1.56e-05 *** ## solar_mi 0.06206 0.01772 3.503 0.000610 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 18.46 on 147 degrees of freedom ## Multiple R-squared: 0.5998,\tAdjusted R-squared: 0.5862 ## F-statistic: 44.07 on 5 and 147 DF, p-value: \u0026lt; 2.2e-16 This R2 and RSE is definitely better than the simple regression model, as now almost 60% of the variation in the data can be explained by the model rather than being closer to 40%, still not the best but definitely better. Now I\u0026rsquo;m going to re-check the diagnostics to see if this satisfies the assumptions a bit better.\n1 2 par(mfrow = c(2, 2)) plot(aq.mv.trans) This definitely helped deal with the pattern in the residuals vs. fitted plot and helps satisfy the linearity assumption of the model. Make Predictions I\u0026rsquo;m going to use this model to make a couple predictions about ozone levels when given information about temperature, solar radiation, and wind speeds, along with confidence intervals.\n1 2 3 4 5 6 7 8 9 10 11 new_data_point \u0026lt;- data.frame( solar_mi = c(250, 120, 320), temp = c(63, 74, 82), wind = c(10.2, 9.5, 7.0) ) predict( aq.mv.trans, new_data_point, interval = \u0026#34;confidence\u0026#34; ) 1 2 3 4 ## fit lwr upr ## 1 26.96636 19.31629 34.61643 ## 2 25.10863 20.28017 29.93708 ## 3 60.25022 54.08080 66.41964 Conclusion I\u0026rsquo;ve tried to perform linear regressions before, as it is very simple to do in R but I really didn\u0026rsquo;t understand any of the outputs of the model or what constitutes as a good model, but the Linear Regression chapter of Intro to Statistical Learning along with running this analysis helped me understand those concepts better.\nI also, until then, didn\u0026rsquo;t really understand the importance of linear regression and thought that it was just similar to correlation, but we can actually use linear regression in order to input new data to get a prediction for the response variable, in this case the amount of ozone in the atmosphere.\nIn working on this analysis I also learned about mean imputation when dealing with missing values, which has its advantages and disadvantages compared to just removing rows with missing values.\n","date":"2024-02-02T00:00:00Z","image":"https://michelleyg1.github.io/p/airquality/images/smoke_hu5459c0360c2b0cb7a147d2df0eb350ca_571128_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/airquality/","title":"Air Quality Regression"},{"content":"One thing that I’ve learned since I’ve been looking into working with large complex survey data like the CDC’s NHANES is about survey weights and how to use them in your analysis. In this analysis I tried to use the survey package in R to use the survey design to analyze the data, as there are certain demographics that were oversampled and undersampled as a part of the survey design.\nLoad Packages 1 2 3 4 5 6 7 suppressPackageStartupMessages({ library(tidyverse) library(nhanesA) library(janitor) library(survey) library(gtsummary) }) Load Data 1 2 demo \u0026lt;- nhanes(\u0026#34;P_DEMO\u0026#34;) demo \u0026lt;- nhanesTranslate(\u0026#34;P_DEMO\u0026#34;, names(demo), data = demo) ## Translated columns: RIDSTATR RIAGENDR RIDRETH1 RIDRETH3 RIDEXMON DMDBORN4 DMDYRUSZ DMDEDUC2 DMDMARTZ RIDEXPRG SIALANG SIAPROXY SIAINTRP FIALANG FIAPROXY FIAINTRP MIALANG MIAPROXY MIAINTRP AIALANGA 1 2 exam \u0026lt;- nhanes(\u0026#34;P_BMX\u0026#34;) exam \u0026lt;- nhanesTranslate(\u0026#34;P_BMX\u0026#34;, names(exam), data = exam) ## Translated columns: BMDSTATS BMIWT BMIHT BMDBMIC Retain Useful Variables 1 2 3 4 5 demo_select \u0026lt;- demo %\u0026gt;% select(SEQN, RIAGENDR, RIDAGEYR, RIDRETH3, SDMVPSU, SDMVSTRA, WTMECPRP) exam_select \u0026lt;- exam %\u0026gt;% select(SEQN, BMXBMI) Merge Data 1 2 3 merged_data \u0026lt;- merge(demo_select, exam_select, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) merged_data$SEQN \u0026lt;- NULL Clean Dataset to Make Analysis Easier First I wanted to see which race categories there are so that I can capture all of them when I recode them into more simple categories.\n1 merged_data %\u0026gt;% tabyl(RIDRETH3) ## RIDRETH3 n percent ## Mexican American 1990 0.12789203 ## Other Hispanic 1544 0.09922879 ## Non-Hispanic White 5271 0.33875321 ## Non-Hispanic Black 4098 0.26336761 ## Non-Hispanic Asian 1638 0.10526992 ## Other Race - Including Multi-Racial 1019 0.06548843 Now, I’m going to rename some variables, and use case_when() to make the race_cat and bmi_cat columns, as well as factoring these columns so that they can be used in my analysis and are not just recognized as character strings by R.\nI also added at the end to output a tibble of the raw NHANES data without the weights to print out how my clean data looks and make sure that I didn’t miss any columns in the renaming process.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 nhanes \u0026lt;- merged_data %\u0026gt;% rename( gender = RIAGENDR, age = RIDAGEYR, bmi = BMXBMI, psu = SDMVPSU, strata = SDMVSTRA, weight = WTMECPRP) %\u0026gt;% mutate( race_cat = case_when(RIDRETH3 == \u0026#34;Mexican American\u0026#34; | RIDRETH3 == \u0026#34;Other Hispanic\u0026#34; ~ \u0026#34;Hispanic\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic White\u0026#34; ~ \u0026#34;White\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic Black\u0026#34; ~ \u0026#34;Black\u0026#34;, RIDRETH3 == \u0026#34;Non-Hispanic Asian\u0026#34; ~ \u0026#34;Asian\u0026#34;, RIDRETH3 == \u0026#34;Other Race - Including Multi-Racial\u0026#34; ~ \u0026#34;Other\u0026#34;), bmi_cat = case_when(bmi \u0026lt; 18.5 ~ \u0026#34;Underweight\u0026#34;, bmi \u0026lt; 25 ~ \u0026#34;Normal\u0026#34;, bmi \u0026lt; 30 ~ \u0026#34;Overweight\u0026#34;, bmi \u0026gt;= 30 ~ \u0026#34;Obese\u0026#34;) ) %\u0026gt;% filter(!is.na(bmi)) %\u0026gt;% mutate_if(., is.character, as.factor) %\u0026gt;% select(-RIDRETH3) # Unweighted nhanes_tibble \u0026lt;- as_tibble(nhanes) %\u0026gt;% filter(age \u0026gt;= 18) %\u0026gt;% select(-psu, -strata, -weight) %\u0026gt;% print() ## # A tibble: 8,790 × 5 ## gender age bmi race_cat bmi_cat ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Female 29 37.8 Asian Obese ## 2 Male 49 29.7 White Overweight ## 3 Male 36 21.9 White Normal ## 4 Male 68 30.2 Other Obese ## 5 Male 76 26.6 White Overweight ## 6 Female 44 39.1 Hispanic Obese ## 7 Female 33 28.9 Asian Overweight ## 8 Female 68 28.1 Black Overweight ## 9 Female 42 31.3 Asian Obese ## 10 Male 58 30.5 Hispanic Obese ## # ℹ 8,780 more rows Refactor Order of BMI Categories R shows factored variables autmatically in alphabetical order, but this did not make much sense for my chart as BMI categories make more sense from underweight to obese, so I used the factor() function to remedy this before I make my table.\n1 2 nhanes$bmi_cat \u0026lt;- factor(nhanes$bmi_cat, levels = c(\u0026#34;Underweight\u0026#34;, \u0026#34;Normal\u0026#34;, \u0026#34;Overweight\u0026#34;, \u0026#34;Obese\u0026#34;)) Survey Design I learned that in order to properly analyze large survey data, and to make it representative of the population at large, you have to create a survey design object and use that in your analysis.\n1 2 3 4 5 nhanes_design \u0026lt;- svydesign(id = ~psu, strata = ~strata, weights = ~weight, nest = TRUE, data = nhanes) Apply Eligibility Criteria It is also important that the data is subset in this manner when working with survey data.\n1 nhanes_adult \u0026lt;- subset(nhanes_design, age \u0026gt;= 18) Summary Statistics BMI Categories by Race/Hispanic Origin, Gender, and Average Age Weighted 1 2 3 4 5 6 7 8 tbl_svysummary(nhanes_adult, by = bmi_cat, include = c(gender, age, race_cat), label = list(gender ~ \u0026#34;Gender\u0026#34;, age ~ \u0026#34;Age\u0026#34;, race_cat ~ \u0026#34;Race/Hispanic Origin\u0026#34;), statistic = list(age ~ \u0026#34;{mean} ({sd})\u0026#34;) ) \u0026#10; Characteristic Underweight, N = 3,797,8841 Normal, N = 62,452,8711 Overweight, N = 76,984,8391 Obese, N = 101,342,0061 Gender Male 1,148,289 (30%) 27,234,973 (44%) 41,256,750 (54%) 48,289,482 (48%) Female 2,649,595 (70%) 35,217,899 (56%) 35,728,089 (46%) 53,052,524 (52%) Age 37 (17) 44 (19) 50 (17) 48 (17) Race/Hispanic Origin Asian 385,874 (10%) 6,348,964 (10%) 5,473,194 (7.1%) 2,321,160 (2.3%) Black 613,071 (16%) 6,036,949 (9.7%) 7,381,673 (9.6%) 13,643,866 (13%) Hispanic 440,910 (12%) 7,245,902 (12%) 14,181,024 (18%) 17,625,000 (17%) Other 106,290 (2.8%) 2,276,951 (3.6%) 2,692,021 (3.5%) 4,801,387 (4.7%) White 2,251,739 (59%) 40,544,106 (65%) 47,256,927 (61%) 62,950,593 (62%) \u0026#10; 1 n (%); Mean (SD) BMI Categories by Race/Hispanic Origin, Gender, and Average Age Unweighted 1 2 3 4 5 6 7 8 tbl_summary(nhanes_tibble, by = bmi_cat, include = c(gender, age, race_cat), label = list(gender ~ \u0026#34;Gender\u0026#34;, age ~ \u0026#34;Age\u0026#34;, race_cat ~ \u0026#34;Race/Hispanic Origin\u0026#34;), statistic = list(age ~ \u0026#34;{mean} ({sd})\u0026#34;) ) \u0026#10; Characteristic Normal, N = 2,1851 Obese, N = 3,6881 Overweight, N = 2,7671 Underweight, N = 1501 Gender Male 1,044 (48%) 1,644 (45%) 1,521 (55%) 62 (41%) Female 1,141 (52%) 2,044 (55%) 1,246 (45%) 88 (59%) Age 46 (20) 50 (17) 52 (18) 39 (20) Race/Hispanic Origin Asian 478 (22%) 163 (4.4%) 403 (15%) 27 (18%) Black 494 (23%) 1,175 (32%) 609 (22%) 48 (32%) Hispanic 341 (16%) 879 (24%) 701 (25%) 19 (13%) Other 110 (5.0%) 198 (5.4%) 113 (4.1%) 6 (4.0%) White 762 (35%) 1,273 (35%) 941 (34%) 50 (33%) \u0026#10; 1 n (%); Mean (SD) Gender vs BMI T-Test Normality Assumption 1 svyqqmath(~bmi, design = nhanes_adult) This deviates from the normal distribution, so I applied a log transformation to the BMI variable to remedy this deviation from the normal distribution at the tail end of the data.\n1 svyqqmath(~log10(bmi), design = nhanes_adult) That looks a lot better, now time to visualize the data and run the analysis. Boxplot Visualization 1 2 3 4 5 svyboxplot(log10(bmi) ~ gender, design = nhanes_adult, xlab = \u0026#34;Gender\u0026#34;, main = \u0026#34;Weighted Boxplot of Mean Log Transformed BMI by Gender\u0026#34;, ylab = \u0026#34;Log Transformed BMI\u0026#34;) T-Test 1 svyttest(log10(bmi) ~ gender, nhanes_adult) ## ## Design-based t-test ## ## data: log10(bmi) ~ gender ## t = 0.46451, df = 24, p-value = 0.6465 ## alternative hypothesis: true difference in mean is not equal to 0 ## 95 percent confidence interval: ## -0.005848037 0.009244940 ## sample estimates: ## difference in mean ## 0.001698451 Race/Hispanic Origin vs. BMI ANOVA 1 2 3 4 5 6 svyboxplot(log10(bmi) ~ race_cat, design = nhanes_adult, xlab = \u0026#34;Race/Hispanic Origin\u0026#34;, ylab = \u0026#34;Log Transformed BMI\u0026#34;, main = \u0026#34;Weighted Boxplot of Mean Log Transformed BMI by Race/Hispanic Origin\u0026#34;) In visually inspecting the boxplot, it looks like there is a difference between the various race/hispanic origin groups and BMI, but lets see. Fit Model 1 race.bmi.glm \u0026lt;- svyglm(log10(bmi) ~ race_cat, nhanes_adult) ANOVA A barrier that I ran into was that the survey package in R does not seen to have an ANOVA function, and when you run the survey GLM through the regular ANOVA function it does not work as I thought it would. I did some researching around and I found that this is a way to assess this for complex surveys using a Wald test.\n1 2 options(scipen = 999) regTermTest(race.bmi.glm, ~race_cat) ## Wald test for race_cat ## in svyglm(formula = log10(bmi) ~ race_cat, design = nhanes_adult) ## F = 184.1178 on 4 and 21 df: p= 0.00000000000000050064 So it looks like there is a statistically significant difference between the various Race/Hispanic Origin categories and BMI, as hypothesized by the boxplot.\nConclusion While the NHANES dataset is still very useful, I did have to dig a lot deeper and do more research to find out how I can properly use this data.\nAs I am also working through another book on statistics, I found out that there is not just parametric statistics if your data is normal, and non parmetric statistics if it isn’t, but that you’re allowed to transform your data to help fit better into the normal distribution so that you can use parametric statistics like T-Tests with more confidence.\n","date":"2024-01-20T00:00:00Z","image":"https://michelleyg1.github.io/p/nhanes-bmi-analysis-using-survey-package/images/green_hu5459c0360c2b0cb7a147d2df0eb350ca_2075947_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/nhanes-bmi-analysis-using-survey-package/","title":"NHANES BMI Analysis Using Survey Package"},{"content":"For this analysis I decided to use the CDC NHANES data (2017 - pre pandemic 2020). I found this guide on importing and cleaning NHANES data using the nhanesA package, and found it very useful in this project.\nNHANES is a very large survey conducted by the CDC that is a crucial resource for public health data analysis, and is all available for free public use. As this was my first time using this dataset, I decided to keep it simple, but I will definitely be coming back to it as there is just so much information collected that can be used for all types of public health analysis.\nLoad Packages 1 2 3 4 5 6 7 suppressPackageStartupMessages({ library(tidyverse) library(nhanesA) library(tableone) library(arsenal) library(agricolae) }) Grab Datasets that I Need Here I used the nhanesA package to get the datasets (or Data Files as they are called on the NHANES site), the two that I needed for this analysis were the demographics dataset\n1 2 demo \u0026lt;- nhanes(\u0026#34;P_DEMO\u0026#34;) demo_translate \u0026lt;- nhanesTranslate(\u0026#34;P_DEMO\u0026#34;, names(demo), data = demo) 1 ## Translated columns: RIDSTATR RIAGENDR RIDRETH1 RIDRETH3 RIDEXMON DMDBORN4 DMDYRUSZ DMDEDUC2 DMDMARTZ RIDEXPRG SIALANG SIAPROXY SIAINTRP FIALANG FIAPROXY FIAINTRP MIALANG MIAPROXY MIAINTRP AIALANGA and the total cholesterol one\n1 2 exam \u0026lt;- nhanes(\u0026#34;P_TCHOL\u0026#34;) exam_translate \u0026lt;- nhanesTranslate(\u0026#34;P_TCHOL\u0026#34;, names(exam), data = exam) 1 2 ## Warning in nhanesTranslate(\u0026#34;P_TCHOL\u0026#34;, names(exam), data = exam): No columns ## were translated Retain Useful Variables Here I am only retaining the variables that I want to bring into my final dataset.\nFrom the demographics file, I decided on SEQN which is the sequence number that will help us merge the two datasets, RIDEXPRG which indicates if the respondent is pregnant, RIAGENDR which stores the participants gender, RIDAGEYR which stores the participant\u0026rsquo;s age in years, and lastly RIDRETH3 which categorizes the participant\u0026rsquo;s race.\nFrom the exam file I only extracted the sequence number, and lab value for total cholesterol.\n1 2 3 4 5 demo_select \u0026lt;- demo_translate %\u0026gt;% select(SEQN, RIDEXPRG, RIAGENDR, RIDAGEYR, RIDRETH3) exam_select \u0026lt;- exam_translate %\u0026gt;% select(SEQN, LBXTC) Merge the Data Using SEQN to Match Values The structure of the NHANES database makes it easy to match which lab values belong to which patient in including the SEQN column on the seperate datasets. I also went ahead and dropped the SEQN column when I was done merging as I only needed it for that task.\n1 2 3 merged_data \u0026lt;- merge(demo_select, exam_select, by = c(\u0026#34;SEQN\u0026#34;), all = TRUE) merged_data$SEQN \u0026lt;- NULL Initial Investigaton of the Data Here I want to see the different categories and how much we have of each so that I can apply the eligibility criteria and recode the values properly.\n1 2 3 4 initial_table \u0026lt;- CreateTableOne(data = merged_data, includeNA = TRUE) print(initial_table, showAllLevels = TRUE) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## ## level ## n ## RIDEXPRG (%) Yes, positive lab pregnancy test or self-reported pregnant at exam ## The participant was not pregnant at exam ## Cannot ascertain if the participant is pregnant at exam ## \u0026lt;NA\u0026gt; ## RIAGENDR (%) Male ## Female ## RIDAGEYR (mean (SD)) ## RIDRETH3 (%) Mexican American ## Other Hispanic ## Non-Hispanic White ## Non-Hispanic Black ## Non-Hispanic Asian ## Other Race - Including Multi-Racial ## LBXTC (mean (SD)) ## ## Overall ## n 15560 ## RIDEXPRG (%) 87 ( 0.6) ## 1604 (10.3) ## 183 ( 1.2) ## 13686 (88.0) ## RIAGENDR (%) 7721 (49.6) ## 7839 (50.4) ## RIDAGEYR (mean (SD)) 33.74 (25.32) ## RIDRETH3 (%) 1990 (12.8) ## 1544 ( 9.9) ## 5271 (33.9) ## 4098 (26.3) ## 1638 (10.5) ## 1019 ( 6.5) ## LBXTC (mean (SD)) 177.46 (40.36) Apply Eligibility Criteria I decided for the eligibility criteria to exclude pregnant women, as well as those under the age of 20, I did that using dplyr\u0026rsquo;s filter() function, I also filtered out the columns where there was no total cholesterol lab value as that is not useful to my analysis.\n1 2 3 4 analytic_data \u0026lt;- merged_data %\u0026gt;% filter(!is.na(LBXTC), RIDAGEYR \u0026gt;= 20, RIDEXPRG != \u0026#34;Yes, positive lab pregnancy test or self-reported pregnant at exam\u0026#34; | is.na(RIDEXPRG)) Recode and Make Categories In this section, I created several new variables using dplyr\u0026rsquo;s mutate() function. I created the age_cat variable that groups the participants by age, the total_cholesterol_cat variable that groups the lab values by their normal, borderline and high ranges, and I also simplified the race data into more concise and broad categories.\nI also renamed the remaining variables to match the naming conventions in the other newly created variables, also to give some more sense to them as NHANES is great but the variables all look like keyboard smashes to me. Their variable search tool helps with that.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 category_data \u0026lt;- analytic_data %\u0026gt;% mutate( age_cat = cut(analytic_data$RIDAGEYR, c(20, 40, 60, Inf), right = FALSE), total_cholesterol_cat = cut(analytic_data$LBXTC, c(0, 200, 240, Inf), labels = c(\u0026#34;Normal\u0026#34;, \u0026#34;Borderline\u0026#34;, \u0026#34;High\u0026#34;), right = FALSE), race = car::recode(analytic_data$RIDRETH3, \u0026#34;c(\u0026#39;Mexican American\u0026#39;, \u0026#39;Other Hispanic\u0026#39;) = \u0026#39;Hispanic\u0026#39;; \u0026#39;Non-Hispanic White\u0026#39; = \u0026#39;White\u0026#39;; \u0026#39;Non-Hispanic Black\u0026#39; = \u0026#39;Black\u0026#39;; \u0026#39;Non-Hispanic Asian\u0026#39; = \u0026#39;Asian\u0026#39;; \u0026#39;Other Race - Including Multi-Racial\u0026#39; = \u0026#39;Other\u0026#39;; else = NA\u0026#34;) ) %\u0026gt;% rename( pregnancy_status = RIDEXPRG, gender = RIAGENDR, age = RIDAGEYR, total_cholesterol = LBXTC ) %\u0026gt;% select(-RIDRETH3, -pregnancy_status) 1 2 cholesterol \u0026lt;- as_tibble(category_data) cholesterol 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 7,845 × 6 ## gender age total_cholesterol age_cat total_cholesterol_cat race ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Female 29 195 [20,40) Normal Asian ## 2 Male 49 147 [40,60) Normal White ## 3 Male 36 164 [20,40) Normal White ## 4 Male 68 105 [60,Inf) Normal Other ## 5 Male 76 233 [60,Inf) Borderline White ## 6 Female 44 212 [40,60) Borderline Hispanic ## 7 Female 68 165 [60,Inf) Normal Black ## 8 Female 42 229 [40,60) Borderline Asian ## 9 Male 58 172 [40,60) Normal Hispanic ## 10 Male 44 189 [40,60) Normal White ## # ℹ 7,835 more rows Table of Total Cholesterol Categories Here I used the arsenal package to make a table displaying how the different total cholesterol categories look within our study sample.\n1 2 3 labels \u0026lt;- list(age_cat = \u0026#34;Age Range\u0026#34;, gender = \u0026#34;Gender\u0026#34;, race = \u0026#34;Race\u0026#34;) tab \u0026lt;- arsenal::tableby(total_cholesterol_cat ~ age_cat + gender + race, data = cholesterol, test = FALSE) summary(tab, labelTranslations = labels, text=TRUE) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## ## ## | | Normal (N=5236) | Borderline (N=1873) | High (N=736) | Total (N=7845) | ## |:-----------|:---------------:|:-------------------:|:------------:|:--------------:| ## |Age Range | | | | | ## |- [20,40) | 1789 (34.2%) | 412 (22.0%) | 117 (15.9%) | 2318 (29.5%) | ## |- [40,60) | 1499 (28.6%) | 789 (42.1%) | 338 (45.9%) | 2626 (33.5%) | ## |- [60,Inf) | 1948 (37.2%) | 672 (35.9%) | 281 (38.2%) | 2901 (37.0%) | ## |Gender | | | | | ## |- Male | 2659 (50.8%) | 850 (45.4%) | 323 (43.9%) | 3832 (48.8%) | ## |- Female | 2577 (49.2%) | 1023 (54.6%) | 413 (56.1%) | 4013 (51.2%) | ## |Race | | | | | ## |- Asian | 559 (10.7%) | 271 (14.5%) | 108 (14.7%) | 938 (12.0%) | ## |- Black | 1430 (27.3%) | 400 (21.4%) | 162 (22.0%) | 1992 (25.4%) | ## |- Hispanic | 1169 (22.3%) | 432 (23.1%) | 164 (22.3%) | 1765 (22.5%) | ## |- Other | 245 (4.7%) | 96 (5.1%) | 36 (4.9%) | 377 (4.8%) | ## |- White | 1833 (35.0%) | 674 (36.0%) | 266 (36.1%) | 2773 (35.3%) | Checking Assumptions Assumption of Normal Distribution Next before I look and see if there are any statistically significant differences between the groups of my choice, I have to look and see the distribution of our data to see what kind of assumptions we can make when running the stats.\n1 2 3 4 5 6 7 ggplot(cholesterol, aes(x = total_cholesterol)) + geom_histogram(binwidth = 15, color = \u0026#34;black\u0026#34;, fill = \u0026#34;#bbbbff\u0026#34;) + labs(x = \u0026#34;Total Cholesterol (mg/dL)\u0026#34;, y = \u0026#34;Count\u0026#34;, title = \u0026#34;Total Cholesterol (mg/dL) Distribution\u0026#34;, caption = \u0026#34;Source: CDC NHANES 2017 - Pre Pandemic 2020\u0026#34;) + theme(plot.title = element_text(hjust = 0.5)) This follows a generally normal distribution, but I\u0026rsquo;m going to take a closer look as I do see outliers towards the end of our bell curve.\n1 2 cholesterol_model \u0026lt;- lm(total_cholesterol ~ race, data = cholesterol) plot(cholesterol_model, which = 2) It seems like there is a systematic deviation from the expected relationship if the data were to be normally distributed. The data is not normally distributed. Assumption of Constant Variance 1 plot(cholesterol_model, which = 3) There is constant variance throughout the data, however since our assumption of normal distribution was violated I am going to check one more assumption that would be needed to run a non parametric test assessing the differences between the central tendency of our chosen groups. Assumption of Similar Skewness for Each Category 1 2 3 4 5 6 7 8 9 ggplot(cholesterol, aes(x = total_cholesterol, fill = race)) + geom_histogram(binwidth = 15, color = \u0026#34;black\u0026#34;) + labs(x = \u0026#34;Total Cholesterol (mg/dL)\u0026#34;, y = \u0026#34;Count\u0026#34;, title = \u0026#34;Total Cholesterol (mg/dL) Distribution by Race/Hispanic Origin\u0026#34;, caption = \u0026#34;Source: CDC NHANES 2017 - Pre Pandemic 2020\u0026#34;) + theme(plot.title = element_text(hjust = 0.5)) + facet_wrap(~race) + scale_fill_manual(values = c(\u0026#34;#E6FFFD\u0026#34;, \u0026#34;#AEE2FF\u0026#34;, \u0026#34;#ACBCFF\u0026#34;, \u0026#34;#B799FF\u0026#34;, \u0026#34;#AA77FF\u0026#34;)) Looks like each category of Race/Hispanic origin does have a similar skewness. Statistical Testing Visualize Data with a Boxplot 1 2 3 4 5 6 7 8 9 10 11 ggplot(cholesterol, aes(x = race, y = total_cholesterol, fill = race)) + geom_boxplot() + scale_fill_manual(values = c(\u0026#34;#E6FFFD\u0026#34;, \u0026#34;#AEE2FF\u0026#34;, \u0026#34;#ACBCFF\u0026#34;, \u0026#34;#B799FF\u0026#34;, \u0026#34;#AA77FF\u0026#34;)) + labs( x = \u0026#34;Race/Hispanic Origin\u0026#34;, y = \u0026#34;Total Cholesterol (mg/dL)\u0026#34;, title = \u0026#34;Total Cholesterol (mg/dL) by Race/Hispanic Origin of Participant\u0026#34;, caption = \u0026#34;Source: CDC NHANES 2017 - Pre Pandemic 2020\u0026#34; ) + theme(legend.position = \u0026#34;none\u0026#34;, plot.title = element_text(hjust = 0.5)) Kruskal Wallis Test Since I checked various assumptions, and the assumption of normality was violated, I decided to go with a non parametric Kruskal Wallis Test to test my hypothesis that there is a different mean total cholesterol value between the 5 different racial/hispanic origin categories.\n1 2 options(scipen = 999) kruskal.test(total_cholesterol ~ race, data = cholesterol) 1 2 3 4 5 6 ## ## Kruskal-Wallis rank sum test ## ## data: total_cholesterol by race ## Kruskal-Wallis chi-squared = 60.582, df = 4, p-value = ## 0.000000000002188 The p value is very very small, meaning there is one or more categories that have a mean total cholesterol that differs from the other. To check which groups are causing there to be a statisticially signifigant difference, I\u0026rsquo;ll run a pairwise comparison.\nPairwise Comparison Test 1 2 pairwise.wilcox.test(cholesterol$total_cholesterol, cholesterol$race, p.adjust.method = \u0026#34;BH\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 ## ## Pairwise comparisons using Wilcoxon rank sum test with continuity correction ## ## data: cholesterol$total_cholesterol and cholesterol$race ## ## Asian Black Hispanic Other ## Black 0.0000000000011 - - - ## Hispanic 0.00096 0.0000031636325 - - ## Other 0.01722 0.01722 0.77812 - ## White 0.0000031636325 0.00069 0.12485 0.61744 ## ## P value adjustment method: BH Looks like there are multiple groups in which a statistically significant difference can be observed, in fact it is present in all except Other and Hispanic, White and Hispanic, and Other and White.\nConclusion I was always hesitant to use the CDC\u0026rsquo;s NHANES dataset as it is so big and there are so many different variables but I\u0026rsquo;m glad that I found the nhanesA package as that made it so easy to do this analysis, and I\u0026rsquo;ll definitely be using it again to avoid having to dig for datasets and also having to download gigantic ones to my poor old computer lol.\nI also learned some other things during this analysis, in terms of R skills I learned about the arsenal package that builds tables to display the various categories that were pre-existing in the data, as well as the categories that I created using the cut() function based on the typical ranges used in medicine for total cholesterol.\nIn terms of statistics, I still have some confusion about the central limit theorem and normal distributions and if you can actually use parametric tests on non normal distributions, as there seems to be a lot of heated debate over this on the various statistics forums that I visited in hope of getting an answer to this question. I decided to play it safe and use a non parametric test, but I do want to learn more about how and when you can violate assumptions if you should at all.\n","date":"2024-01-12T00:00:00Z","image":"https://michelleyg1.github.io/p/total-cholesterol-cdc-nhanes-analysis/images/purple_hu3d03a01dcc18bc5be0e67db3d8d209a6_1915810_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/total-cholesterol-cdc-nhanes-analysis/","title":"Total Cholesterol CDC NHANES Analysis"},{"content":"I found this dataset by filtering through the various settings that they have on the National Agriculture Statistics Service quick stats tool to see the condition of blueberries by year and week in the state of New Jersey.\nA lot of people think that New Jersey is only the city and the shore, but its not called the Garden State for nothing! For its small size, New Jersey punches above its weight class in producing and exporting various agricultural products.\nOne crop that New Jersey is known for is its blueberries.\nLoad Package 1 library(tidyverse) 1 2 3 4 5 6 7 8 9 10 ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors 1 library(janitor) 1 2 3 4 5 6 ## ## Attaching package: \u0026#39;janitor\u0026#39; ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## chisq.test, fisher.test Import Dataset 1 blueberries \u0026lt;- read_csv(\u0026#34;/Users/michellegulotta/Desktop/my_first_project/blueberries/blueberry.csv\u0026#34;) 1 2 3 4 5 6 7 8 9 10 ## Rows: 225 Columns: 21 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026#34;,\u0026#34; ## chr (9): Program, Period, Geo Level, State, watershed_code, Commodity, Data... ## dbl (3): Year, State ANSI, Value ## lgl (8): Ag District, Ag District Code, County, County ANSI, Zip Code, Regi... ## date (1): Week Ending ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 1 2 3 4 5 6 7 8 blueberries \u0026lt;- blueberries %\u0026gt;% janitor::clean_names(., \u0026#34;snake\u0026#34;) %\u0026gt;% select(year, period, week_ending, data_item, value) %\u0026gt;% rename( condition = data_item, percent = value, week_number = period ) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 225 × 5 ## year week_number week_ending condition percent ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## 2 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## 3 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 100 ## 4 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## 5 2023 WEEK #26 2023-07-02 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## 6 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 18 ## 7 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 19 ## 8 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 44 ## 9 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 19 ## 10 2023 WEEK #27 2023-07-09 BLUEBERRIES, TAME - CONDITION, MEASURE… 0 ## # ℹ 215 more rows Cleaning Data Clean up Condition and Week Number Column I wanted to clean up the condition column as I noticed it was pretty redundant, and all we really needed was the condition. This would also make it eaiser as this is going to be the legend on our graph. Another thing that I wanted to do was to make the week number column more simple, as the name of the variable already tells us that this is the week number, so all we really need in the observation is the number.\n1 2 blueberries$condition \u0026lt;- str_replace_all(blueberries$condition, \u0026#34;BLUEBERRIES, TAME - CONDITION, MEASURED IN PCT\u0026#34;, \u0026#34;\u0026#34;) blueberries$week_number \u0026lt;- str_replace_all(blueberries$week_number, \u0026#34;WEEK #\u0026#34;, \u0026#34;\u0026#34;) Checking to make sure that the different conditions look good\n1 unique(blueberries$condition) 1 ## [1] \u0026#34; EXCELLENT\u0026#34; \u0026#34; FAIR\u0026#34; \u0026#34; GOOD\u0026#34; \u0026#34; POOR\u0026#34; \u0026#34; VERY POOR\u0026#34; It looks like there\u0026rsquo;s some random leading and trailing white space, so I\u0026rsquo;m going to clean that up using the trimws() function, as well as the str_to_title() function to make it more readable and look nicer on our graph\n1 2 blueberries$condition \u0026lt;- trimws(blueberries$condition) blueberries$condition \u0026lt;- str_to_title(blueberries$condition) Checking the conditions again\n1 unique(blueberries$condition) 1 ## [1] \u0026#34;Excellent\u0026#34; \u0026#34;Fair\u0026#34; \u0026#34;Good\u0026#34; \u0026#34;Poor\u0026#34; \u0026#34;Very Poor\u0026#34; Looks good! Time to make a data visualization\nData Visualization 1 2 3 4 5 6 blueberries_2021 \u0026lt;- blueberries %\u0026gt;% filter(year == 2021) %\u0026gt;% mutate( condition_f = factor(condition) ) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 55 × 6 ## year week_number week_ending condition percent condition_f ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 2021 25 2021-06-27 Excellent 25 Excellent ## 2 2021 25 2021-06-27 Fair 25 Fair ## 3 2021 25 2021-06-27 Good 50 Good ## 4 2021 25 2021-06-27 Poor 0 Poor ## 5 2021 25 2021-06-27 Very Poor 0 Very Poor ## 6 2021 26 2021-07-04 Excellent 84 Excellent ## 7 2021 26 2021-07-04 Fair 8 Fair ## 8 2021 26 2021-07-04 Good 8 Good ## 9 2021 26 2021-07-04 Poor 0 Poor ## 10 2021 26 2021-07-04 Very Poor 0 Very Poor ## # ℹ 45 more rows Factor to Change Order of Legend One of the problems that I came across while trying to make this graph was that the legend would appear in alphabetical order rather than in the order that made sense from excellent to very poor. After some trying and researching solutions, I realized that I could use the factor() function to change the order of the levels.\n1 2 blueberries_2021$condition_f \u0026lt;- factor(blueberries_2021$condition_f, levels = c(\u0026#34;Excellent\u0026#34;, \u0026#34;Good\u0026#34;, \u0026#34;Fair\u0026#34;, \u0026#34;Poor\u0026#34;, \u0026#34;Very Poor\u0026#34;)) Make Plot 1 2 3 4 5 6 7 8 9 10 11 ggplot(blueberries_2021, aes(x = week_ending, y = percent, fill = condition_f)) + geom_area(alpha = 0.5, position = \u0026#34;identity\u0026#34;) + scale_fill_manual(values = c(\u0026#34;#785EF0\u0026#34;, \u0026#34;#009E73\u0026#34;, \u0026#34;#FFB000\u0026#34;, \u0026#34;#FE6100\u0026#34;, \u0026#34;#DC267F\u0026#34;)) + labs(title = \u0026#34;Condition of Blueberries Measured in Percent in New Jersey in 2021\u0026#34;, x = \u0026#34;Week\u0026#34;, y = \u0026#34;Percent\u0026#34;, fill = \u0026#34;Condition\u0026#34;, caption = \u0026#34;Source: USDA National Agrigultural Statistics Service\u0026#34;) + theme_light() Conclusion Now it\u0026rsquo;s the beginning of January but this has me looking forward to the beginning of July!\nWorking on this specific visualization helped me work on my data cleaning skills. I also learned more about how factors work and how R automatically puts a character vector in alphabetical order, and that in order to get it to appear in the order you want on a legend, you have to change the order of the factor variable. I didn\u0026rsquo;t think about that before I started but it definitely makes sense, R doesn\u0026rsquo;t just know how humans rank the quality of blueberries.\nI also learned about geom_area(), I first tried to use geom_line() and was able to make that pretty easily, but when I tried to change it to geom_area() there was a lot that I had to change in order to get the effect that I wanted on my graph.\n","date":"2024-01-07T00:00:00Z","image":"https://michelleyg1.github.io/p/jersey-blueberries/images/blueberry_hu3d03a01dcc18bc5be0e67db3d8d209a6_1579336_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/jersey-blueberries/","title":"Jersey Blueberries"},{"content":"I found this interesting dataset on Kaggle and wanted to do an exploratory analysis as I also have had asthma since I was a kid and wanted to see if I could find any interesting patterns within the data.\nLoad Packages 1 2 3 4 suppressPackageStartupMessages({ library(tidyverse) library(janitor) }) Import Dataset 1 2 3 asthma \u0026lt;- read.csv(\u0026#34;/Users/michellegulotta/Desktop/my_first_project/asthma/CDIAsthmaByStateTransposed2010-2020.csv\u0026#34;) asthma \u0026lt;- asthma %\u0026gt;% janitor::clean_names(., \u0026#34;snake\u0026#34;) Taking a look at the first few rows of the data to see what kind of information this dataset is providing\n1 head(asthma) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## geo_loc year state pop_est ## 1 POINT (-86.63186076199969 32.84057112200048) 2010 Alabama 4785514 ## 2 POINT (-147.72205903599973 64.84507995700051) 2010 Alaska 713982 ## 3 POINT (-111.76381127699972 34.865970280000454) 2010 Arizona 6407342 ## 4 POINT (-92.27449074299966 34.74865012400045) 2010 Arkansas 2921998 ## 5 POINT (-120.99999953799971 37.63864012300047) 2010 California 37319550 ## 6 POINT (-106.13361092099967 38.843840757000464) 2010 Colorado 5047539 ## f_fatal m_fatal o_fatal f_er m_er o_er f_hosp m_hosp o_hosp ## 1 44 17 61 NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA ## 3 31 27 58 22280 18335 40650 4717 3352 8073 ## 4 29 8 37 NA NA NA 1876 962 2841 ## 5 251 152 403 116571 96999 217637 19399 12958 34583 ## 6 33 15 48 NA NA NA 2385 1942 4336 Asthma Fatalities by Gender in New Jersey Data Visualization Clean Up Data Since its my home state, I decided to narrow the data that I wanted to look at down a bit to just New Jersey. I also only was interested in looking at fatalities for this particular graph as it is the most severe type of asthma incident recorded in this data.\nThe first thing that I needed to do was to pivot the data so that gender was its own observation, I originally missed out on this step when I was trying to make the graph and had a hard time coming up with the ggplot code as I was just repeating adding a different shape to my graph for each column. After a bit of trial and error I realized there was probably a way to do it where I wouldn’t have to add the columns individually as their own geoms.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 nj_asthma_fatalities \u0026lt;- asthma %\u0026gt;% filter(state == \u0026#34;New Jersey\u0026#34;) %\u0026gt;% select(year, ends_with(\u0026#34;_fatal\u0026#34;)) %\u0026gt;% pivot_longer( cols = ends_with(\u0026#34;_fatal\u0026#34;), names_to = \u0026#34;gender\u0026#34;, values_to = \u0026#34;fatalities\u0026#34;, names_pattern = \u0026#34;(.*)_fatal\u0026#34; ) %\u0026gt;% mutate( gender = factor(gender, c(\u0026#34;f\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;o\u0026#34;), c(\u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;, \u0026#34;Overall\u0026#34;)) ) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 33 × 3 ## year gender fatalities ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Female 49 ## 2 2010 Male 31 ## 3 2010 Overall 80 ## 4 2011 Female 62 ## 5 2011 Male 32 ## 6 2011 Overall 94 ## 7 2012 Female 63 ## 8 2012 Male 40 ## 9 2012 Overall 103 ## 10 2013 Female 73 ## # ℹ 23 more rows Create a Graph Now to take my newly pivoted data and create a graph using the ggplot2 package:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ggplot(data = nj_asthma_fatalities) + geom_smooth(mapping = aes( x = year, y = fatalities, group = gender, color = gender), se = FALSE) + scale_color_manual(values = c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;black\u0026#34;)) + scale_x_continuous(n.breaks = 10) + labs(title = \u0026#34;Asthma Fatalities in New Jersey by Gender from 2010 to 2020\u0026#34;, x = \u0026#34;Years\u0026#34;, y = \u0026#34;Number of Fatalities\u0026#34;) + theme(plot.title = element_text(hjust=0.5)) + theme_bw() 1 ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula = \u0026#39;y ~ x\u0026#39; This graph shows an interesting trend that at least in the state of New Jersey over the 2010s decade asthma fatality incidents increased and then decreased, just to increase once again around 2017. Also, another trend that caught my eye was that that females made up the majority of overall fatalities that were recorded.\nAs these are not proportional to the population, they do not tell us the whole story. I’m interested in seeing if the mortality rates show the same pattern as anyone living in New Jersey can tell you that the population has increased over this past decade just from the traffic alone, does the population change account for the increase in asthma fatality incidents?\nCalculate Mortality Rate per 100,000 People Column I calculated the cause specific mortality rate per 100,000 people in the whole population of the state.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 nj_asthma_mortality_rate \u0026lt;- asthma %\u0026gt;% filter(state == \u0026#34;New Jersey\u0026#34;) %\u0026gt;% select(year, pop_est, ends_with(\u0026#34;_fatal\u0026#34;)) %\u0026gt;% pivot_longer( cols = ends_with(\u0026#34;_fatal\u0026#34;), names_to = \u0026#34;gender\u0026#34;, values_to = \u0026#34;fatalities\u0026#34;, names_pattern = \u0026#34;(.*)_fatal\u0026#34; ) %\u0026gt;% mutate( gender = factor(gender, c(\u0026#34;f\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;o\u0026#34;), c(\u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;, \u0026#34;Overall\u0026#34;)), mortality_rate = round(((fatalities / pop_est) * 100000), 2) ) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 33 × 5 ## year pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 8799451 Female 49 0.56 ## 2 2010 8799451 Male 31 0.35 ## 3 2010 8799451 Overall 80 0.91 ## 4 2011 8828552 Female 62 0.7 ## 5 2011 8828552 Male 32 0.36 ## 6 2011 8828552 Overall 94 1.06 ## 7 2012 8845671 Female 63 0.71 ## 8 2012 8845671 Male 40 0.45 ## 9 2012 8845671 Overall 103 1.16 ## 10 2013 8857821 Female 73 0.82 ## # ℹ 23 more rows Create a Graph of Asthma Mortality Rate per 100,000 People and Group By Gender Then I used ggplot2 to graph this new column to compare the mortality rate per 100,000 people over the decade of 2010 to 2020 to see if the same trend emerges:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ggplot(data = nj_asthma_mortality_rate) + geom_smooth(mapping = aes(x = year, y = mortality_rate, group = gender, color = gender), se = FALSE) + scale_color_manual(values = c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;black\u0026#34;)) + scale_x_continuous(n.breaks = 10) + labs(title = \u0026#34;Asthma Mortality Rate in New Jersey by Gender from 2010 to 2020\u0026#34;, x = \u0026#34;Years\u0026#34;, y = \u0026#34;Number of Fatalities\u0026#34;) + theme(plot.title = element_text(hjust=0.5)) + theme_bw() 1 ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula = \u0026#39;y ~ x\u0026#39; Interesting, the graphs look pretty much the same to me so the population increase is not the reason for the increase in asthma fatality incidents as the mortality rate from asthma follows the same pattern as the fatality incidents over time, as well as the gender disparity.\nNationwide Analysis of Gender Differences In Asthma Mortality Rate Clean Up Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 asthma_mortality \u0026lt;- asthma %\u0026gt;% select(year, state, pop_est, f_fatal, m_fatal) %\u0026gt;% pivot_longer( cols = ends_with(\u0026#34;_fatal\u0026#34;), names_to = \u0026#34;gender\u0026#34;, values_to = \u0026#34;fatalities\u0026#34;, names_pattern = \u0026#34;(.*)_fatal\u0026#34; ) %\u0026gt;% mutate( gender = factor(gender, c(\u0026#34;f\u0026#34;, \u0026#34;m\u0026#34;, \u0026#34;o\u0026#34;), c(\u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;, \u0026#34;Overall\u0026#34;)), mortality_rate = round(((fatalities / pop_est) * 100000), 2) ) %\u0026gt;% filter(gender != \u0026#34;Overall\u0026#34;) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 1,122 × 6 ## year state pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Alabama 4785514 Female 44 0.92 ## 2 2010 Alabama 4785514 Male 17 0.36 ## 3 2010 Alaska 713982 Female NA NA ## 4 2010 Alaska 713982 Male NA NA ## 5 2010 Arizona 6407342 Female 31 0.48 ## 6 2010 Arizona 6407342 Male 27 0.42 ## 7 2010 Arkansas 2921998 Female 29 0.99 ## 8 2010 Arkansas 2921998 Male 8 0.27 ## 9 2010 California 37319550 Female 251 0.67 ## 10 2010 California 37319550 Male 152 0.41 ## # ℹ 1,112 more rows I noticed there were a decent amount of missing values just by glancing at this lets see how many exactly\n1 sum(is.na(asthma_mortality$mortality_rate)) 1 ## [1] 264 Hm, 264/1122 that is not that bad, also it does look like they\u0026rsquo;re states with limited populations, so that might be the reason is that there just weren\u0026rsquo;t any fatal asthma incidents in that particular year.\nI\u0026rsquo;m going to remove the missing values for our next step in this analysis\n1 2 3 asthma_mortality_no_miss \u0026lt;- asthma_mortality %\u0026gt;% drop_na(mortality_rate) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 858 × 6 ## year state pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Alabama 4785514 Female 44 0.92 ## 2 2010 Alabama 4785514 Male 17 0.36 ## 3 2010 Arizona 6407342 Female 31 0.48 ## 4 2010 Arizona 6407342 Male 27 0.42 ## 5 2010 Arkansas 2921998 Female 29 0.99 ## 6 2010 Arkansas 2921998 Male 8 0.27 ## 7 2010 California 37319550 Female 251 0.67 ## 8 2010 California 37319550 Male 152 0.41 ## 9 2010 Colorado 5047539 Female 33 0.65 ## 10 2010 Colorado 5047539 Male 15 0.3 ## # ℹ 848 more rows Create a Histogram of Fatal Asthma Incidents for Each Gender I then decided to create a histogram to look at how these asthma fatalities are distributed.\nI\u0026rsquo;m going to use ggplot2 to make a histogram comparing the distribution of mortality rates between the two populations\n1 2 3 4 5 6 ggplot(data = asthma_mortality_no_miss, aes(x = mortality_rate, fill = gender)) + geom_histogram(color = \u0026#34;black\u0026#34;) + scale_fill_manual(values=c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;)) + labs(title = \u0026#34;Asthma Mortality Rate by Gender in the USA from 2010 to 2020\u0026#34;, x = \u0026#34;Mortality Rate\u0026#34;, y= \u0026#34;Number of Observations\u0026#34;) 1 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. It looks like there are some outliers, let me take a look at the data sorted by mortality rate descending\n1 asthma_mortality_no_miss[order(asthma_mortality_no_miss$mortality_rate, decreasing = TRUE),] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 858 × 6 ## year state pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2020 New Mexico 211839 Female 26 12.3 ## 2 2020 Pennsylvania 1299444 Female 97 7.46 ## 3 2020 Illinois 1278658 Male 89 6.96 ## 4 2020 Illinois 1278658 Female 88 6.88 ## 5 2020 New Mexico 211839 Male 13 6.14 ## 6 2020 Pennsylvania 1299444 Male 69 5.31 ## 7 2014 Mississippi 2991892 Female 39 1.3 ## 8 2016 Mississippi 2990595 Female 38 1.27 ## 9 2014 Oregon 3965447 Female 50 1.26 ## 10 2014 Iowa 3110643 Female 38 1.22 ## # ℹ 848 more rows Wow, it looks like in 2020 there was a huge increase due to the pandemic most likely. I\u0026rsquo;m going to take a look at the data from 2010-2019 to get a closer look at the distribution.\n1 2 3 asthma_mortality_drop_2020 \u0026lt;- asthma_mortality_no_miss %\u0026gt;% filter(year != 2020) %\u0026gt;% print() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## # A tibble: 780 × 6 ## year state pop_est gender fatalities mortality_rate ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2010 Alabama 4785514 Female 44 0.92 ## 2 2010 Alabama 4785514 Male 17 0.36 ## 3 2010 Arizona 6407342 Female 31 0.48 ## 4 2010 Arizona 6407342 Male 27 0.42 ## 5 2010 Arkansas 2921998 Female 29 0.99 ## 6 2010 Arkansas 2921998 Male 8 0.27 ## 7 2010 California 37319550 Female 251 0.67 ## 8 2010 California 37319550 Male 152 0.41 ## 9 2010 Colorado 5047539 Female 33 0.65 ## 10 2010 Colorado 5047539 Male 15 0.3 ## # ℹ 770 more rows And repeat the histogram process with the 2010 to 2019 data\n1 2 3 4 5 6 ggplot(data = asthma_mortality_drop_2020, aes(x = mortality_rate, fill = gender)) + geom_histogram(color = \u0026#34;black\u0026#34;) + scale_fill_manual(values=c(\u0026#34;pink\u0026#34;, \u0026#34;blue\u0026#34;)) + labs(title = \u0026#34;Asthma Mortality Rate by Gender in the USA from 2010 to 2019\u0026#34;, x = \u0026#34;Mortality Rate\u0026#34;, y= \u0026#34;Number of Observations\u0026#34;) 1 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. I want to compare the means of these populations to see if there is a statistically significant difference in fatal asthma incidence between the two genders throughout the whole USA.\nStatistical Testing From the histogram, these don\u0026rsquo;t really seem to fit a normal distribution, but rather a skewed distribution, but let\u0026rsquo;s run the Shapiro-Wilk test to be sure instead of just eyeballing it and assuming.\n1 2 options(scipen = 999) shapiro.test(asthma_mortality_no_miss$mortality_rate) 1 2 3 4 5 ## ## Shapiro-Wilk normality test ## ## data: asthma_mortality_no_miss$mortality_rate ## W = 0.27179, p-value \u0026lt; 0.00000000000000022 Okay, since the p value is not greater than 0.05, I can\u0026rsquo;t use a t-test as that assumes normal distribution.\nI\u0026rsquo;m going to go for a non-parametric test since we\u0026rsquo;re not assuming any particular distribution here to test my hypothesis that when it comes to asthma fatalities females have a higher mortality rate than males. Since I have two unpaired samples and I want to test how their values compare, I\u0026rsquo;m going to use the Mann-Whitney test.\n1 wilcox.test(asthma_mortality_no_miss$mortality_rate) 1 2 3 4 5 6 ## ## Wilcoxon signed rank test with continuity correction ## ## data: asthma_mortality_no_miss$mortality_rate ## V = 368511, p-value \u0026lt; 0.00000000000000022 ## alternative hypothesis: true location is not equal to 0 With these results we can reject the null hypothesis, and say within this data there is a statistically significant difference between the two populations.\nConclusion Before this analysis I had no idea of the gender differences that arise in asthma incidents, upon looking into this further after completing this analysis I came across a paper that discusses the gender differences in asthma prevalence and severity on a biological level.\nAccording to the Asthma and Allergy Network, In people under 18, it is more common for boys to have asthma than girls, but this prevalence switches when analyzing adult populations. The fact that women have a higher risk of death from asthma when compared to men is also confirmed by this source.\n","date":"2024-01-03T00:00:00Z","image":"https://michelleyg1.github.io/p/asthma-incidents-analysis/images/featured_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpeg","permalink":"https://michelleyg1.github.io/p/asthma-incidents-analysis/","title":"Asthma Incidents Analysis"}]